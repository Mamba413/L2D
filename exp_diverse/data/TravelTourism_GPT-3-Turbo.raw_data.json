{
  "original": [
    "Process calculi based on logic, such as $\\pi$DILL and CP, provide a foundation for deadlock-free concurrent programming. However, in previous work, there is a mismatch between the rules for constructing proofs and the term constructors of the $\\pi$-calculus: the fundamental operator for parallel composition does not correspond to any rule of linear logic. Kokke et al. (2019) introduced Hypersequent Classical Processes (HCP), which addresses this mismatch using hypersequents (collections of sequents) to register parallelism in the typing judgements. However, the step from CP to HCP is a big one. As of yet, HCP does not have reduction semantics, and the addition of delayed actions means that CP processes interpreted as HCP processes do not behave as they would in CP. We introduce HCP-, a variant of HCP with reduction semantics and without delayed actions. We prove progress, preservation, and termination, and show that HCP- supports the same communication protocols as CP.",
    "A simple variant of the BDDC preconditioner in which constraints are imposed on a selected set of subobjects (subdomain subedges, subfaces and vertices between pairs of subedges) is presented. We are able to show that the condition number of the preconditioner is bounded by $C \\big(1+\\log (L/h)\\big)^2$, where $C$ is a constant, and $h$ and $L$ are the characteristic sizes of the mesh and the subobjects, respectively. As $L$ can be chosen almost freely, the condition number can theoretically be as small as $O(1)$. We will discuss the pros and cons of the preconditioner and its application to heterogeneous problems. Numerical results on supercomputers are provided.",
    "We give examples of where the Heun function exists as solutions of wave equations encountered in general relativity. While the Dirac equation written in the background of Nutku helicoid metric yields Mathieu functions as its solutions in four spacetime dimensions, the trivial generalization to five dimensions results in the double confluent Heun function. We reduce this solution to the Mathieu function with some transformations. We must apply Atiyah-Patodi-Singer spectral boundary conditions to this system since the metric has a singularity at the origin.",
    "As it was shown by many authors, a slow decrease in X-rays observed during the decay phase of long duration flares (LDE) can be explained only by a magnetic reconnection and energy release ceaselessly ongoing in the coronal part of a flare. Using RHESSI data we try to answer two following questions. How effective are these processes at the LDEs decay phase and how can precisely the energy release rate be calculated based on these data? To answer the questions images of the selected LDEs during their decay phase were reconstructed. Physical parameters of flare coronal sources obtained from image spectral analysis allowed us to study the efficiency of the energy release process. We also examined terms included in the energy equation to find out what is the accuracy of determination of each term.",
    "By means of a multi-scale analysis we describe the typical geometrical structure of the clusters under the FK measure in random media. Our result holds in any dimension greater or equal to 2 provided that slab percolation occurs under the averaged measure, which should be the case in the whole supercritical phase. This work extends the one of Pisztora and provides an essential tool for the analysis of the supercritical regime in disordered FK models and in the corresponding disordered Ising and Potts models.",
    "Photospheric absorption lines in classical T Tauri stars (CTTS) are weak compared to normal stars. This so-called veiling is normally identified with an excess continuous emission formed in shock-heated gas at the stellar surface below the accretion streams. We have selected four stars (RW Aur A, RU Lup, S CrA NW and S CrA SE) with unusually strong veiling to make a detailed investigation of veiling versus stellar brightness and emission line strengths for comparisons to standard accretion models. We have monitored the stars photometrically and spectroscopically at several epochs. In standard accretion models a variable accretion rate will lead to a variable excess emission. Consequently, the stellar brightness should vary accordingly. We find that the veiling of absorption lines in these stars is strongly variable and usually so large that it would require the release of several stellar luminosities of potential energy. At states of very large line dilution, the correspondingly large veiling factors derived correlate only weakly with brightness. Moreover, the emission line strengths violate the expected trend of veiling versus line strength. The veiling can change dramatically in one night, and is not correlated with the phase of the rotation periods found for two stars. We show that in at least three of the stars, when the veiling becomes high, the photospheric lines become filled-in by line emission, which produces large veiling factors unrelated to changes in any continuous emission from shocked regions. We also consider to what extent extinction by dust and electron scattering in the accretion stream may affect veiling measures in CTTS. We conclude that the degree of veiling cannot be used as a measure of accretion rates in CTTS with rich emission line spectra.",
    "Giant low surface brightness (GLSB) galaxies are commonly thought to be massive, dark matter dominated systems. However, this conclusion is based on highly uncertain rotation curves. We present here a new study of two prototypical GLSB galaxies: Malin 1 and NGC 7589. We re-analysed existing HI observations and derived new rotation curves, which were used to investigate the distributions of luminous and dark matter in these galaxies. In contrast to previous findings, the rotation curves of both galaxies show a steep rise in the central parts, typical of high surface brightness (HSB) systems. Mass decompositions with a dark matter halo show that baryons may dominate the dynamics of the inner regions. Indeed, a \"maximum disk\" fit gives stellar mass-to-light ratios in the range of values typically found for HSB galaxies. These results, together with other recent studies, suggest that GLSB galaxies are systems with a double structure: an inner HSB early-type spiral galaxy and an outer extended LSB disk. We also tested the predictions of MOND: the rotation curve of NGC 7589 is reproduced well, whereas Malin 1 represents a challenging test for the theory.",
    "The multiplicity distribution, multiplicity moment, scaled variance, entropy and reduced entropy of target evaporated fragment emitted in forward and backward hemispheres in 12 A GeV $^{4}$He, 3.7 A GeV $^{16}$O, 60 A GeV $^{16}$O, 1.7 A GeV $^{84}$Kr and 10.7 A GeV $^{197}$Au induced emulsion heavy targets (AgBr) interactions are investigated. It is found that the multiplicity distribution of target evaporated fragments emitted in forward and backward hemispheres can be fitted by a Gaussian distribution. The multiplicity moments of target evaporated particles emitted in forward and backward hemispheres increase with the order of the moment {\\em q}, and second-order multiplicity moment is energy independent over the entire energy for all the interactions in the forward and backward hemisphere respectively. The scaled variance, a direct measure of multiplicity fluctuations, is close to one for all the interactions which may be said that there is a feeble correlation among the produced particles. The entropy of target evaporated fragments emitted in forward and backward hemispheres are the same within experimental errors, respectively.",
    "We investigate theoretically the temporal behavior of a quantum dot under off-resonant optical excitation targeted at fast acoustic phonon-assisted state preparation. We demonstrate that in a preparation process driven by short laser pulses three processes can be identified: a dressing of the states during the switch on of the laser pulse, a subsequent phonon-induced relaxation and an undressing at the end of the pulse. By analyzing excitation scenarios with different pulse shapes we highlight the decisive impact of an adiabatic undressing on the final state in short pulse protocols. Furthermore, we show that in exciton-biexciton systems the laser characteristics such as the pulse detuning and the pulse length as well as the biexciton binding energy can be used to select the targeted quantum dot state.",
    "In the quantum mechanical Hilbert space formalism, the probabilistic interpretation is a later ad-hoc add-on, more or less enforced by the experimental evidence, but not motivated by the mathematical model itself. A model involving a clear probabilistic interpretation from the very beginning is provided by the quantum logics with unique conditional probabilities. It includes the projection lattices in von Neumann algebras and here probability conditionalization becomes identical with the state transition of the Lueders - von Neumann measurement process. This motivates the definition of a hierarchy of five compatibility and comeasurability levels in the abstract setting of the quantum logics with unique conditional probabilities. Their meanings are: the absence of quantum interference or influence, the existence of a joint distribution, simultaneous measurability, and the independence of the final state after two successive measurements from the sequential order of these two measurements. A further level means that two elements of the quantum logic (events) belong to the same Boolean subalgebra. In the general case, the five compatibility and comeasurability levels appear to differ, but they all coincide in the common Hilbert space formalism of quantum mechanics, in von Neumann algebras, and in some other cases.",
    "An analysis is presented of wave-vector dispersion in elliptically birefringent stratified magneto-optic media having one-dimensional periodicity. It is found that local normal-mode polarization-state differences between adjacent layers lead to mode coupling and impact the wave-vector dispersion and the character of the Bloch states of the system. This coupling produces extra terms in the dispersion relation not present in uniform circularly birefringent magneto-optic stratified media. Normal mode coupling lifts the degeneracy at frequency band cross-over points under certain conditions and induces a magnetization-dependent optical band gap. This study examines the conditions for band gap formation in the system. It shows that such a frequency-split can be characterized by a simple coupling parameter that depends on the relation between polarization states of local normal modes in adjacent layers. The character of the Bloch states and conditions for maximizing the strength of the band splitting in these systems are analyzed.",
    "We study a natural extension of classical empirical risk minimization, where the hypothesis space is a random subspace of a given space. In particular, we consider possibly data dependent subspaces spanned by a random subset of the data, recovering as a special case Nystr\\\"om approaches for kernel methods. Considering random subspaces naturally leads to computational savings, but the question is whether the corresponding learning accuracy is degraded. These statistical-computational tradeoffs have been recently explored for the least squares loss and self-concordant loss functions, such as the logistic loss. Here, we work to extend these results to convex Lipschitz loss functions, that might not be smooth, such as the hinge loss used in support vector machines. This extension requires developing new proofs, that use different technical tools. Our main results show the existence of different settings, depending on how hard the learning problem is, for which computational efficiency can be improved with no loss in performance. Theoretical results are illustrated with simple numerical experiments.",
    "The notion of patient's consent plays a major role in granting access to medical data. In typical healthcare systems, consent is captured by a form that the patient has to fill in and sign. In e-Health systems, the paper-form consent is being replaced by the integration of the notion of consent in the mechanisms that regulate the access to the medical data. This helps in empowering the patient with the capability of granting and revoking consent in a more effective manner. However, the process of granting and revoking consent greatly varies according to the situation in which the patient is. Our main argument is that such a level of detail is very difficult and error-prone to capture as a set of authorisation policies. In this paper, we present ACTORS, a goal-driven approach to manage consent. The main idea behind ACTORS is to leverage the goal-driven approach of Teleo-Reactive (TR) programming for managing consent that takes into account changes regarding the domains and contexts in which the patient is providing her consent.",
    "This paper is concerned with the mathematical analysis of the inverse random source problem for the time fractional diffusion equation, where the source is assumed to be driven by a fractional Brownian motion. Given the random source, the direct problem is to study the stochastic time fractional diffusion equation. The inverse problem is to determine the statistical properties of the source from the expectation and variance of the final time data. For the direct problem, we show that it is well-posed and has a unique mild solution under a certain condition. For the inverse problem, the uniqueness is proved and the instability is characterized. The major ingredients of the analysis are based on the properties of the Mittag--Leffler function and the stochastic integrals associated with the fractional Brownian motion.",
    "Manifold learning methods play a prominent role in nonlinear dimensionality reduction and other tasks involving high-dimensional data sets with low intrinsic dimensionality. Many of these methods are graph-based: they associate a vertex with each data point and a weighted edge with each pair. Existing theory shows that the Laplacian matrix of the graph converges to the Laplace-Beltrami operator of the data manifold, under the assumption that the pairwise affinities are based on the Euclidean norm. In this paper, we determine the limiting differential operator for graph Laplacians constructed using $\\textit{any}$ norm. Our proof involves an interplay between the second fundamental form of the manifold and the convex geometry of the given norm's unit ball. To demonstrate the potential benefits of non-Euclidean norms in manifold learning, we consider the task of mapping the motion of large molecules with continuous variability. In a numerical simulation we show that a modified Laplacian eigenmaps algorithm, based on the Earthmover's distance, outperforms the classic Euclidean Laplacian eigenmaps, both in terms of computational cost and the sample size needed to recover the intrinsic geometry.",
    "We present an efficient integral equation approach to solve the heat equation, $u_t (\\x) - \\Delta u(\\x) = F(\\x,t)$, in a two-dimensional, multiply connected domain, and with Dirichlet boundary conditions. Instead of using integral equations based on the heat kernel, we take the approach of discretizing in time, first. This leads to a non-homogeneous modified Helmholtz equation that is solved at each time step. The solution to this equation is formulated as a volume potential plus a double layer potential.The volume potential is evaluated using a fast multipole-accelerated solver. The boundary conditions are then satisfied by solving an integral equation for the homogeneous modified Helmholtz equation. The integral equation solver is also accelerated by the fast multipole method (FMM). For a total of $N$ points in the discretization of the boundary and the domain, the total computational cost per time step is $O(N)$ or $O(N\\log N)$.",
    "We discuss a scheme in which sequential state-discrimination measurements are performed on qudits to determine the quantum state in which they were initially prepared. The qudits belong to a set of nonorthogonal quantum states and hence cannot be distinguished with certainty. Unambiguous state discrimination allows error-free measurements at the expense of occasionally failing to give a conclusive answer about the state of the qudit. Qudits have the potential to carry more information per transmission than qubits. We considered the situation in which Alice sends one of N qudits, where the dimension of the qudits is also N. We look at two cases, one in which the states all have the same overlap and one in which the qudits are divided into two sets, with qudits in different sets having different overlaps. We also study the robustness of our scheme against a simple eavesdropping attack and found that by using qudits rather than qubits, there is a greater probability that an eavesdropper will introduce errors and be detected.",
    "This work aims to provide a more secure access control in Hyperledger Fabric blockchain by combining multiple ID's, attributes, and policies with the components that regulate access control. The access control system currently used by Hyperledger Fabric is first completely analyzed. Next, a new implementation is proposed that builds upon the existing solution but provides users and developers with easier ways to make access control decisions based on combinations of multiple ID's, attributes, and policies. Our proposed implementation encapsulates the Fabric CA client to facilitate attribute addition and simplify the process of registering and enrolling a newly created certificate (corresponding to a new user). This research, concludes that it is possible to combine multiple ID's, attributes, and policies with the help of Hyperledger Fabric's smart contract technology. Furthermore, it could be seen that the performance impact for real-world applications is negligible compared to the insecure case of always providing access to a resource without performing access control.",
    "This work introduces pyramidal convolution (PyConv), which is capable of processing the input at multiple filter scales. PyConv contains a pyramid of kernels, where each level involves different types of filters with varying size and depth, which are able to capture different levels of details in the scene. On top of these improved recognition capabilities, PyConv is also efficient and, with our formulation, it does not increase the computational cost and parameters compared to standard convolution. Moreover, it is very flexible and extensible, providing a large space of potential network architectures for different applications. PyConv has the potential to impact nearly every computer vision task and, in this work, we present different architectures based on PyConv for four main tasks on visual recognition: image classification, video action classification/recognition, object detection and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baselines. For instance, on image recognition, our 50-layers network outperforms in terms of recognition performance on ImageNet dataset its counterpart baseline ResNet with 152 layers, while having 2.39 times less parameters, 2.52 times lower computational complexity and more than 3 times less layers. On image segmentation, our novel framework sets a new state-of-the-art on the challenging ADE20K benchmark for scene parsing. Code is available at: https://github.com/iduta/pyconv",
    "The status of the solar axion search with the CERN Axion Solar Telescope (CAST) will be discussed. Results from the first part of CAST phase II where the magnet bores were filled with 4He gas at variable pressure in order to scan axion masses up to 0.4 eV will be presented. From the absence of excess X-rays when the magnet was pointing to the Sun, we set a typical upper limit on the axion-photon coupling of g < 2.17 x 10^10 GeV$-1 at 95% CL for axion masses lower than 0.4 eV, the exact result depending on the pressure setting. Our search for axions with masses up to about 1.2 eV using 3He as a buffer gas is in progress in the second part of CAST phase II. Expectations for sensibilities will be given. Near future perspectives as well as more long term options for a new helioscope experiment will be evoked.",
    "Observations indicate that the Arctic sea ice cover is rapidly retreating while the Antarctic sea ice cover is steadily expanding. State-of-the-art climate models, by contrast, typically simulate a moderate decrease in both the Arctic and Antarctic sea ice covers. However, in each hemisphere there is a small subset of model simulations that have sea ice trends similar to the observations. Based on this, a number of recent studies have suggested that the models are consistent with the observations in each hemisphere when simulated internal climate variability is taken into account. Here we examine sea ice changes during 1979-2013 in simulations from the most recent Coupled Model Intercomparison Project (CMIP5) as well as the Community Earth System Model Large Ensemble (CESM-LE), drawing on previous work that found a close relationship in climate models between global-mean surface temperature and sea ice extent. We find that all of the simulations with 1979-2013 Arctic sea ice retreat as fast as observed have considerably more global warming than observations during this time period. Using two separate methods to estimate the sea ice retreat that would occur under the observed level of global warming in each simulation in both ensembles, we find that simulated Arctic sea ice retreat as fast as observed would occur less than 1% of the time. This implies that the models are not consistent with the observations. In the Antarctic, we find that simulated sea ice expansion as fast as observed typically corresponds with too little global warming, although these results are more equivocal. We show that because of this, the simulations do not capture the observed asymmetry between Arctic and Antarctic sea ice trends. This suggests that the models may be getting the right sea ice trends for the wrong reasons in both polar regions.",
    "Bio-features are fast becoming a key tool to authenticate the IoT devices; in this sense, the purpose of this investigation is to summaries the factors that hinder biometrics models' development and deployment on a large scale, including human physiological (e.g., face, eyes, fingerprints-palm, or electrocardiogram) and behavioral features (e.g., signature, voice, gait, or keystroke). The different machine learning and data mining methods used by authentication and authorization schemes for mobile IoT devices are provided. Threat models and countermeasures used by biometrics-based authentication schemes for mobile IoT devices are also presented. More specifically, We analyze the state of the art of the existing biometric-based authentication schemes for IoT devices. Based on the current taxonomy, We conclude our paper with different types of challenges for future research efforts in biometrics-based authentication schemes for IoT devices.",
    "Device fingerprinting over the web has received much attention both by the research community and the commercial market a like. Almost all the fingerprinting features proposed to date depend on software run on the device. All of these features can be changed by the user, thereby thwarting the device's fingerprint. In this position paper we argue that the recent emergence of the HTML5 standard gives rise to a new class of fingerprinting features that are based on the \\emph{hardware} of the device. Such features are much harder to mask or change thus provide a higher degree of confidence in the fingerprint. We propose several possible fingerprint methods that allow a HTML5 web application to identify a device's hardware. We also present an initial experiment to fingerprint a device's GPU.",
    "We present the partition function of Chern-Simons theory with the exceptional gauge group on three-sphere in the form of a partition function of the refined closed topological string with relation $2\\tau=g_s(1-b) $ between single K\\\"ahler parameter $\\tau$, string coupling constant $g_s$ and refinement parameter $b$, where $b=\\frac{5}{3},\\frac{5}{2},3,4,6$ for $G_2, F_4, E_6, E_7, E_8$, respectively. The non-zero BPS invariants $N^d_{J_L,J_R}$ ($d$ - degree) are $N^2_{0,\\frac{1}{2}}=1, N^{11}_{0,1}=1$. Besides these terms, partition function of Chern-Simons theory contains term corresponding to the refined constant maps of string theory. Derivation is based on the universal (in Vogel's sense) form of a Chern-Simons partition function on three-sphere, restricted to exceptional line $Exc$ with Vogel's parameters satisfying $\\gamma=2(\\alpha+\\beta)$. This line contains points, corresponding to the all exceptional groups. The same results are obtained for $F$ line $\\gamma=\\alpha+\\beta$ (containing $SU(4), SO(10)$ and $E_6$ groups), with the non-zero $N^2_{0,\\frac{1}{2}}=1, N^{7}_{0,1}=1$. In both cases refinement parameter $b$ ($=-\\epsilon_2/\\epsilon_1$ in terms of Nekrasov's parameters) is given in terms of universal parameters, restricted to the line, by $b=-\\beta/\\alpha$.",
    "The centerpoint theorem is a well-known and widely used result in discrete geometry. It states that for any point set $P$ of $n$ points in $\\mathbb{R}^d$, there is a point $c$, not necessarily from $P$, such that each halfspace containing $c$ contains at least $\\frac{n}{d+1}$ points of $P$. Such a point $c$ is called a centerpoint, and it can be viewed as a generalization of a median to higher dimensions. In other words, a centerpoint can be interpreted as a good representative for the point set $P$. But what if we allow more than one representative? For example in one-dimensional data sets, often certain quantiles are chosen as representatives instead of the median. We present a possible extension of the concept of quantiles to higher dimensions. The idea is to find a set $Q$ of (few) points such that every halfspace that contains one point of $Q$ contains a large fraction of the points of $P$ and every halfspace that contains more of $Q$ contains an even larger fraction of $P$. This setting is comparable to the well-studied concepts of weak $\\varepsilon$-nets and weak $\\varepsilon$-approximations, where it is stronger than the former but weaker than the latter.",
    "We have produced a new software package for the simulation of pulsar populations, \\textsc{PsrPopPy}, based on the \\textsc{Psrpop} package. The codebase has been re-written in Python (save for some external libraries, which remain in their native Fortran), utilising the object-oriented features of the language, and improving the modularity of the code. Pre-written scripts are provided for running the simulations in `standard' modes of operation, but the code is flexible enough to support the writing of personalised scripts. The modular structure also makes the addition of experimental features (such as new models for period or luminosity distributions) more straightforward than with the previous code. We also discuss potential additions to the modelling capabilities of the software. Finally, we demonstrate some potential applications of the code; first, using results of surveys at different observing frequencies, we find pulsar spectral indices are best fit by a normal distribution with mean $-1.4$ and standard deviation $1.0$. Second, we model pulsar spin evolution to calculate the best-fit for a relationship between a pulsar's luminosity and spin parameters. We used the code to replicate the analysis of Faucher-Gigu\\`ere & Kaspi, and have subsequently optimized their power-law dependence of radio luminosity, $L$, with period, $P$, and period derivative, $\\dot{P}$. We find that the underlying population is best described by $L \\propto P^{-1.39 \\pm 0.09} \\dot{P}^{0.48 \\pm 0.04}$ and is very similar to that found for $\\gamma$-ray pulsars by Perera et al. Using this relationship, we generate a model population and examine the age-luminosity relation for the entire pulsar population, which may be measurable after future large-scale surveys with the Square Kilometer Array.",
    "We study the dynamics of a spin ensemble strongly coupled to a single-mode resonator driven by external pulses. When the mean frequency of the spin ensemble is in resonance with the cavity mode, damped Rabi oscillations are found between the spin ensemble and the cavity mode which we describe very accurately, including the dephasing effect of the inhomogeneous spin broadening. We demonstrate that a precise knowledge of this broadening is crucial both for a qualitative and a quantitative understanding of the temporal spin-cavity dynamics. On this basis we show that coherent oscillations between the spin ensemble and the cavity can be enhanced by a few orders of magnitude, when driving the system with pulses that match special resonance conditions. Our theoretical approach is tested successfully with an experiment based on an ensemble of negatively charged nitrogen-vacancy (NV) centers in diamond strongly coupled to a superconducting coplanar single-mode waveguide resonator.",
    "We investigate the ground-state Riemannian metric and the cyclic quantum distance of an inhomogeneous quantum Ising spin-1/2 chain in a transverse field. This model can be diagonalized by using a general canonical transformation to the fermionic Hamiltonian mapped from the spin system. The ground-state Riemannian metric is derived exactly on a parameter manifold ring $S^1$, which is introduced by performing a gauge transformation to the spin Hamiltonian through a twist operator. The ground-state cyclic quantum distance and the second derivative of the ground-state energy are studied in different inhomogeneous exchange coupling parameter region. Particularly, we show that the quantum ferromagnetic phase in the uniform Ising chain can be characterized by an invariant cyclic quantum distance with a constant ground-state Riemannian metric, and this metric will rapidly decay to zero in the paramagnetic phase.",
    "Rotation measure synthesis allows the estimation of Faraday dispersion via a Fourier transform and is the primary tool to probe cosmic magnetic fields. We show this can be considered mathematically equivalent to the one dimensional interferometric intensity measurement equation, albeit in a different Fourier space. As a result, familiar concepts in two dimensional intensity interferometry designed to correctly account for a range of instrumental conditions can be translated to the analysis of Faraday dispersion. In particular, we show how to model the effect of channel averaging during Faraday reconstruction, which has to date limited the progress of polarimetic science using wide-band measurements. Further, we simulate 1d sparse reconstruction with channel averaging for realistic frequency coverages, and show that it is possible to recover signals with large rotation measure values that were previously excluded from possible detection. This is especially important for low-frequency and wide-band polarimetry. We extended these ideas to introduce mosaicking in Faraday depth into the channel averaging process. This work, thus provides the first framework for correctly undertaking wide-band rotation measure synthesis, including the provision to add data from multiple telescopes, a prospect that should vastly improve the quality and quantity of polarimetric science. This is of particular importance for extreme environments which generate high magnetic fields such as those associated with pulsars and Fast Radio Bursts (FRBs), and will allow such sources to be accurately used as probes of cosmological fields.",
    "Study of the characteristic properties of charged particle production in hadron-nucleus collisions at high energies, by utilising the approaches from different statistical models is performed.~Predictions from different approaches using the Negative Binomial distribution, shifted Gompertz distribution, Weibull distribution and the Krasznovszky-Wagner distribution are utilised for a comparative study of the relative successes of these models.~These distributions derived from a variety of functional forms are based on either phenomenological parameterizations or some model of the underlying dynamics.~Some of these have have also been used to study the data at the LHC for both proton-proton and nucleus-nucleus collisions.~Various physical and derived observables have been used for the analysis.",
    "In 1975 John Tukey proposed a multivariate median which is the 'deepest' point in a given data cloud in R^d. Later, in measuring the depth of an arbitrary point z with respect to the data, David Donoho and Miriam Gasko considered hyperplanes through z and determined its 'depth' by the smallest portion of data that are separated by such a hyperplane. Since then, these ideas has proved extremely fruitful. A rich statistical methodology has developed that is based on data depth and, more general, nonparametric depth statistics. General notions of data depth have been introduced as well as many special ones. These notions vary regarding their computability and robustness and their sensitivity to reflect asymmetric shapes of the data. According to their different properties they fit to particular applications. The upper level sets of a depth statistic provide a family of set-valued statistics, named depth-trimmed or central regions. They describe the distribution regarding its location, scale and shape. The most central region serves as a median. The notion of depth has been extended from data clouds, that is empirical distributions, to general probability distributions on R^d, thus allowing for laws of large numbers and consistency results. It has also been extended from d-variate data to data in functional spaces.",
    "Strain-engineering in SiGe nanostructures is fundamental for the design of optoelectronic devices at the nanoscale. Here we explore a new strategy, where SiGe structures are laterally confined by the Si substrate, to obtain high tensile strain avoiding the use of external stressors, and thus improving the scalability. Spectro-microscopy techniques, finite element method simulations and ab initio calculations are used to investigate the strain state of laterally confined Ge-rich SiGe nano-stripes. Strain information is obtained by tip enhanced Raman spectroscopy with an unprecedented lateral resolution of ~ 30 nm. The nano-stripes exhibit a large tensile hydrostatic strain component, which is maximum at the center of the top free surface, and becomes very small at the edges. The maximum lattice deformation is larger than the typical values of thermally relaxed Ge/Si(001) layers. This strain enhancement originates from a frustrated relaxation in the out-of-plane direction, resulting from the combination of the lateral confinement induced by the substrate side walls and the plastic relaxation of the misfit strain in the (001) plane at the SiGe/Si interface. The effect of this tensile lattice deformation at the stripe surface is probed by work function mapping, performed with a spatial resolution better than 100 nm using X-ray photoelectron emission microscopy. The nano-stripes exhibit a positive work function shift with respect to a bulk SiGe alloy, quantitatively confirmed by electronic structure calculations of tensile strained configurations. The present results have a potential impact on the design of optoelectronic devices at a nanometer length scale.",
    "During an infectious disease pandemic, it is critical to share electronic medical records or models (learned from these records) across regions. Applying one region's data/model to another region often have distribution shift issues that violate the assumptions of traditional machine learning techniques. Transfer learning can be a solution. To explore the potential of deep transfer learning algorithms, we applied two data-based algorithms (domain adversarial neural networks and maximum classifier discrepancy) and model-based transfer learning algorithms to infectious disease detection tasks. We further studied well-defined synthetic scenarios where the data distribution differences between two regions are known. Our experiments show that, in the context of infectious disease classification, transfer learning may be useful when (1) the source and target are similar and the target training data is insufficient and (2) the target training data does not have labels. Model-based transfer learning works well in the first situation, in which case the performance closely matched that of the data-based transfer learning models. Still, further investigation of the domain shift in real world research data to account for the drop in performance is needed.",
    "Bound states in the continuum (BIC) have been at the forefront of research in optics and photonics over the past decade. It is of great interest to study the effects associated with quasi-BICs in the simplest structures, where quasi-BICs are very pronounced. An example is a dielectric cylinder, and in a number of works, quasi-BICs have been studied both in single cylinders and in structures composed of cylinders. In this work, we studied the properties of quasi-BICs during the transition from a homogeneous dielectric cylinder in an air environment to a ring with narrow walls while increasing the diameter of the inner air cylinder gradually. The results demonstrate the quasi-BIC crossover from the strong-coupling to the weak-coupling regime, which manifests itself in the transition from avoided crossing of branches to their intersection with the quasi-BIC being preserved on only one straight branch. In the regime of strong-coupling and quasi-BIC, three waves interfere in the far-field zone: two waves corresponding to the resonant modes of the structure and the wave scattered by the structure as a whole. The validity of the Fano resonance concept is discussed, since it describes the interference of only two waves under weak coupling conditions.",
    "Turbulent thermal diffusion is a combined effect of the temperature stratified turbulence and inertia of small particles. It causes the appearance of a non-diffusive turbulent flux of particles in the direction of the turbulent heat flux. This non-diffusive turbulent flux of particles is proportional to the product of the mean particle number density and the effective velocity of inertial particles. The theory of this effect has been previously developed only for small temperature gradients and small Stokes numbers (Phys. Rev. Lett. {\\bf 76}, 224, 1996). In this study a generalized theory of turbulent thermal diffusion for arbitrary temperature gradients and Stokes numbers has been developed. The laboratory experiments in the oscillating grid turbulence and in the multi-fan produced turbulence have been performed to validate the theory of turbulent thermal diffusion in strongly stratified turbulent flows. It has been shown that the ratio of the effective velocity of inertial particles to the characteristic vertical turbulent velocity for large Reynolds numbers is less than 1. The effective velocity of inertial particles as well as the effective coefficient of turbulent thermal diffusion increase with Stokes numbers reaching the maximum at small Stokes numbers and decreases for larger Stokes numbers. The effective coefficient of turbulent thermal diffusion also decreases with the mean temperature gradient. It has been demonstrated that the developed theory is in a good agreement with the results of the laboratory experiments.",
    "A standard model for the visibility of pulsar radio emission is based on the assumption that the emission is confined to a narrow cone about the tangent to a dipolar field line. The widely accepted rotating vector model (RVM) is an approximation in which the line of sight is fixed and the field line is not strictly tangent to it. We refer to an exact treatment (Gangadhara 2004) as the tangent model. In the tangent model (but not in the RVM) the visible point changes as a function of pulsar rotational phase, $\\psi$, defining a trajectory on a sphere of radius $r$. We solve for the trajectory and for the angular velocity of the visible point around it. We note the recent claim that this motion is observable using interstellar holography (Pen et al. 2014). We estimate the error introduced by use of the RVM and find that it is significant for pulsars with emission over a wide range of $\\psi$. The RVM tends to underestimate the range of $\\psi$ over which emission is visible. We suggest that the geometry alone strongly favors the visible pulsar radio being emitted at a heights more than ten percent of the light-cylinder distance, where our neglect of retardation effects becomes significant.",
    "In image recognition, there are many cases where training samples cannot cover all target classes. Zero-shot learning (ZSL) utilizes the class semantic information to classify samples of the unseen categories that have no corresponding samples contained in the training set. In this paper, we propose an end-to-end framework, called Global Semantic Consistency Network (GSC-Net for short), which makes complete use of the semantic information of both seen and unseen classes, to support effective zero-shot learning. We also adopt a soft label embedding loss to further exploit the semantic relationships among classes. To adapt GSC-Net to a more practical setting, Generalized Zero-shot Learning (GZSL), we introduce a parametric novelty detection mechanism. Our approach achieves the state-of-the-art performance on both ZSL and GZSL tasks over three visual attribute datasets, which validates the effectiveness and advantage of the proposed framework.",
    "The popular view according to which Category theory provides a support for Mathematical Structuralism is erroneous. Category-theoretic foundations of mathematics require a different philosophy of mathematics. While structural mathematics studies invariant forms (Awodey) categorical mathematics studies covariant transformations which, generally, don t have any invariants. In this paper I develop a non-structuralist interpretation of categorical mathematics and show its consequences for history of mathematics and mathematics education.",
    "We show that in a non-equilibrium system of an exciton-polariton condensate, where polaritons are generated from incoherent pumping, a ring-shaped pump allows for stationary vortex memory elements of topological charge $m = 1$ or $m = -1$. Using simple potential guides we can choose whether to copy the same charge or invert it onto another spatially separate ring pump. Such manipulation of binary information opens the possibility of a new type processing using vortices as topologically protected memory components.",
    "During the three years long assessment phase of the LOFT mission, candidate to the M3 launch opportunity of the ESA Cosmic Vision programme, we estimated and measured the radiation damage of the silicon drift detectors (SDDs) of the satellite instrumentation. In particular, we irradiated the detectors with protons (of 0.8 and 11 MeV energy) to study the increment of leakage current and the variation of the charge collection efficiency produced by the displacement damage, and we \"bombarded\" the detectors with hypervelocity dust grains to measure the effect of the debris impacts. In this paper we describe the measurements and discuss the results in the context of the LOFT mission.",
    "In this paper we examine the ability of low-level multimodal features to extract movie similarity, in the context of a content-based movie recommendation approach. In particular, we demonstrate the extraction of multimodal representation models of movies, based on textual information from subtitles, as well as cues from the audio and visual channels. With regards to the textual domain, we emphasize our research in topic modeling of movies based on their subtitles, in order to extract topics that discriminate between movies. Regarding the visual domain, we focus on the extraction of semantically useful features that model camera movements, colors and faces, while for the audio domain we adopt simple classification aggregates based on pretrained models. The three domains are combined with static metadata (e.g. directors, actors) to prove that the content-based movie similarity procedure can be enhanced with low-level multimodal information. In order to demonstrate the proposed content representation approach, we have built a small dataset of 160 widely known movies. We assert movie similarities, as propagated by the individual modalities and fusion models, in the form of recommendation rankings. Extensive experimentation proves that all three low-level modalities (text, audio and visual) boost the performance of a content-based recommendation system, compared to the typical metadata-based content representation, by more than $50\\%$ relative increase. To our knowledge, this is the first approach that utilizes a wide range of features from all involved modalities, in order to enhance the performance of the content similarity estimation, compared to the metadata-based approaches.",
    "We study black hole radiation of a Reissner-Nordstrom black hole with an electric charge in the framework of quantum gravity. Based on a canonical quantization for a spherically symmetric geometry, under physically plausible assumptions, we solve the Wheeler-De Witt equation in the regions not only between the outer apparent horizon and the spatial infinity but also between the spacetime singularity and the inner apparent horizon, and then show that the mass loss rate of an evaporating black hole due to thermal radiation agrees with the semiclassical result when we choose an integration constant properly by physical reasoning. Furthermore, we also solve the Wheeler-De Witt equation in the region between the inner Cauchy horizon and the outer apparent horizon, and show that the mass loss rate of an evaporating black hole has the same expression. The present study is the natural generalization of the case of a Schwarzschild black hole to that of a charged Reissner-Nordstrom black hole.",
    "We present multi-agent A* (MAA*), the first complete and optimal heuristic search algorithm for solving decentralized partially-observable Markov decision problems (DEC-POMDPs) with finite horizon. The algorithm is suitable for computing optimal plans for a cooperative group of agents that operate in a stochastic environment such as multirobot coordination, network traffic control, `or distributed resource allocation. Solving such problems efiectively is a major challenge in the area of planning under uncertainty. Our solution is based on a synthesis of classical heuristic search and decentralized control theory. Experimental results show that MAA* has significant advantages. We introduce an anytime variant of MAA* and conclude with a discussion of promising extensions such as an approach to solving infinite horizon problems.",
    "We present morphological classifications obtained using machine learning for objects in SDSS DR6 that have been classified by Galaxy Zoo into three classes, namely early types, spirals and point sources/artifacts. An artificial neural network is trained on a subset of objects classified by the human eye and we test whether the machine learning algorithm can reproduce the human classifications for the rest of the sample. We find that the success of the neural network in matching the human classifications depends crucially on the set of input parameters chosen for the machine-learning algorithm. The colours and parameters associated with profile-fitting are reasonable in separating the objects into three classes. However, these results are considerably improved when adding adaptive shape parameters as well as concentration and texture. The adaptive moments, concentration and texture parameters alone cannot distinguish between early type galaxies and the point sources/artifacts. Using a set of twelve parameters, the neural network is able to reproduce the human classifications to better than 90% for all three morphological classes. We find that using a training set that is incomplete in magnitude does not degrade our results given our particular choice of the input parameters to the network. We conclude that it is promising to use machine- learning algorithms to perform morphological classification for the next generation of wide-field imaging surveys and that the Galaxy Zoo catalogue provides an invaluable training set for such purposes.",
    "The Lambek calculus is a well-known logical formalism for modelling natural language syntax. The original calculus covered a substantial number of intricate natural language phenomena, but only those restricted to the context-free setting. In order to address more subtle linguistic issues, the Lambek calculus has been extended in various ways. In particular, Morrill and Valentin (2015) introduce an extension with so-called exponential and bracket modalities. Their extension is based on a non-standard contraction rule for the exponential that interacts with the bracket structure in an intricate way. The standard contraction rule is not admissible in this calculus. In this paper we prove undecidability of the derivability problem in their calculus. We also investigate restricted decidable fragments considered by Morrill and Valentin and we show that these fragments belong to the NP class.",
    "The transition between the two phases of 4D Euclidean Dynamical Triangulation [1] was long believed to be of second order until in 1996 first order behavior was found for sufficiently large systems [5,9]. However, one may wonder if this finding was affected by the numerical methods used: to control volume fluctuations, in both studies [5,9] an artificial harmonic potential was added to the action; in [9] measurements were taken after a fixed number of accepted instead of attempted moves which introduces an additional error. Finally the simulations suffer from strong critical slowing down which may have been underestimated. In the present work, we address the above weaknesses: we allow the volume to fluctuate freely within a fixed interval; we take measurements after a fixed number of attempted moves; and we overcome critical slowing down by using an optimized parallel tempering algorithm [12]. With these improved methods, on systems of size up to 64k 4-simplices, we confirm that the phase transition is first order. In addition, we discuss a local criterion to decide whether parts of a triangulation are in the elongated or crumpled state and describe a new correspondence between EDT and the balls in boxes model. The latter gives rise to a modified partition function with an additional, third coupling. Finally, we propose and motivate a class of modified path-integral measures that might remove the metastability of the Markov chain and turn the phase transition into second order.",
    "We characterize the virtually nilpotent finitely generated groups (or, equivalently by Gromov's theorem, groups of polynomial growth) for which the Domino Problem is decidable: These are the virtually free groups, i.e. finite groups, and those having $\\Z$ as a subgroup of finite index.",
    "Gamma rays from the annihilation of dark matter particles in the Galactic halo provide a particularly promising means of indirectly detecting dark matter. Here, we demonstrate that pronounced spectral features at energies near the dark matter particles' mass, which are a generic prediction for most models, can significantly improve the sensitivity of gamma-ray telescopes to dark matter signals. We derive projected limits on such features (including the traditionally looked-for line signals) and show that they can be much more efficient in constraining the nature of dark matter than the model-independent broad spectral features expected at lower energies.",
    "The attainment of carbon neutrality requires a research agenda that addresses the technical and economic challenges that will be encountered as we progress toward 100% renewable electricity generation. Increasing proportions of variable renewable energy (VRE) sources (such as wind turbines and photovoltaic systems) render the supply-and-demand balance of VRE-dominated power grids difficult. The operational characteristics and effects of VRE inverters also require attention. Here, we examine the implications of the paradigm shift to carbon neutrality and summarize the associated research challenges in terms of system planning, operation, and sta-bility, and the need for energy storage integration, demand-side participation, distributed con-trol and estimation, and energy sector coupling. We also highlight the existing literature gaps, and our recent studies that can fill in the gaps, thereby facilitating the improvement of grid op-eration and estimation. The numerical results of comparative case studies are also provided on the operational stability and economics of power grids with a high level of VRE sources, assist-ing stakeholders in establishing specific roadmaps and making relevant decisions.",
    "Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to \"absorb\" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers. We present a novel network architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to drastically better compressed performance than several relevant baselines.",
    "We conducted a Project-Based Learning (PBL)-type exercise incorporating Japanese cartoon (manga) techniques into Requirements Development (RD) processes. Manga has established techniques, such as those for character setting and story development, that we thought are also valid for RD processes. Using this manga-driven method, students were able to clarify high-level project goals early in the development life-cycle, and succeeded in defining high quality and unique system ideas.",
    "The standard Hawking formula predicts the complete evaporation of black holes. Taking into account effects of quantum gravity, we investigate fermions' tunnelling from a 5-dimensional rotating black string. The temperature is determined not only by the string, but also affected by the quantum number of the emitted fermion and the effect of the extra spatial dimension. The quantum correction slows down the increase of the temperature, which naturally leads to the remnant in the evaporation.",
    "We introduce second-order vector representations of words, induced from nearest neighborhood topological features in pre-trained contextual word embeddings. We then analyze the effects of using second-order embeddings as input features in two deep natural language processing models, for named entity recognition and recognizing textual entailment, as well as a linear model for paraphrase recognition. Surprisingly, we find that nearest neighbor information alone is sufficient to capture most of the performance benefits derived from using pre-trained word embeddings. Furthermore, second-order embeddings are able to handle highly heterogeneous data better than first-order representations, though at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information further improves model performance in some cases. Due to variance in the random initializations of word embeddings, utilizing nearest neighbor features from multiple first-order embedding samples can also contribute to downstream performance gains. Finally, we identify intriguing characteristics of second-order embedding spaces for further research, including much higher density and different semantic interpretations of cosine similarity.",
    "RIS-aided millimeter wave wireless systems benefit from robustness to blockage and enhanced coverage. In this paper, we study the ability of RIS to also provide enhanced localization capabilities as a by-product of communication. We consider sparse reconstruction algorithms to obtain high resolution channel estimates that are mapped to position information. In RIS-aided mmWave systems, the complexity of sparse recovery becomes a bottleneck, given the large number of elements of the RIS and the large communication arrays. We propose to exploit a multidimensional orthogonal matching pursuit strategy for compressive channel estimation in a RIS-aided millimeter wave system. We show how this algorithm, based on computing the projections on a set of independent dictionaries instead of a single large dictionary, enables high accuracy channel estimation at reduced complexity. We also combine this strategy with a localization approach which does not rely on the absolute time of arrival of the LoS path. Localization results in a realistic 3D indoor scenario show that RIS-aided wireless system can also benefit from a significant improvement in localization accuracy.",
    "Detection and quantification of information leaks through timing side channels are important to guarantee confidentiality. Although static analysis remains the prevalent approach for detecting timing side channels, it is computationally challenging for real-world applications. In addition, the detection techniques are usually restricted to 'yes' or 'no' answers. In practice, real-world applications may need to leak information about the secret. Therefore, quantification techniques are necessary to evaluate the resulting threats of information leaks. Since both problems are very difficult or impossible for static analysis techniques, we propose a dynamic analysis method. Our novel approach is to split the problem into two tasks. First, we learn a timing model of the program as a neural network. Second, we analyze the neural network to quantify information leaks. As demonstrated in our experiments, both of these tasks are feasible in practice --- making the approach a significant improvement over the state-of-the-art side channel detectors and quantifiers. Our key technical contributions are (a) a neural network architecture that enables side channel discovery and (b) an MILP-based algorithm to estimate the side-channel strength. On a set of micro-benchmarks and real-world applications, we show that neural network models learn timing behaviors of programs with thousands of methods. We also show that neural networks with thousands of neurons can be efficiently analyzed to detect and quantify information leaks through timing side channels.",
    "The inner asteroid belt between 2.1 and 2.5 au is of particular dynamical significance because it is the dominant source of both chondritic meteorites and near-Earth asteroids. This inner belt is bounded by an eccentricity-type secular resonance and by the 1:3 mean motion resonance with Jupiter. Unless asteroid perihelia are low enough to allow scattering by Mars, escape requires transport to one of the bounding resonances. In addition Yarkovsky forces are generally ineffective in changing either the eccentricity and/or inclination for asteroids with diameter $\\gtrsim$30 km. Thus, large asteroids with pericentres far from Mars may only escape from the inner belt through large changes in their eccentricities. In this paper we study chaotic diffusion of orbits near the 1:2 mean motion resonance with Mars in a systematic way. We show that, while chaotic orbital evolution in both resonant and non-resonant orbits increase the dispersion of the inclinations and eccentricities, it does not significantly change their mean values. We show further that, while the dispersive growth is greatest for resonant orbits, at high $e$ the resonance acts to mitigate asteroid scattering by Mars - making the asteroid lifetime in the belt longer than it would have been for a non-resonant orbit. For asteroids of all sizes in both resonant and non-resonant orbits, the changes in eccentricity needed to account for the observations cannot be achieved by gravitational forces alone. The role of resonant trapping in protecting asteroids from encounters with Mars is also analysed.",
    "The presence of nonstandard neutrino interactions (NSI) has a large effect on the precision measurements at next generation neutrino oscillation experiments. Other type of experiments are needed to constrain the NSI parameter space. We study the constraints on NSI with electrons from current and future $e^+e^-$ collider experiments including Belle II, STCF and CEPC. We find that Belle II and STCF will provide competitive and complementary bounds on electron-type NSI parameters compared to the current global analysis, and strong improvements for the constraints on tau-type NSI. In addition, CEPC alone will impose stringent constraints on the parameter space of NSI with electrons. We find that the degeneracy between the left-handed (vector) and right-handed (axial-vector) NSI parameters can be lifted by combining the data from three different running modes, and the allowed ranges for $|\\epsilon_{ee}^{eL}|$ ($|\\epsilon_{ee}^{eV}|$) and $|\\epsilon_{ee}^{eR}|$ ($|\\epsilon_{ee}^{eA}|$) can be constrained to be smaller than 0.002 at CEPC even if both of them are present.",
    "The Deep Underground Neutrino Experiment (DUNE) is a leading-edge experiment designed to perform neutrino science and proton decay searches. In particular, the far detector will consist of four 10-kton Liquid Argon (LAr) Time Projection Chambers using both single and dual-phase technologies. The latter provides charge amplification in the gaseous phase. In order to optimize these designs, two large prototypes are taking data at CERN since 2018. Previously, a dual-phase 4-tonne demonstrator was constructed and exposed to cosmic muons in 2017 and exhibited good performance in terms of charge and light collection. The light detection system is important to provide a trigger to the charge acquisition system and to obtain additional information from the scintillation light produced in the particle interaction. In the demonstrator, five cryogenic photo-multipliers were installed with different base polarity configurations and wavelength shifting methods. During the detector operation, scintillation light data were collected in different drift and amplification field conditions. An overview of the light detection system performance and results on the light production and propagation are presented. Our studies allowed to improve the understanding of some LAr properties.",
    "Major chip manufacturers have all introduced Multithreaded processors. These processors are used for running a variety of workloads. Efficient resource utilization is an important design aspect in such processors. Particularly, it is important to take advantage of available memory-level parallelism(MLP). In this paper I propose a MLP aware operating system (OS) scheduling algorithm for Multithreaded Multi-core processors. By observing the MLP available in each thread and by balancing it with available MLP resources in the system the OS will come up with a new schedule of threads for the next quantum that could potentially improve overall performance. We do a qualitative comparison of our solution with other hardware and software techniques. This work can be extended by doing a quantitative evaluation and by further refining the scheduling optimization.",
    "We consider the problem of calibrating a compressed sensing measurement system under the assumption that the decalibration consists in unknown gains on each measure. We focus on {\\em blind} calibration, using measures performed on a few unknown (but sparse) signals. A naive formulation of this blind calibration problem, using $\\ell_{1}$ minimization, is reminiscent of blind source separation and dictionary learning, which are known to be highly non-convex and riddled with local minima. In the considered context, we show that in fact this formulation can be exactly expressed as a convex optimization problem, and can be solved using off-the-shelf algorithms. Numerical simulations demonstrate the effectiveness of the approach even for highly uncalibrated measures, when a sufficient number of (unknown, but sparse) calibrating signals is provided. We observe that the success/failure of the approach seems to obey sharp phase transitions.",
    "We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder- decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.",
    "An increasing number of use cases require a timely extraction of non-trivial knowledge from semantically annotated data streams, especially on the Web and for the Internet of Things (IoT). Often, this extraction requires expressive reasoning, which is challenging to compute on large streams. We propose Laser, a new reasoner that supports a pragmatic, non-trivial fragment of the logic LARS which extends Answer Set Programming (ASP) for streams. At its core, Laser implements a novel evaluation procedure which annotates formulae to avoid the re-computation of duplicates at multiple time points. This procedure, combined with a judicious implementation of the LARS operators, is responsible for significantly better runtimes than the ones of other state-of-the-art systems like C-SPARQL and CQELS, or an implementation of LARS which runs on the ASP solver Clingo. This enables the application of expressive logic-based reasoning to large streams and opens the door to a wider range of stream reasoning use cases.",
    "The second law of thermodynamics dictates the fundamental limits to the amount of energy and information that can be exchanged between physical systems. In this work, we extend a thermodynamic formalism describing this flow of energy and information developed for a pair of bipartite systems to many multipartite systems. We identify a natural thermodynamic quantity that describes the information exchanged among these systems. We then introduce and discuss a refined version. Our results are illustrated with a model of two, competing Maxwell demons.",
    "The negligible intrinsic spin-orbit coupling (SOC) in graphene can be enhanced by proximity effects in stacked heterostructures of graphene and transition metal dichalcogenides (TMDCs). The composition of the TMDC layer plays a key role in determining the nature and strength of the resultant SOC induced in the graphene layer. Here, we study the evolution of the proximity-induced SOC as the TMDC layer is deliberately defected. Alloyed ${\\rm G/W_{\\chi}Mo_{1-\\chi}Se_2}$ heterostructures with diverse compositions ($\\chi$) and defect distributions are simulated using density functional theory. Comparison with continuum and tight-binding models allows both local and global signatures of the metal-atom alloying to be clarified. Our findings show that, despite some dramatic perturbation of local parameters for individual defects, the low-energy spin and electronic behaviour follow a simple effective medium model which depends only on the composition ratio of the metallic species in the TMDC layer. Furthermore, we demonstrate that the topological state of such alloyed systems can be feasibly tuned by controlling this ratio.",
    "Atomic masses play a crucial role in many nuclear astrophysics calculations. The lack of experimental values for relevant exotic nuclides triggered a rapid development of new mass measurement devices around the world. The Time-of-Flight (TOF) mass measurements offer a complementary technique to the most precise one, Penning trap measurements, the latter being limited by the rate and half-lives of the ions of interest. The NSCL facility provides a well-suited infrastructure for TOF mass measurements of very exotic nuclei. At this facility, we have recently implemented a TOF-Brho technique and performed mass measurements of neutron-rich nuclides in the Fe region, important for r-process calculations and for calculations of processes occurring in the crust of accreting neutron stars.",
    "Although the super-massive (AGN) and stellar mass (XRBs) black holes have many properties in common, the broad emission lines (BELs) are exclusively signatures of the AGN. Based on the detection of these lines from SDSS data bases, there seems to be no AGN with mass M_BH \\lesssim 10^5 M_sun. In this paper we investigate if such low mass black holes are really non-existent or they are undetected because the BELs in them are not produced efficiently. Using the ionizing spectral energy distribution for a wide range of black hole mass, 10 - 10^9 M_sun, spanning XRBs to AGN, we calculate the equivalent widths (EWs) of ultraviolet and optical lines Ly\\alpha 1216 \\AA, H\\beta 4861 \\AA, CIV 1549 \\AA and MgII 2798 \\AA. The LOC (locally optimally emitting cloud) model has been used to describe the broad emission line region (BELR) for the calculations. We find that the hardening of the SED shape with decreasing mass do not decrease the BEL EWs. However, finite size of the BELR, as measured by the line widths, which is controlled by the mass of the black hole, regulates the production of these emission lines. There seems to be a peak in the EWs of the emission lines for typical AGN black holes of ~ 10^8 M_sun, below which the lines become intrinsically fainter with a sharp fall-off below ~ 10^6 M_sun. This may be the cause of the absence of low mass AGN in SDSS.",
    "The precision of synchronization algorithms based on the theory of pulse-coupled oscillators is evaluated on FPGA-based radios for the first time. Measurements show that such algorithms can reach precision in the low microsecond range when being implemented in the physical layer. Furthermore, we propose an algorithm extension accounting for phase rate deviations of the hardware and show that an improved precision below one microsecond is possible with this extension in the given setup. The resulting algorithm can thus be applied in ad hoc wireless systems for fully distributed synchronization of transmission slots or sleep cycles, in particular, if centralized synchronization is impossible.",
    "Human Trajectory Prediction (HTP) has gained much momentum in the last years and many solutions have been proposed to solve it. Proper benchmarking being a key issue for comparing methods, this paper addresses the question of evaluating how complex is a given dataset with respect to the prediction problem. For assessing a dataset complexity, we define a series of indicators around three concepts: Trajectory predictability; Trajectory regularity; Context complexity. We compare the most common datasets used in HTP in the light of these indicators and discuss what this may imply on benchmarking of HTP algorithms. Our source code is released on Github.",
    "The purpose of this paper is to show how a class of classical linear stochastic systems can be physically implemented using quantum optical components. Quantum optical systems typically have much higher bandwidth than electronic devices, meaning faster response and processing times, and hence has the potential for providing better performance than classical systems. A procedure is provided for constructing the quantum optical realization. The paper also describes the use of the quantum optical realization in a measurement feedback loop. Some examples are given to illustrate the application of the main results.",
    "Systems biology uses large networks of biochemical reactions to model the functioning of biological cells from the molecular to the cellular scale. The dynamics of dissipative reaction networks with many well separated time scales can be described as a sequence of successive equilibrations of different subsets of variables of the system. Polynomial systems with separation are equilibrated when at least two monomials, of opposite signs, have the same order of magnitude and dominate the others. These equilibrations and the corresponding truncated dynamics, obtained by eliminating the dominated terms, find a natural formulation in tropical analysis and can be used for model reduction.",
    "We performed spectral analysis of Suzaku data of the galactic disk and outflow regions of the starburst galaxy M82. Thermal modeling of the central disk regions requires at least three temperature components. The Ly$\\beta$ line fluxes of O VIII and Ne X exceed those expected from a plasma in collisional ionization equilibrium. The ratios of Ly$\\beta$/Ly$\\alpha$ lines for O VIII and Ne X are higher than those of collisional ionization equilibrium, which may be caused by the process of charge exchange. In the outflow wind region, the spectra are well reproduced with two-temperature thermal models, and we have derived the metal abundances of O, Ne, Mg, and Fe in the outflow. The ratios of O/Fe, Ne/Fe, and Mg/Fe are about 2, 3, and 2, respectively, relative to the solar value determined by Lodders (2003). Since there is no evidence of charge exchange in outflow region, the metal abundances should be more reliable than those in the central region. This abundance pattern indicates that starburst activity enriches the outflow through SN II metal ejection into intergalactic space.",
    "Dust grains are classically thought to form in the winds of asymptotic giant branch (AGB) stars. However, there is increasing evidence today for dust formation in supernovae (SNe). To establish the relative importance of these two classes of stellar sources of dust, it is important to know the fraction of freshly formed dust in SN ejecta that is able to survive the passage of the reverse shock and be injected in the interstellar medium. We have developed a new code (GRASH\\_Rev) which follows the newly-formed dust evolution throughout the supernova explosion until the merging of the forward shock with the circumstellar ISM. We have considered four well studied SNe in the Milky Way and Large Magellanic Cloud: SN1987A, CasA, the Crab Nebula, and N49. For all the simulated models, we find good agreement with observations and estimate that between 1 and 8$\\%$ of the observed mass will survive, leading to a SN dust production rate of $(3.9 \\pm 3.7) \\times 10^{-4}$ M$_{\\odot}$yr$^{-1}$ in the Milky Way. This value is one order of magnitude larger than the dust production rate by AGB stars but insufficient to counterbalance the dust destruction by SNe, therefore requiring dust accretion in the gas phase.",
    "This paper investigates in hatching process strategies for additive manufacturing using an electron beam by numerical simulations. The underlying physical model and the corresponding three dimensional thermal free surface lattice Boltzmann method of the simulation software are briefly presented. The simulation software has already been validated on the basis of experiments up to 1.2 kW beam power by hatching a cuboid with a basic process strategy, whereby the results are classified into `porous', `good' and `uneven', depending on their relative density and top surface smoothness. In this paper we study the limitations of this basic process strategy in terms of higher beam powers and scan velocities to exploit the future potential of high power electron beam guns up to 10 kW. Subsequently, we introduce modified process strategies, which circumvent these restrictions, to build the part as fast as possible under the restriction of a fully dense part with a smooth top surface. These process strategies are suitable to reduce the build time and costs, maximize the beam power usage and therefore use the potential of high power electron beam guns.",
    "Bayesian optimization (BO) is a class of global optimization algorithms, suitable for minimizing an expensive objective function in as few function evaluations as possible. While BO budgets are typically given in iterations, this implicitly measures convergence in terms of iteration count and assumes each evaluation has identical cost. In practice, evaluation costs may vary in different regions of the search space. For example, the cost of neural network training increases quadratically with layer size, which is a typical hyperparameter. Cost-aware BO measures convergence with alternative cost metrics such as time, energy, or money, for which vanilla BO methods are unsuited. We introduce Cost Apportioned BO (CArBO), which attempts to minimize an objective function in as little cost as possible. CArBO combines a cost-effective initial design with a cost-cooled optimization phase which depreciates a learned cost model as iterations proceed. On a set of 20 black-box function optimization problems we show that, given the same cost budget, CArBO finds significantly better hyperparameter configurations than competing methods.",
    "This work contributes a marsupial robotic system-of-systems involving a legged and an aerial robot capable of collaborative mapping and exploration path planning that exploits the heterogeneous properties of the two systems and the ability to selectively deploy the aerial system from the ground robot. Exploiting the dexterous locomotion capabilities and long endurance of quadruped robots, the marsupial combination can explore within large-scale and confined environments involving rough terrain. However, as certain types of terrain or vertical geometries can render any ground system unable to continue its exploration, the marsupial system can - when needed - deploy the flying robot which, by exploiting its 3D navigation capabilities, can undertake a focused exploration task within its endurance limitations. Focusing on autonomy, the two systems can co-localize and map together by sharing LiDAR-based maps and plan exploration paths individually, while a tailored graph search onboard the legged robot allows it to identify where and when the ferried aerial platform should be deployed. The system is verified within multiple experimental studies demonstrating the expanded exploration capabilities of the marsupial system-of-systems and facilitating the exploration of otherwise individually unreachable areas.",
    "We propose a method for polarising antiprotons in a storage ring by means of a polarised positron beam moving parallel to the antiprotons. If the relative velocity is adjusted to $v/c \\approx 0.002$ the cross section for spin-flip is as large as about $2 \\cdot 10^{13}$ barn as shown by new QED-calculations of the triple spin-cross sections. Two possibilities for providing a positron source with sufficient flux density are presented. A polarised positron beam with a polarisation of 0.70 and a flux density of approximately $1.5 \\cdot 10^{10}$/(mm$^2$ s) appears to be feasible by means of a radioactive $^{11}$C dc-source. A more involved proposal is the production of polarised positrons by pair production with circularly polarised photons. It yields a polarisation of 0.76 and requires the injection into a small storage ring. Such polariser sources can be used at low (100 MeV) as well as at high (1 GeV) energy storage rings providing a time of about one hour for polarisation build-up of about $10^{10}$ antiprotons to a polarisation of about 0.18. A comparison with other proposals show a gain in the figure-of-merit by a factor of about ten.",
    "Loops are essential secondary structure elements in folded DNA and RNA molecules and proliferate close to the melting transition. Using a theory for nucleic acid secondary structures that accounts for the logarithmic entropy c ln m for a loop of length m, we study homopolymeric single-stranded nucleic acid chains under external force and varying temperature. In the thermodynamic limit of a long strand, the chain displays a phase transition between a low temperature / low force compact (folded) structure and a high temperature / high force molten (unfolded) structure. The influence of c on phase diagrams, critical exponents, melting, and force extension curves is derived analytically. For vanishing pulling force, only for the limited range of loop exponents 2 < c < 2.479 a melting transition is possible; for c <= 2 the chain is always in the folded phase and for 2.479 < c always in the unfolded phase. A force induced melting transition with singular behavior is possible for all loop exponents c < 2.479 and can be observed experimentally by single molecule force spectroscopy. These findings have implications for the hybridization or denaturation of double stranded nucleic acids. The Poland-Scheraga model for nucleic acid duplex melting does not allow base pairing between nucleotides on the same strand in denatured regions of the double strand. If the sequence allows these intra-strand base pairs, we show that for a realistic loop exponent c ~ 2.1 pronounced secondary structures appear inside the single strands. This leads to a lower melting temperature of the duplex than predicted by the Poland-Scheraga model. Further, these secondary structures renormalize the effective loop exponent c^, which characterizes the weight of a denatured region of the double strand, and thus affect universal aspects of the duplex melting transition.",
    "We study the Zeeman spin-splitting in hole quantum wires oriented along the $[011]$ and $[01\\bar{1}]$ crystallographic axes of a high mobility undoped (100)-oriented AlGaAs/GaAs heterostructure. Our data shows that the spin-splitting can be switched `on' (finite $g^{*}$) or `off' (zero $g^{*}$) by rotating the field from a parallel to a perpendicular orientation with respect to the wire, and the properties of the wire are identical for the two orientations with respect to the crystallographic axes. We also find that the $g$-factor in the parallel orientation decreases as the wire is narrowed. This is in contrast to electron quantum wires, where the $g$-factor is enhanced by exchange effects as the wire is narrowed. This is evidence for a $k$-dependent Zeeman splitting that arises from the spin-3/2 nature of holes.",
    "An analogy with real Clifford algebras on even-dimensional vector spaces suggests to assign a couple of space and time dimensions modulo 8 to any algebra (represented over a complex Hilbert space) containing two self-adjoint involutions and an anti-unitary operator with specific commutation relations. It is shown that this assignment is compatible with the tensor product: the space and time dimensions of the tensor product are the sums of the space and time dimensions of its factors. This could provide an interpretation of the presence of such algebras in PT-symmetric Hamiltonians or the description of topological matter. This construction is used to build an indefinite (i.e. pseudo-Riemannian) version of the spectral triples of noncommutative geometry, defined over Krein spaces instead of Hilbert spaces. Within this framework, we can express the Lagrangian (both bosonic and fermionic) of a Lorentzian almost-commutative spectral triple. We exhibit a space of physical states that solves the fermion-doubling problem. The example of quantum electrodynamics is described.",
    "We study the space-time symmetries of the actions obtained by expanding the action for a massive free relativistic particle around the Galilean action. We obtain all the point space-time symmetries of the post-Galilean actions by working in canonical space. We also construct an infinite collection of generalized Schr\\\"odinger algebras parameterized by an integer $M$, with $M=0$ corresponding to the standard Schr\\\"odinger algebra. We discuss the Schr\\\"odinger equations associated to these algebras, their solutions and projective phases.",
    "Accretion disc theory is less developed than stellar evolution theory although a similarly mature phenomenological picture is ultimately desired. While the interplay of theory and numerical simulations has amplified community awareness of the role of magnetic fields in angular momentum transport, there remains a long term challenge to incorporate insight gained from simulations back into improving practical models for comparison with observations. Here we emphasize the need to incorporate the role of non-local transport more precisely. To show where large scale transport would fit into the theoretical framework and how it is currently missing, we review why the wonderfully practical approach of Shakura-Sunyaev (1973,SS73) is necessarily a mean field theory, and one which does not include large scale transport. Observations of coronae and jets combined with the interpretation of results even from shearing box simulations of the magnetorotational instability (MRI) suggest that a significant fraction of disc transport is indeed non-local. We show that the Maxwell stresses in saturation are dominated by large scale contributions and the physics of MRI transport is not fully captured by a viscosity. We also clarify the standard physical interpretation of the MRi as it applies to shearing boxes. Computational limitations have so far focused most attention toward local simulations but the next generation of global simulations should help to inform improved mean field theories. Mean field accretion theory and mean field dynamo theory should in fact be unified into a single theory that predicts the time evolution of spectra and luminosity from separate disc, corona, and outflow contributions. Finally, we note that any mean field theory has a finite predictive precision that needs to be quantified when comparing the predictions to observations.",
    "This paper is devoted to the global well-posedness of two Diffuse Interface systems modeling the motion of an incompressible two-phase fluid mixture in presence of capillarity effects in a bounded smooth domain $\\Omega\\subset \\mathbb{R}^d$, $d=2,3$. We focus on dissipative mixing effects originating from the mass-conserving Allen-Cahn dynamics with the physically relevant Flory-Huggins potential. More precisely, we study the mass-conserving Navier-Stokes-Allen-Cahn system for nonhomogeneous fluids and the mass-conserving Euler-Allen-Cahn system for homogeneous fluids. We prove existence and uniqueness of global weak and strong solutions as well as their property of separation from the pure states. In our analysis, we combine the energy and entropy estimates, a novel end-point estimate of the product of two functions, a new estimate for the Stokes problem with non-constant viscosity, and logarithmic type Gronwall arguments.",
    "We present explicit expressions for Fock-space projection operators that correspond to realistic final states in scattering experiments. Our operators automatically sum over unobserved quanta and account for non-emission into sub-regions of momentum space.",
    "In this talk we discuss mathematical structures associated to Feynman graphs. Feynman graphs are the backbone of calculations in perturbative quantum field theory. The mathematical structures -- apart from being of interest in their own right -- allow to derive algorithms for the computation of these graphs. Topics covered are the relations of Feynman integrals to periods, shuffle algebras and multiple polylogarithms.",
    "We report on a calculation of the generalized parton distributions of the photon when there is non-zero momentum transfer both in the transverse and longitudinal directions. By taking Fourier transforms of the GPDs with respect transverse and longitudinal momentum transfer, we obtain the parton distributions of the photon in position space.",
    "Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.",
    "We compute the leading logarithmic behaviour of the cross-section for the production of a pseudoscalar Higgs boson in gluon-gluon fusion to all-orders in perturbation theory, in the limit of large partonic centre of mass energy. We also calculate the Higgs rapidity distribution to the same accuracy. We include the contributions of top and bottom quarks, together with their interference. Our results are given in terms of single and double integrals, evaluated explicitly up to next-to next-to leading order (NNLO). We use our results to improve the known NNLO inclusive cross-section computed in the effective theory where the fermions in the loop are integrated out. The size of finite fermion mass effects on the inclusive cross-section is found to be small, reaching a few percent only for large values of the pseudoscalar mass.",
    "Finding outlying elements in probability distributions can be a hard problem. Taking a real example from Voting Rights Act enforcement, we consider the problem of maximizing the number of simultaneous majority-minority districts in a political districting plan. An unbiased random walk on districting plans is unlikely to find plans that approach this maximum. A common search approach is to use a biased random walk: preferentially select districting plans with more majority-minority districts. Here, we present a third option, called short bursts, in which an unbiased random walk is performed for a small number of steps (called the burst length), then re-started from the most extreme plan that was encountered in the last burst. We give empirical evidence that short-burst runs outperform biased random walks for the problem of maximizing the number of majority-minority districts, and that there are many values of burst length for which we see this improvement. Abstracting from our use case, we also consider short bursts where the underlying state space is a line with various probability distributions, and then explore some features of more complicated state spaces and how these impact the effectiveness of short bursts.",
    "We report on a molecular dynamics investigation of the wetting properties of graphitic surfaces by various solutions at concentrations 1-8 wt% of commercially available non-ionic surfactants with long hydrophilic chains, linear or T-shaped. These are surfactants of length up to 160 [\\AA]. It turns out that molecular dynamics simulations of such systems ask for a number of solvent particles that can be reached without seriously compromising computational efficiency only by employing a coarse-grained model. The MARTINI force field with polarizable water offers a framework particularly suited for the parameterization of our systems. In general, its advantages over other coarse-grained models are the possibility to explore faster long time scales and the wider range of applicability. Although the accuracy is sometimes put under question, the results for the wetting properties by pure water are in good agreement with those for the corresponding atomistic systems and theoretical predictions. On the other hand, the bulk properties of various aqueous surfactant solutions indicate that the micellar formation process is too strong. For this reason, a typical experimental configuration is better approached by preparing the droplets with the surfactants arranged in the initial state in the vicinity of contact line. Cross-comparisons are possible and illuminating, but equilibrium contanct angles as obtained from simulations overestimate the experimental results. Nevertheless, our findings can provide guidelines for the preliminary assessment and screening of surfactants. [See pdf file for full abstract]",
    "We review recent experiments in which superfluid $^3$He has been studied under highly controlled confinement in nanofluidic sample chambers. We discuss the experimental challenges and their resolution. These methods open the way to a systematic investigation of the superfluidity of $^3$He films, and the surface and edge excitations of topological superfluids.",
    "Code-mixed machine translation has become an important task in multilingual communities and extending the task of machine translation to code mixed data has become a common task for these languages. In the shared tasks of WMT 2022, we try to tackle the same for both English + Hindi to Hinglish and Hinglish to English. The first task dealt with both Roman and Devanagari script as we had monolingual data in both English and Hindi whereas the second task only had data in Roman script. To our knowledge, we achieved one of the top ROUGE-L and WER scores for the first task of Monolingual to Code-Mixed machine translation. In this paper, we discuss the use of mBART with some special pre-processing and post-processing (transliteration from Devanagari to Roman) for the first task in detail and the experiments that we performed for the second task of translating code-mixed Hinglish to monolingual English.",
    "Contrastive learning has shown promising potential in self-supervised spatio-temporal representation learning. Most works naively sample different clips to construct positive and negative pairs. However, we observe that this formulation inclines the model towards the background scene bias. The underlying reasons are twofold. First, the scene difference is usually more noticeable and easier to discriminate than the motion difference. Second, the clips sampled from the same video often share similar backgrounds but have distinct motions. Simply regarding them as positive pairs will draw the model to the static background rather than the motion pattern. To tackle this challenge, this paper presents a novel dual contrastive formulation. Concretely, we decouple the input RGB video sequence into two complementary modes, static scene and dynamic motion. Then, the original RGB features are pulled closer to the static features and the aligned dynamic features, respectively. In this way, the static scene and the dynamic motion are simultaneously encoded into the compact RGB representation. We further conduct the feature space decoupling via activation maps to distill static- and dynamic-related features. We term our method as \\textbf{D}ual \\textbf{C}ontrastive \\textbf{L}earning for spatio-temporal \\textbf{R}epresentation (DCLR). Extensive experiments demonstrate that DCLR learns effective spatio-temporal representations and obtains state-of-the-art or comparable performance on UCF-101, HMDB-51, and Diving-48 datasets.",
    "The electronic bandstructure and the Fermi surfaces of ferromagnetic CeRh3B2 are calculated by using FLAPW and LSDA+U method. As assuming several kinds of the ground state to describe the 4f electronic state, we propose a fully orbital- and spin-polarized state |lz=0, sx=1/2> as the ground state, instead of the conventional LS-coupled CEF ground state, generally expected in typical 4f compounds. This is supported by the fact that both the observed magnetic moment and the observed dHvA frequencies are well explained by the calculated electronic structure and the Fermi surfaces. The unconventional ground state is stabilized by the strong 4f-4f direct mixing between the neighbored Ce atoms along the extremely small distance along the c-axis in the hexagonal crystal cell.",
    "Matrix acidization simulation is a challenging task in the study of flows in porous media, due to the changing porosity in the procedure. The improved DBF framework is one model to do this simulation, and its numerical scheme discretises the mass and momentum conservation equations together to form a pressure-velocity linear system. However, this linear system can only be solved by direct solvers to solve for pressure and velocity simultaneously, since zeros appear in the diagonal of the coefficient matrix. Considering the large-scale attribute of matrix acidization simulation, the solving time of direct solvers is not intolerant. Thus, a decoupled scheme is proposed in this work to decouple the coupled pressure-velocity linear system into two independent linear systems: one is to solve for pressure, and the other one is to solve for velocity. Both of the new linear systems can be solved by parallel and iterative solvers, which guarantees the large-scale simulation can be finished in a reasonable time period. A numerical experiment is carried out to demonstrate the correctness of the decoupled scheme and its higher computing efficiency.",
    "Sensemaking and narrative are two inherently interconnected concepts about how people understand the world around them. Sensemaking is the process by which people structure and interconnect the information they encounter in the world with the knowledge and inferences they have made in the past. Narratives are important constructs that people use sensemaking to create; ones that reflect provide a more holistic account of the world than the information within any given narrative is able to alone. Both are important to how human beings parse the world, and both would be valuable for a computational system attempting to do the same. In this paper, we discuss theories of sensemaking and narrative with respect to how people build an understanding of the world based on the information they encounter, as well as the links between the fields of sensemaking and narrative research. We highlight a specific computational task, visual storytelling, whose solutions we believe can be enhanced by employing a sensemaking and narrative component. We then describe our system for visual storytelling using sensemaking and narrative and discuss examples from its current implementation.",
    "Evaluation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. However, currently, there exists no way to quantify how metrics respond to change in the dialect of a generated utterance. We thus formalize dialect robustness and dialect awareness as goals for NLG evaluation metrics. We introduce a suite of methods and corresponding statistical tests one can use to assess metrics in light of the two goals. Applying the suite to current state-of-the-art metrics, we demonstrate that they are not dialect-robust and that semantic perturbations frequently lead to smaller decreases in a metric than the introduction of dialect features. As a first step to overcome this limitation, we propose a training schema, NANO, which introduces regional and language information to the pretraining process of a metric. We demonstrate that NANO provides a size-efficient way for models to improve the dialect robustness while simultaneously improving their performance on the standard metric benchmark.",
    "Geographic routing consists in using the position information of nodes to assist in the routing process, and has been a widely studied subject in sensor networks. One of the outstanding challenges facing geographic routing has been its applicability. Authors either make some broad assumptions on an idealized version of wireless networks which are often unverifiable, or they use costly methods to planarize the communication graph. The overarching questions that drive us are the following. When, and how should we use geographic routing? Is there a criterion to tell whether a communication network is fit for geographic routing? When exactly does geographic routing make sense? In this paper we formulate the four principles that define geographic routing and explore their topological consequences. Given a localized communication network, we then define and compute its geographic eccentricity, which measures its fitness for geographic routing. Finally we propose a distributed algorithm that either enables geographic routing on the network or proves that its geographic eccentricity is too high.",
    "We show that spatial variation and correlation of superconductivity fluctuations in a two-band model are scaled by two characteristic lengths. This results in substantially more complicated picture compared to one-band systems. In particular, short-range correlations are always present in a two-band scenario, even near the phase transition point.",
    "We present online prediction methods for time series that let us explicitly handle nonstationary artifacts (e.g. trend and seasonality) present in most real time series. Specifically, we show that applying appropriate transformations to such time series before prediction can lead to improved theoretical and empirical prediction performance. Moreover, since these transformations are usually unknown, we employ the learning with experts setting to develop a fully online method (NonSTOP-NonSTationary Online Prediction) for predicting nonstationary time series. This framework allows for seasonality and/or other trends in univariate time series and cointegration in multivariate time series. Our algorithms and regret analysis subsume recent related work while significantly expanding the applicability of such methods. For all the methods, we provide sub-linear regret bounds using relaxed assumptions. The theoretical guarantees do not fully capture the benefits of the transformations, thus we provide a data-dependent analysis of the follow-the-leader algorithm that provides insight into the success of using such transformations. We support all of our results with experiments on simulated and real data.",
    "We present a heuristic framework for attacking the undecidable termination problem of logic programs, as an alternative to current termination/non-termination proof approaches. We introduce an idea of termination prediction, which predicts termination of a logic program in case that neither a termination nor a non-termination proof is applicable. We establish a necessary and sufficient characterization of infinite (generalized) SLDNF-derivations with arbitrary (concrete or moded) queries, and develop an algorithm that predicts termination of general logic programs with arbitrary non-floundering queries. We have implemented a termination prediction tool and obtained quite satisfactory experimental results. Except for five programs which break the experiment time limit, our prediction is 100% correct for all 296 benchmark programs of the Termination Competition 2007, of which eighteen programs cannot be proved by any of the existing state-of-the-art analyzers like AProVE07, NTI, Polytool and TALP.",
    "Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoretically tractable variant of random regression forests and prove that our algorithm is consistent. We also provide an empirical evaluation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in practice. Our experiments provide insight into the relative importance of different simplifications that theoreticians have made to obtain tractable models for analysis.",
    "Factorial Hidden Markov Models (FHMMs) are powerful models for sequential data but they do not scale well with long sequences. We propose a scalable inference and learning algorithm for FHMMs that draws on ideas from the stochastic variational inference, neural network and copula literatures. Unlike existing approaches, the proposed algorithm requires no message passing procedure among latent variables and can be distributed to a network of computers to speed up learning. Our experiments corroborate that the proposed algorithm does not introduce further approximation bias compared to the proven structured mean-field algorithm, and achieves better performance with long sequences and large FHMMs.",
    "Molecular dynamics simulations have been performed on pure liquid water, aqueous solutions of sodium chloride, and polymer solutions exposed to a strong external electric field with the goal to gain molecular insight into the structural response to the field. Several simulation methodologies have been used to elucidate the molecular mechanisms of the processes leading to the formation of liquid bridges and jets (in the production of nanofibers). It is shown that in the established nanoscale structures, the molecules form a chain with their dipole moments oriented parallel to the applied field throughout the entire sample volume. The presence of ions may disturb this structure leading to its ultimate disintegration into droplets; the concentration dependence of the threshold field required to stabilize a liquid column has been determined. Conformational changes of the polymer in the jetting process have also been observed.",
    "Recommender systems are increasingly used to predict and serve content that aligns with user taste, yet the task of matching new users with relevant content remains a challenge. We consider podcasting to be an emerging medium with rapid growth in adoption, and discuss challenges that arise when applying traditional recommendation approaches to address the cold-start problem. Using music consumption behavior, we examine two main techniques in inferring Spotify users preferences over more than 200k podcasts. Our results show significant improvements in consumption of up to 50\\% for both offline and online experiments. We provide extensive analysis on model performance and examine the degree to which music data as an input source introduces bias in recommendations.",
    "We calculate the Casimir energy and entropy for two perfect metal spheres in the large and short separation limit. We obtain nonmonotonic behavior of the Helmholtz free energy with separation and temperature, leading to parameter ranges with negative entropy, and also nonmonotonic behavior of the entropy with temperature and with the separation between the spheres. The appearance of this anomalous behavior of the entropy is discussed as well as its thermodynamic consequences.",
    "Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.",
    "In this paper, we propose two mask-based beamforming methods using a deep neural network (DNN) trained by multichannel loss functions. Beamforming technique using time-frequency (TF)-masks estimated by a DNN have been applied to many applications where TF-masks are used for estimating spatial covariance matrices. To train a DNN for mask-based beamforming, loss functions designed for monaural speech enhancement/separation have been employed. Although such a training criterion is simple, it does not directly correspond to the performance of mask-based beamforming. To overcome this problem, we use multichannel loss functions which evaluate the estimated spatial covariance matrices based on the multichannel Itakura--Saito divergence. DNNs trained by the multichannel loss functions can be applied to construct several beamformers. Experimental results confirmed their effectiveness and robustness to microphone configurations.",
    "Nano-FTIR imaging is a powerful scanning-based technique at nanometer spatial resolution which combines Fourier transform infrared spectroscopy (FTIR) and scattering-type scanning near-field optical microscopy (s-SNOM). However, recording large spatial areas with nano-FTIR is limited by long measurement times due to its sequential data acquisition. Several mathematical approaches have been proposed to tackle this problem. All of them have in common that only a small fraction of randomly chosen measurements is required. However, choosing the fraction of measurements in a random fashion poses practical challenges for scanning procedures and does not lead to time savings as large as desired. We consider different, practically relevant sub-sampling schemes assuring a faster acquisition. It is demonstrated that results for almost all considered sub-sampling schemes, namely original Lissajous, triangle Lissajous, and random reflection sub-sampling, at a sub-sampling rate of 10%, are comparable to results when using a random sub-sampling of 10%. This implies that random sub-sampling is not required for efficient data acquisition.",
    "We determine from Polyakov loop correlators the screening masses in th e deconfined phase of the (3+1)d SU(3) pure gauge theory at finite temperature near transition, for two different channels of angular momentum and parity. Their ratio is compared with that of the massive excitations with the same quantum numbers in the 3d 3-state Potts model in the broken phase near the transition point at zero magnetic field. Moreover we study the inverse decay length of the correlation between the real parts and between the imaginary parts of the Polyakov loop and compare the results with expectations from perturbation theory and mean-field Polyakov loop models.",
    "The Mahalanobis distance-based confidence score, a recently proposed anomaly detection method for pre-trained neural classifiers, achieves state-of-the-art performance on both out-of-distribution (OoD) and adversarial examples detection. This work analyzes why this method exhibits such strong performance in practical settings while imposing an implausible assumption; namely, that class conditional distributions of pre-trained features have tied covariance. Although the Mahalanobis distance-based method is claimed to be motivated by classification prediction confidence, we find that its superior performance stems from information not useful for classification. This suggests that the reason the Mahalanobis confidence score works so well is mistaken, and makes use of different information from ODIN, another popular OoD detection method based on prediction confidence. This perspective motivates us to combine these two methods, and the combined detector exhibits improved performance and robustness. These findings provide insight into the behavior of neural classifiers in response to anomalous inputs.",
    "Algorithms like those for differentiating functional expressions manipulate the syntactic structure of mathematical expressions in a mathematically meaningful way. A formalization of such an algorithm should include a specification of its computational behavior, a specification of its mathematical meaning, and a mechanism for applying the algorithm to actual expressions. Achieving these goals requires the ability to integrate reasoning about the syntax of the expressions with reasoning about what the expressions mean. A syntax framework is a mathematical structure that is an abstract model for a syntax reasoning system. It contains a mapping of expressions to syntactic values that represent the syntactic structures of the expressions; a language for reasoning about syntactic values; a quotation mechanism to refer to the syntactic value of an expression; and an evaluation mechanism to refer to the value of the expression represented by a syntactic value. We present and compare two approaches, based on instances of a syntax framework, to formalize a syntax-based mathematical algorithm in a formal theory T. In the first approach the syntactic values for the expressions manipulated by the algorithm are members of an inductive type in T, but quotation and evaluation are functions defined in the metatheory of T. In the second approach every expression in T is represented by a syntactic value, and quotation and evaluation are operators in T itself.",
    "The paper concerns two interacting consumer-resource pairs based on chemostat-like equations under the assumption that the dynamics of the resource is considerably slower than that of the consumer. The presence of two different time scales enables to carry out a fairly complete analysis of the problem. This is done by treating consumers and resources in the coupled system as fast-scale and slow-scale variables respectively and subsequently considering developments in phase planes of these variables, fast and slow, as if they are independent. When uncoupled, each pair has unique asymptotically stable steady state and no self-sustained oscillatory behavior (although damped oscillations about the equilibrium are admitted). When the consumer-resource pairs are weakly coupled through direct reciprocal inhibition of consumers, the whole system exhibits self-sustained relaxation oscillations with a period that can be significantly longer than intrinsic relaxation time of either pair. It is shown that the model equations adequately describe locally linked consumer-resource systems of quite different nature: living populations under interspecific interference competition and lasers coupled via their cavity losses.",
    "Wireless local area networks (WLAN) still suffer from a severe performance discrepancy between different users in the uplink. This is because of the spatially varying channel conditions provided by the wireless medium. Cooperative medium access control (MAC) protocols as for example CoopMAC were proposed to mitigate this problem. In this work, it is shown that cooperation implies for cooperating nodes a tradeoff between throughput and bit-cost, which is the energy needed to transmit one bit. The tradeoff depends on the degree of cooperation. For carrier sense multiple access (CSMA) based networks, the throughput/bit-cost tradeoff curve is theoretically derived. A new distributed CSMA protocol called fairMAC is proposed and it is theoretically shown that fairMAC can asymptotically achieve any operating point on the tradeoff curve when the packet lengths go to infinity. The theoretical results are validated through Monte Carlo simulations.",
    "Social tagging has become an interesting approach to improve search and navigation over the actual Web, since it aggregates the tags added by different users to the same resource in a collaborative way. This way, it results in a list of weighted tags describing its resource. Combined to a classical taxonomic classification system such as that by Wikipedia, social tags can enhance document navigation and search. On the one hand, social tags suggest alternative navigation ways, including pivot-browsing, popularity-driven navigation, and filtering. On the other hand, it provides new metadata, sometimes uncovered by documents' content, that can substantially improve document search. In this work, the inclusion of an interface to add user-defined tags describing Wikipedia articles is proposed, as a way to improve article navigation and retrieval. As a result, a prototype on applying tags over Wikipedia is proposed in order to evaluate its effectiveness.",
    "Quantum Computing and especially Quantum Machine Learning, in a short period of time, has gained a lot of interest through research groups around the world. This can be seen in the increasing number of proposed models for pattern classification applying quantum principles to a certain degree. Despise the increasing volume of models, there is a void in testing these models on real datasets and not only on synthetic ones. The objective of this work is to classify patterns with binary attributes using a quantum classifier. Specially, we show results of a complete quantum classifier applied to image datasets. The experiments show favorable output while dealing with balanced classification problems as well as with imbalanced classes where the minority class is the most relevant. This is promising in medical areas, where usually the important class is also the minority class.",
    "We present near- and mid-infrared observations on the shock-cloud interaction region in the southern part of the supernova remnant HB 21, performed with the InfraRed Camera (IRC) aboard AKARI satellite and the Wide InfraRed Camera (WIRC) at the Palomar 5 m telescope. The IRC 4 um (N4), 7 um (S7), and 11 um (S11) band images and the WIRC H2 v=1->0 S(1) 2.12 um image show similar diffuse features, around a shocked CO cloud. We analyzed the emission through comparison with the H2 line emission of several shock models. The IRC colors are well explained by the thermal admixture model of H2 gas--whose infinitesimal H2 column density has a power-law relation with the temperature $T$, $dN\\sim T^{-b}dT$--with n(H2) $\\sim3.9\\times10^4$ cm^{-2}, $b\\sim4.2$, and N(H2;T>100K) $\\sim2.8\\times10^{21}$ cm^{-2}. We interpreted these parameters with several different pictures of the shock-cloud interactions--multiple planar C-shocks, bow shocks, and shocked clumps--and discuss their weaknesses and strengths. The observed H2 v=1->0 S(1) intensity is four times greater than the prediction from the power-law admixture model, the same tendency as found in the northern part of HB 21 (Paper I). We also explored the limitation of the thermal admixture model with respect to the derived model parameters.",
    "Recently, vision transformers started to show impressive results which outperform large convolution based models significantly. However, in the area of small models for mobile or resource constrained devices, ConvNet still has its own advantages in both performance and model complexity. We propose ParC-Net, a pure ConvNet based backbone model that further strengthens these advantages by fusing the merits of vision transformers into ConvNets. Specifically, we propose position aware circular convolution (ParC), a light-weight convolution op which boasts a global receptive field while producing location sensitive features as in local convolutions. We combine the ParCs and squeeze-exictation ops to form a meta-former like model block, which further has the attention mechanism like transformers. The aforementioned block can be used in plug-and-play manner to replace relevant blocks in ConvNets or transformers. Experiment results show that the proposed ParC-Net achieves better performance than popular light-weight ConvNets and vision transformer based models in common vision tasks and datasets, while having fewer parameters and faster inference speed. For classification on ImageNet-1k, ParC-Net achieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11% parameters and 13% computational cost but gaining 0.2% higher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288) compared with MobileViT, and uses only 0.5 times parameters but gaining 2.7% accuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC segmentation tasks, ParC-Net also shows better performance. Source code is available at https://github.com/hkzhang91/ParC-Net",
    "From the algebraic solution of $x^{n}-x+t=0$ for $n=2,3,4$ and the corresponding solution in terms of hypergeometric functions, we obtain a set of reduction formulas for hypergeometric functions. By differentiation and integration of these results, and applying other known reduction formulas of hypergeometric functions, we derive new reduction formulas of special functions as well as the calculation of some infinite integrals in terms of elementary functions.",
    "Air-gap covert channels are special types of covert communication channels that enable attackers to exfiltrate data from isolated, network-less computers. Various types of air-gap covert channels have been demonstrated over the years, including electromagnetic, magnetic, acoustic, optical, and thermal. In this paper, we introduce a new type of vibrational (seismic) covert channel. We observe that computers vibrate at a frequency correlated to the rotation speed of their internal fans. These inaudible vibrations affect the entire structure on which the computer is placed. Our method is based on malware's capability of controlling the vibrations generated by a computer, by regulating its internal fan speeds. We show that the malware-generated covert vibrations can be sensed by nearby smartphones via the integrated, sensitive \\textit{accelerometers}. Notably, the accelerometer sensors in smartphones can be accessed by any app without requiring the user permissions, which make this attack highly evasive. We implemented AiR-ViBeR, malware that encodes binary information, and modulate it over a low frequency vibrational carrier. The data is then decoded by malicious application on a smartphone placed on the same surface (e.g., on a desk). We discuss the attack model, provide technical background, and present the implementation details and evaluation results. Our results show that using AiR-ViBeR, data can be exfiltrated from air-gapped computer to a nearby smartphone on the same table, or even an adjacent table, via vibrations. Finally, we propose a set of countermeasures for this new type of attack.",
    "The total cost of a 25 W average load magnetic refrigerator using commercial grade Gd is calculated using a numerical model. The price of magnetocaloric material, magnet material and cost of operation are considered, and all influence the total cost. The lowest combined total cost with a device lifetime of 15 years is found to be in the range \\$150-\\$400 depending on the price of the magnetocaloric and magnet material. The cost of the magnet is largest, followed closely by the cost of operation, while the cost of the magnetocaloric material is almost negligible. For the lowest cost device, the optimal magnetic field is about 1.4 T, the particle size is 0.23 mm, the length of the regenerator is 40-50 mm and the utilization is about 0.2, for all device lifetimes and material and magnet prices, while the operating frequency vary as function of device lifetime. The considered performance characteristics are based on the performance of a conventional A$^{+++}$ refrigeration unit. In a rough life time cost comparison between the magnetic refrigeration device and such a unit we find similar costs, the former being slightly cheaper, assuming the cost of the magnet can be recuperated at end of life.",
    "We prove the existence of initial data sets which possess an asymptotically flat and an asymptotically cylindrical end. Such geometries are known as trumpets in the community of numerical relativists.",
    "Enzymes utilize protein architectures to create highly specialized structural motifs that can greatly enhance the rates of complex chemical transformations. Here we use experiments, combined with ab initio simulations that exactly include nuclear quantum effects, to show that a triad of strongly hydrogen bonded tyrosine residues within the active site of the enzyme ketosteroid isomerase (KSI) facilitates quantum proton delocalization. This delocalization dramatically stabilizes the deprotonation of an active site tyrosine residue, resulting in a very large isotope effect on its acidity. When an intermediate analog is docked, it is incorporated into the hydrogen bond network, giving rise to extended quantum proton delocalization in the active site. These results shed light on the role of nuclear quantum effects in the hydrogen bond network that stabilizes the reactive intermediate of KSI, and the behavior of protons in biological systems containing strong hydrogen bonds.",
    "In this work, we propose ENSEI, a secure inference (SI) framework based on the frequency-domain secure convolution (FDSC) protocol for the efficient execution of privacy-preserving visual recognition. Our observation is that, under the combination of homomorphic encryption and secret sharing, homomorphic convolution can be obliviously carried out in the frequency domain, significantly simplifying the related computations. We provide protocol designs and parameter derivations for number-theoretic transform (NTT) based FDSC. In the experiment, we thoroughly study the accuracy-efficiency trade-offs between time- and frequency-domain homomorphic convolution. With ENSEI, compared to the best known works, we achieve 5--11x online time reduction, up to 33x setup time reduction, and up to 10x reduction in the overall inference time. A further 33% of bandwidth reductions can be obtained on binary neural networks with only 1% of accuracy degradation on the CIFAR-10 dataset.",
    "Recommender systems benefit us in tackling the problem of information overload by predicting our potential choices among diverse niche objects. So far, a variety of personalized recommendation algorithms have been proposed and most of them are based on similarities, such as collaborative filtering and mass diffusion. Here, we propose a novel vertex similarity index named CosRA, which combines advantages of both the cosine index and the resource-allocation (RA) index. By applying the CosRA index to real recommender systems including MovieLens, Netflix and RYM, we show that the CosRA-based method has better performance in accuracy, diversity and novelty than some benchmark methods. Moreover, the CosRA index is free of parameters, which is a significant advantage in real applications. Further experiments show that the introduction of two turnable parameters cannot remarkably improve the overall performance of the CosRA index.",
    "Many real-world applications adopt multi-label data streams as the need for algorithms to deal with rapidly changing data increases. Changes in data distribution, also known as concept drift, cause the existing classification models to rapidly lose their effectiveness. To assist the classifiers, we propose a novel algorithm called Label Dependency Drift Detector (LD3), an implicit (unsupervised) concept drift detector using label dependencies within the data for multi-label data streams. Our study exploits the dynamic temporal dependencies between labels using a label influence ranking method, which leverages a data fusion algorithm and uses the produced ranking to detect concept drift. LD3 is the first unsupervised concept drift detection algorithm in the multi-label classification problem area. In this study, we perform an extensive evaluation of LD3 by comparing it with 14 prevalent supervised concept drift detection algorithms that we adapt to the problem area using 12 datasets and a baseline classifier. The results show that LD3 provides between 19.8\\% and 68.6\\% better predictive performance than comparable detectors on both real-world and synthetic data streams.",
    "The universality of the Cepheid Period-Luminosity relations has been under discussion since metallicity effects have been assumed to play a role in the value of the intercept and, more recently, of the slope of these relations. The goal of the present study is to calibrate the Galactic PL relations in various photometric bands (from B to K) and to compare the results to the well-established PL relations in the LMC. We use a set of 59 calibrating stars, the distances of which are measured using five different distance indicators: Hubble Space Telescope and revised Hipparcos parallaxes, infrared surface brightness and interferometric Baade-Wesselink parallaxes, and classical Zero-Age-Main-Sequence-fitting parallaxes for Cepheids belonging to open clusters or OB stars associations. A detailed discussion of absorption corrections and projection factor to be used is given. We find no significant difference in the slopes of the PL relations between LMC and our Galaxy. We conclude that the Cepheid PL relations have universal slopes in all photometric bands, not depending on the galaxy under study (at least for LMC and Milky Way). The possible zero-point variation with metal content is not discussed in the present work, but an upper limit of 18.50 for the LMC distance modulus can be deduced from our data.",
    "Ensembling methods are well known for improving prediction accuracy. However, they are limited in the sense that they cannot discriminate among component models effectively. In this paper, we propose stacking with auxiliary features that learns to fuse relevant information from multiple systems to improve performance. Auxiliary features enable the stacker to rely on systems that not just agree on an output but also the provenance of the output. We demonstrate our approach on three very different and difficult problems -- the Cold Start Slot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet object detection tasks. We obtain new state-of-the-art results on the first two tasks and substantial improvements on the detection task, thus verifying the power and generality of our approach.",
    "We present a simple and fast method to simulate spin-torque driven magnetisation dynamics in nano-pillar spin-valve structures. The approach is based on the coupling between a spin transport code based on random matrix theory and a micromagnetics finite-elements software. In this way the spatial dependence of both spin transport and magnetisation dynamics is properly taken into account. Our results are compared with experiments. The excitation of the spin-wave modes, in- cluding the threshold current for steady state magnetisation precession and the nonlinear frequency shift of the modes are reproduced correctly. The giant magneto resistance effect and the magnetisa- tion switching also agree with experiment. The similarities with recently described spin-caloritronics devices are also discussed.",
    "We present a simple framework to compute hyperbolic Voronoi diagrams of finite point sets as affine diagrams. We prove that bisectors in Klein's non-conformal disk model are hyperplanes that can be interpreted as power bisectors of Euclidean balls. Therefore our method simply consists in computing an equivalent clipped power diagram followed by a mapping transformation depending on the selected representation of the hyperbolic space (e.g., Poincar\\'e conformal disk or upper-plane representations). We discuss on extensions of this approach to weighted and $k$-order diagrams, and describe their dual triangulations. Finally, we consider two useful primitives on the hyperbolic Voronoi diagrams for designing tailored user interfaces of an image catalog browsing application in the hyperbolic disk: (1) finding nearest neighbors, and (2) computing smallest enclosing balls.",
    "The problem of diffusive bond-dissociation in a double well potential under application of an external force is scrutinized. We compute the probability distribution of rupture forces and present a detailed discussion of the influence of finite rebinding probabilities on the dynamic force spectrum. In particular, we focus on barrier crossing upon extension, i.e. under linearly increased load, and upon relaxation starting from completely separated bonds. For large loading rates the rupture force and the rejoining force depend on the loading rate in the expected manner determined by the shape of the potential. For small loading rates the mean forces obtained from pull and relax modes approach each other as the system reaches equilibrium. We investigate the dependence of the rupture force distributions and mean rupture forces on external parameters like cantilever stiffness and influence of a soft linker. We find that depending on the implementation of a soft linker the equilibrium rupture force is either unaffected by the presence of the linker or changes in a predictable way with the linker-compliance. Additionally, we show that it is possible to extract the equilibrium constant of the on- and off-rates from the determination of the equilibrium rupture forces.",
    "We propose an invariant feature space for the detection of viscous dominated and turbulent regions (i.e., boundary layers and wakes). The developed methodology uses the principal invariants of the strain and rotational rate tensors as input to an unsupervised Machine Learning Gaussian mixture model. The selected feature space is independent of the coordinate frame used to generate the processed data, as it relies on the principal invariants of strain and rotational rate, which are Galilean invariants. This methodology allows us to identify two distinct flow regions: a viscous dominated, rotational region (boundary layer and wake region) and an inviscid, irrotational region (outer flow region). We test the methodology on a laminar and a turbulent (using Large Eddy Simulation) case for flows past a circular cylinder at $Re=40$ and $Re=3900$. The simulations have been conducted using a high-order nodal Discontinuous Galerkin Spectral Element Method (DGSEM). The results obtained are analysed to show that Gaussian mixture clustering provides an effective identification method of viscous dominated and rotational regions in the flow. We also include comparisons with traditional sensors to show that the proposed clustering does not depend on the selection of an arbitrary threshold, as required when using traditional sensors.",
    "Engineered quantum systems allow us to observe phenomena that are not easily accessible naturally. The LEGO-like nature of superconducting circuits makes them particularly suited for building and coupling artificial atoms. Here, we introduce an artificial molecule, composed of two strongly coupled fluxonium atoms, which possesses a tunable magnetic moment. Using an applied external flux, one can tune the molecule between two regimes: one in which the ground-excited state manifold has a magnetic dipole moment and one in which the ground-excited state manifold has only a magnetic quadrupole moment. By varying the applied external flux, we find the coherence of the molecule to be limited by local flux noise. The ability to engineer and control artificial molecules paves the way for building more complex circuits for protected qubits and quantum simulation.",
    "We describe a method for time-critical decision making involving sequential tasks and stochastic processes. The method employs several iterative refinement routines for solving different aspects of the decision making problem. This paper concentrates on the meta-level control problem of deliberation scheduling, allocating computational resources to these routines. We provide different models corresponding to optimization problems that capture the different circumstances and computational strategies for decision making under time constraints. We consider precursor models in which all decision making is performed prior to execution and recurrent models in which decision making is performed in parallel with execution, accounting for the states observed during execution and anticipating future states. We describe algorithms for precursor and recurrent models and provide the results of our empirical investigations to date.",
    "The role model strategy is introduced as a method for designing an estimator by approaching the output of a superior estimator that has better input observations. This strategy is shown to yield the optimal Bayesian estimator when a Markov condition is fulfilled. Two examples involving simple channels are given to illustrate its use. The strategy is combined with time averaging to construct a statistical model by numerically solving a convex program. The role model strategy was developed in the context of low complexity decoder design for iterative decoding. Potential applications outside the field of communications are discussed.",
    "Computer vision systems currently lack the ability to reliably recognize artistically rendered objects, especially when such data is limited. In this paper, we propose a method for recognizing objects in artistic modalities (such as paintings, cartoons, or sketches), without requiring any labeled data from those modalities. Our method explicitly accounts for stylistic domain shifts between and within domains. To do so, we introduce a complementary training modality constructed to be similar in artistic style to the target domain, and enforce that the network learns features that are invariant between the two training modalities. We show how such artificial labeled source domains can be generated automatically through the use of style transfer techniques, using diverse target images to represent the style in the target domain. Unlike existing methods which require a large amount of unlabeled target data, our method can work with as few as ten unlabeled images. We evaluate it on a number of cross-domain object and scene classification tasks and on a new dataset we release. Our experiments show that our approach, though conceptually simple, significantly improves the accuracy that existing domain adaptation techniques obtain for artistic object recognition.",
    "This paper studies the large time behavior of solutions to semi-linear Cauchy problems with quadratic nonlinearity in gradients. The Cauchy problem considered has a general state space and may degenerate on the boundary of the state space. Two types of large time behavior are obtained: i) pointwise convergence of the solution and its gradient; ii) convergence of solutions to associated backward stochastic differential equations. When the state space is R^d or the space of positive definite matrices, both types of convergence are obtained under growth conditions on model coefficients. These large time convergence results have direct applications in risk sensitive control and long term portfolio choice problems.",
    "The decaying vacuum model (DV), treating dark energy as a varying vacuum, has been studied well recently. The vacuum energy decays linearly with the Hubble parameter in the late-times, $\\rho_\\Lambda(t) \\propto H(t)$, and produces the additional matter component. We constrain the parameters of the DV model using the recent data-sets from supernovae, gamma-ray bursts, baryon acoustic oscillations, CMB, the Hubble rate and x-rays in galaxy clusters. It is found that the best fit of matter density contrast $\\Omega_m$ in the DV model is much lager than that in $\\Lambda$CDM model. We give the confidence contours in the $\\Omega_m-h$ plane up to $3\\sigma$ confidence level. Besides, the normalized likelihoods of $\\Omega_m$ and $h$ are presented, respectively. %",
    "Perpendicular MgO-based Magnetic Tunnel Junctions are optimal candidates as building block of Spin Transfer Torque (STT) magnetoresistive memories. However, up to now, the only STT is not enough to achieve switching current density below 106 A/cm2. A recent work [Wang et al., Nature Mater., vol. 11, pp 64-68, Jan. 2012] has experimentally demonstrated the possibility to perform magnetization switching assisted by an electric-field at ultra-low current density. Theoretically, this switching has been studied by using a macrospin approach only. Here, we show a full micromagnetic study. We found that the switching occurs via a complex nucleation process including the nucleation of magnetic vortexes.",
    "We present a generalisation of Rosenblatt's traditional perceptron learning algorithm to the class of proximal activation functions and demonstrate how this generalisation can be interpreted as an incremental gradient method applied to a novel energy function. This novel energy function is based on a generalised Bregman distance, for which the gradient with respect to the weights and biases does not require the differentiation of the activation function. The interpretation as an energy minimisation algorithm paves the way for many new algorithms, of which we explore a novel variant of the iterative soft-thresholding algorithm for the learning of sparse perceptrons.",
    "The radiation force exerted on an object by an acoustic wave is a widely studied phenomenon since the early work of Rayleigh, Langevin and Brillouin and has led in the last decade to tremendous developments for acoustic micromanipulation. Despite extensive work on this phenomenon, the expressions of the acoustic radiation force applied on a particle have so far been derived only for a steady particle, hence neglecting the effect of its displacement on the radiated wave. In this work we study the acoustic radiation force exerted on a monopolar source translating at a constant velocity small compared to the sound speed. We demonstrate that the asymmetry of the emitted field resulting from Doppler effect induces a radiation force on the source opposite to its motion.",
    "Modelling the base of the solar convective envelope is a tedious problem. Since the first rotation inversions, solar modellers are confronted with the fact that a region of very limited extent has an enormous physical impact on the Sun. Indeed, it is the transition region from differential to solid body rotation, the tachocline, which furthermore is influenced by turbulence and is also supposed to be the seat of the solar magnetic dynamo. Moreover, solar models show significant disagreement with the sound speed profile in this region. In this paper, we show how helioseismology can provide further constraints on this region by carrying out an inversion of the Ledoux discriminant. We compare these inversions for Standard Solar Models built using various opacity tables and chemical abundances and discuss the origins of the discrepancies between Solar Models and the Sun.",
    "There is a clear desire to model and comprehend human behavior. Trends in research covering this topic show a clear assumption that many view human reasoning as the presupposed standard in artificial reasoning. As such, topics such as game theory, theory of mind, machine learning, etc. all integrate concepts which are assumed components of human reasoning. These serve as techniques to attempt to both replicate and understand the behaviors of humans. In addition, next generation autonomous and adaptive systems will largely include AI agents and humans working together as teams. To make this possible, autonomous agents will require the ability to embed practical models of human behavior, which allow them not only to replicate human models as a technique to \"learn\", but to to understand the actions of users and anticipate their behavior, so as to truly operate in symbiosis with them. The main objective of this paper it to provide a succinct yet systematic review of the most important approaches in two areas dealing with quantitative models of human behaviors. Specifically, we focus on (i) techniques which learn a model or policy of behavior through exploration and feedback, such as Reinforcement Learning, and (ii) directly model mechanisms of human reasoning, such as beliefs and bias, without going necessarily learning via trial-and-error.",
    "Breaking down botnets have always been a big challenge. The robustness of C&C channels is increased, and the detection of botmaster is harder in P2P botnets. In this paper, we propose a probabilistic method to reconstruct the topologies of the C&C channel for P2P botnets. Due to the geographic dispersion of P2P botnet members, it is not possible to supervise all members, and there does not exist all necessary data for applying other graph reconstruction methods. So far, no general method has been introduced to reconstruct C&C channel topology for all type of P2P botnet. In our method, the probability of connections between bots is estimated by using the inaccurate receiving times of several cascades, network model parameters of C&C channel, and end-to-end delay distribution of the Internet. The receiving times can be collected by observing the external reaction of bots to commands. The results of our simulations show that more than 90% of the edges in a 1000-member network with node degree mean 50, have been accurately estimated by collecting the inaccurate receiving times of 22 cascades. In case the receiving times of just half of the bots are collected, this accuracy of estimation is obtained by using 95 cascades.",
    "In grand unified theories (GUT), non-universal boundary conditions for the gaugino masses may arise at the unification scale, and affect the observability of the neutral MSSM Higgs bosons (h/H/A) at the LHC. The implications of such non-universal gaugino masses are investigated for the Higgs boson production in the SUSY cascade decay chain gluino --> squark quark, squark --> neutralino_2 quark, neutralino_2 --> neutralino_1 h/H/A, h/H/A --> b b-bar produced in pp interactions. In the singlet representation with universal gaugino masses only the light Higgs boson can be produced in this cascade with the parameter region of interest for us, while with non-universal gaugino masses heavy neutral MSSM Higgs boson production may dominate. The allowed parameter space in the light of the WMAP constraints on the cold dark matter relic density is investigated in the above scenarios for gaugino mass parameters. We also demonstrate that combination of representations can give the required amount of dark matter in any point of the parameter space. In the non-universal case we show that heavy Higgs bosons can be detected in the studied cascade in parameter regions with the WMAP preferred neutralino relic density.",
    "Subwavelength modulators play an indispensable role in integrated photonic-electronic circuits. Due to weak light-matter interactions, it is always a challenge to develop a modulator with a nanometer scale footprint, low switching energy, low insertion loss and large modulation depth. In this paper, we propose the design of a vanadium dioxide dual-mode plasmonic waveguide electroabsorption modulator using a metal-insulator-VO$_2$-insulator-metal (MIVIM) waveguide platform. By varying the index of vanadium dioxide, the modulator can route plasmonic waves through the low-loss dielectric insulator layer during the \"on\" state and high-loss VO$_2$ layer during the \"off\" state, thereby significantly reducing the insertion loss while maintaining a large modulation depth. This ultracompact waveguide modulator, for example, can achieve a large modulation depth of ~10dB with an active size of only 200x50x220nm$^3$ (or ~{\\lambda}$^3$/1700), requiring a drive-voltage of ~4.6V. This high performance plasmonic modulator could potentially be one of the keys towards fully-integrated plasmonic nanocircuits in the next-generation chip technology.",
    "As a car becomes more connected, a countermeasure against automobile theft has become a significant task in the real world. To respond to automobile theft, data mining, biometrics, and additional authentication methods are proposed. Among current countermeasures, data mining method is one of the efficient ways to capture the owner driver's unique characteristics. To identify the owner driver from thieves, previous works applied various algorithms toward driving data. Such data mining methods utilized supervised learning, thus required labeled data set. However, it is unrealistic to gather and apply the thief's driving pattern. To overcome this problem, we propose driver identification method with GAN. GAN has merit to build identification model by learning the owner driver's data only. We trained GAN only with owner driver's data and used trained discriminator to identify the owner driver. From actual driving data, we evaluated our identification model recognizes the owner driver well. By ensembling various driver authentication methods with the proposed model, we expect industry can develop automobile theft countermeasures available in the real world.",
    "Slow oscillations (SlO) of magnetoresistance is a convenient tool to measure electronic structure parameters in quasi-two-dimensional metals. We study the possibility to apply this method to multi-band conductors, e.g. to iron-based high-temperature superconducting materials. We show that SlO can be used to measure the interlayer transfer integral in multi-band conductors similar to single-band metals. In addition, the SlO allow to measure and compare the effective masses or the electron scattering rates in various bands.",
    "Recent progress in the field of precision calculations for Standard Model processes at the LHC is reviewed, highlighting examples of weak gauge-boson and Higgs-boson production, as discussed at the 27th Rencontres de Blois, 2015.",
    "This paper proposes a speech emotion recognition method based on speech features and speech transcriptions (text). Speech features such as Spectrogram and Mel-frequency Cepstral Coefficients (MFCC) help retain emotion-related low-level characteristics in speech whereas text helps capture semantic meaning, both of which help in different aspects of emotion detection. We experimented with several Deep Neural Network (DNN) architectures, which take in different combinations of speech features and text as inputs. The proposed network architectures achieve higher accuracies when compared to state-of-the-art methods on a benchmark dataset. The combined MFCC-Text Convolutional Neural Network (CNN) model proved to be the most accurate in recognizing emotions in IEMOCAP data.",
    "In this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efficiently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the benefit of variational semantic memory in boosting few-shot recognition.",
    "The relativistic four-quark equations with the open-charm and the open-strange are found in the framework of coupled-channel formalism. The dynamical mixing of the meson-meson states with the four-quark states is considered. The four-quark amplitudes including the quarks of four flavors (u, d, s, c) are constructed. The poles of these amplitudes determine the masses of tetraquarks. The mass values of the tetraquarks with the spin-parity JP=1-,2- are calculated.",
    "The Fisher Matrix is the backbone of modern cosmological forecasting. We describe the Fisher4Cast software: a general-purpose, easy-to-use, Fisher Matrix framework. It is open source, rigorously designed and tested and includes a Graphical User Interface (GUI) with automated LATEX file creation capability and point-and-click Fisher ellipse generation. Fisher4Cast was designed for ease of extension and, although written in Matlab, is easily portable to open-source alternatives such as Octave and Scilab. Here we use Fisher4Cast to present new 3-D and 4-D visualisations of the forecasting landscape and to investigate the effects of growth and curvature on future cosmological surveys. Early releases have been available at http://www.cosmology.org.za since May 2008 with 750 downloads in the first year. Version 2.2 is made public with this paper and includes a Quick Start guide and the code used to produce the figures in this paper, in the hope that it will be useful to the cosmology and wider scientific communities.",
    "The representation of knowledge based on first-order logic captures the richness of natural language and supports multiple probabilistic inference models. Although symbolic representation enables quantitative reasoning with statistical probability, it is difficult to utilize with machine learning models as they perform numerical operations. In contrast, knowledge embedding (i.e., high-dimensional and continuous vectors) is a feasible approach to complex reasoning that can not only retain the semantic information of knowledge but also establish the quantifiable relationship among them. In this paper, we propose recursive neural knowledge network (RNKN), which combines medical knowledge based on first-order logic with recursive neural network for multi-disease diagnosis. After RNKN is efficiently trained from manually annotated Chinese Electronic Medical Records (CEMRs), diagnosis-oriented knowledge embeddings and weight matrixes are learned. Experimental results verify that the diagnostic accuracy of RNKN is superior to that of some classical machine learning models and Markov logic network (MLN). The results also demonstrate that the more explicit the evidence extracted from CEMRs is, the better is the performance achieved. RNKN gradually exhibits the interpretation of knowledge embeddings as the number of training epochs increases.",
    "Several solenoids are usually installed in electron cooler device to guide the motion of the electron beam in the cooler. However, the solenoids also have influence to the ion beam in the cooler storage ring. The transverse motion of the ion beam in storage ring will become coupled, if the solenoids installed in the electron cooler are not compensated perfectly. In this paper, the coupled transverse motion due to the uncompensated cooler's solenoids of CSRm (The main storage ring in the IMP, Lan Zhou, China) is studied, and the coupled beam envelopes are calculated by a new method.",
    "A near-infrared excess is detected at the white dwarf PHL5038 in UKIDSS photometry, consistent with the presence of a cool, substellar companion. We have obtained H- and K-grism spectra and images of PHL5038 using NIRI on Gemini North. The target is spatially and spectrally resolved into two components; an 8000K DA white dwarf, and a likely L8 brown dwarf companion, separated by 0.94\". The spectral type of the secondary was determined using standard spectral indices for late L and T dwarfs. The projected orbital separation of the binary is 55AU, and so it becomes only the second known wide WD+dL binary to be found after GD165AB. This object could potentially be used as a benchmark for testing substellar evolutionary models at intermediate to older ages.",
    "We investigate the impact of dynamical streams and substructure on estimates of the local escape speed and total mass of Milky Way-mass galaxies from modelling the high velocity tail of local halo stars. We use a suite of high-resolution, magneto-hydrodynamical cosmological zoom-in simulations, which resolve phase space substructure in local volumes around solar-like positions. We show that phase space structure varies significantly between positions in individual galaxies and across the suite. Substructure populates the high velocity tail unevenly and leads to discrepancies in the mass estimates. We show that a combination of streams, sample noise and truncation of the high velocity tail below the escape speed leads to a distribution of mass estimates with a median that falls below the true value by $\\sim 20 \\%$, and a spread of a factor of 2 across the suite. Correcting for these biases, we derive a revised value for the Milky Way mass presented in Deason et al. of $1.29 ^{+0.37}_{-0.47} \\times 10^{12}$ $\\rm M_{\\odot}$.",
    "Using no conventional measurements in position space, information extraction rates exceeding one bit per photon are achieved by employing high-dimensional correlated orbital angular momentum (OAM) states for object recognition. The correlations are shown to be insensitive to axial rotation of the target object: the information structure of an object's joint OAM coincidence spectrum is unchanged even when the object undergoes random rotations between each measurement. Additionally, OAM correlations alone are shown to be sufficient for full image reconstruction of complex, off-axis objects, and novel object symmetries are observed in the phases of OAM-object interaction transition amplitudes. Variations in mutual information rates, due to off-axis translation in the beam field, are studied, and it is shown that object symmetry signatures and information rates are independent of environmental factors sufficiently far from the beam center. The results motivate dynamic scanning applications in contexts where symmetry and small numbers of noninvasive measurements are desired.",
    "During Parker Solar Probe's first two orbits there are widespread observations of rapid magnetic field reversals known as switchbacks. These switchbacks are extensively found in the near-Sun solar wind, appear to occur in patches, and have possible links to various phenomena such as magnetic reconnection near the solar surface. As switchbacks are associated with faster plasma flows, we questioned whether they are hotter than the background plasma and whether the microphysics inside a switchback is different to its surroundings. We have studied the reduced distribution functions from the Solar Probe Cup instrument and considered time periods with markedly large angular deflections, to compare parallel temperatures inside and outside switchbacks. We have shown that the reduced distribution functions inside switchbacks are consistent with a rigid phase space rotation of the background plasma. As such, we conclude that the proton core parallel temperature is the same inside and outside of switchbacks, implying that a T-V relationship does not hold for the proton core parallel temperature inside magnetic field switchbacks. We further conclude that switchbacks are consistent with Alfv\\'enic pulses travelling along open magnetic field lines. The origin of these pulses, however, remains unknown. We also found that there is no obvious link between radial Poynting flux and kinetic energy enhancements suggesting that the radial Poynting flux is not important for the dynamics of switchbacks.",
    "Under the Nainital-Cape Survey, eight $\\delta\\,$Scuti type pulsators have been discovered with the pulsation periods in the range of several minutes to few hours. In order to understand these observed pulsational variabilities, we have performed non-adiabatic linear stability analyses in models of these stars having mass in the range of 1 to 3 M$_{\\odot}$. Several low order p-modes are found to be unstable where the pulsation periods associated with these unstable modes are in good agreement with the observed periods. Particularly for HD$\\,$118660, HD$\\,$113878, HD$\\,$102480, HD$\\,$98851, and HD$\\,$25515, we demonstrate that the observed variabilities can be explained with the low order radial p-mode pulsations.",
    "A new perspective on the classical mechanical formulation of particle trajectories in lorentz-violating theories is presented. Using the extended hamiltonian formalism, a Legendre Transformation between the associated covariant Lagrangian and Hamiltonian varieties is constructed. This approach enables calculation of trajectories using hamilton's equations in momentum space and the Euler-Lagrange equations in velocity space away from certain singular points that arise in the theory. Singular points are naturally de-singularized by requiring the trajectories to be smooth functions of both velocity and momentum variables. In addition, it is possible to identify specific sheets of the dispersion relations that correspond to specific solutions for the lagrangian. Examples corresponding to bipartite Finsler functions are computed in detail. A direct connection between the lagrangians and the field-theoretic solutions to the Dirac equation is also established for a special case.",
    "Spectrum management has been identified as a crucial step towards enabling the technology of a cognitive radio network (CRN). Most of the current works dealing with spectrum management in the CRN focus on a single task of the problem, e.g., spectrum sensing, spectrum decision, spectrum sharing or spectrum mobility. In this two-part paper, we argue that for certain network configurations, jointly performing several tasks of the spectrum management improves the spectrum efficiency. Specifically, our aim is to study the uplink resource management problem in a CRN where there exist multiple cognitive users (CUs) and access points (APs). The CUs, in order to maximize their uplink transmission rates, have to associate to a suitable AP (spectrum decision), and to share the channels used by this AP with other CUs (spectrum sharing). These tasks are clearly interdependent, and the problem of how they should be carried out efficiently and in a distributed manner is still open in the literature.",
    "A simple, exactly solvable statistical model is presented for the description of baryonic matter in the thermodynamic conditions associated to the evolution of core-collapsing supernova. It is shown that the model presents a first order phase transition in the grandcanonical ensemble which is not observed in the canonical ensemble. Similar to other model systems studied in condensed matter physics, this ensemble in-equivalence is accompanied by negative susceptibility and discontinuities in the intensive observables conjugated to the order parameter. This peculiar behavior originates from the fact that baryonic matter is subject to attractive short range strong forces as well as repulsive long range electromagnetic interactions, partially screened by a background of electrons. As such, it is expected in any theoretical treatment of nuclear matter in the stellar environment. Consequences for the phenomenology of supernova dynamics are drawn.",
    "To meet requirements of high performance THz-FEL (Free Electron Laser), a compact scheme of FEL injector was proposed. Thermionic cathode was chosen to emit electrons instead of photo-cathode with complex structure and high cost. The effective bunch charge was improved to ~200pC by adopting enhanced EC-ITC (External Cathode Independently Tunable Cells) RF gun to extract micro-bunches, and back bombardment effects were almost eliminated as well. Constant gradient accelerator structures were designed to improve energy to ~14MeV, while focusing system was applied for emittance suppressing and bunch state maintenance. Physical design and beam dynamics of key components for FEL injector were analyzed. Furthermore, start-to-end simulations with multi-pulses were performed by using homemade MATLAB and Parmela. The results show that continual high brightness electron bunches with low energy spread and emittance could be obtained stably.",
    "Several claims have been made of anomalies in the large-angle properties of the cosmic microwave background anisotropy as measured by WMAP. In most cases, the statistical significance of these anomalies is hard or even impossible to assess, due to the fact that the statistics used to quantify the anomalies were chosen a posteriori. On the other hand, the possibility of detecting new physics on the largest observable scales is so exciting that, in my opinion, it is worthwhile to examine the claims carefully. I will focus on three particular claims: the lack of large-angle power, the north-south power asymmetry, and multipole alignments. In all cases, the problem of a posteriori statistics can best be solved by finding a new data set that probes similar physical scales to the large-angle CMB. This is a difficult task, but there are some possible routes to achieving it.",
    "Multi-photon states can be produced in multiple parametric down conversion (PDC) processes. The nonlinear crystal in such a case is pumped with high power. In theory, the more populated these states are, the deeper is the conflict with local realistic description. However, the interference contrast in multi-photon PDC experiments can be quite low for high pumping. We show how the contrast can be improved. The idea employs currently accessible optical devices, the multiport beam splitters. They are capable of splitting the incoming light in one input mode to $M$ output modes. Our scheme works as a POVM filter. It may provide a feasible CHSH-Bell inequality test, and thus can be useful in e.g. schemes reducing communication complexity.",
    "The spectrum of exponents of the transfer matrix provides the localization lengths of Anderson's model for a particle in a lattice with disordered potential. I show that a duality identity for determinants and Jensen's identity for subharmonic functions, give a formula for the spectrum in terms of eigenvalues of the Hamiltonian with non-Hermitian boundary conditions. The formula is exact; it involves an average over a Bloch phase, rather than disorder. A preliminary investigation of non-Hermitian spectra of Anderson's model in D=1,2 and on the smallest exponent is presented.",
    "In this article, we improve extreme learning machines for regression tasks using a graph signal processing based regularization. We assume that the target signal for prediction or regression is a graph signal. With this assumption, we use the regularization to enforce that the output of an extreme learning machine is smooth over a given graph. Simulation results with real data confirm that such regularization helps significantly when the available training data is limited in size and corrupted by noise.",
    "To properly describe heating in weakly collisional turbulent plasmas such as the solar wind, inter-particle collisions should be taken into account. Collisions can convert ordered energy into heat by means of irreversible relaxation towards the thermal equilibrium. Recently, Pezzi et al. (Phys. Rev. Lett., vol. 116, 2016, p. 145001) showed that the plasma collisionality is enhanced by the presence of fine structures in velocity space. Here, the analysis is extended by directly comparing the effects of the fully nonlinear Landau operator and a linearized Landau operator. By focusing on the relaxation towards the equilibrium of an out of equilibrium distribution function in a homogeneous force-free plasma, here it is pointed out that it is significant to retain nonlinearities in the collisional operator to quantify the importance of collisional effects. Although the presence of several characteristic times associated with the dissipation of different phase space structures is recovered in both the cases of the nonlinear and the linearized operators, the influence of these times is different in the two cases. In the linearized operator case, the recovered characteristic times are systematically larger than in the fully nonlinear operator case, this suggesting that fine velocity structures are dissipated slower if nonlinearities are neglected in the collisional operator.",
    "In this research work, we have demonstrated the application of Mask-RCNN (Regional Convolutional Neural Network), a deep-learning algorithm for computer vision and specifically object detection, to semiconductor defect inspection domain. Stochastic defect detection and classification during semiconductor manufacturing has grown to be a challenging task as we continuously shrink circuit pattern dimensions (e.g., for pitches less than 32 nm). Defect inspection and analysis by state-of-the-art optical and e-beam inspection tools is generally driven by some rule-based techniques, which in turn often causes to misclassification and thereby necessitating human expert intervention. In this work, we have revisited and extended our previous deep learning-based defect classification and detection method towards improved defect instance segmentation in SEM images with precise extent of defect as well as generating a mask for each defect category/instance. This also enables to extract and calibrate each segmented mask and quantify the pixels that make up each mask, which in turn enables us to count each categorical defect instances as well as to calculate the surface area in terms of pixels. We are aiming at detecting and segmenting different types of inter-class stochastic defect patterns such as bridge, break, and line collapse as well as to differentiate accurately between intra-class multi-categorical defect bridge scenarios (as thin/single/multi-line/horizontal/non-horizontal) for aggressive pitches as well as thin resists (High NA applications). Our proposed approach demonstrates its effectiveness both quantitatively and qualitatively.",
    "We give a new bound on the parameter $\\lambda$ (number of common neighbors of a pair of adjacent vertices) in a distance-regular graph $G$, improving and generalizing bounds for strongly regular graphs by Spielman (1996) and Pyber (2014). The new bound is one of the ingredients of recent progress on the complexity of testing isomorphism of strongly regular graphs (Babai, Chen, Sun, Teng, Wilmes 2013). The proof is based on a clique geometry found by Metsch (1991) under certain constraints on the parameters. We also give a simplified proof of the following asymptotic consequence of Metsch's result: if $k\\mu = o(\\lambda^2)$ then each edge of $G$ belongs to a unique maximal clique of size asymptotically equal to $\\lambda$, and all other cliques have size $o(\\lambda)$. Here $k$ denotes the degree and $\\mu$ the number of common neighbors of a pair of vertices at distance 2. We point out that Metsch's cliques are \"asymptotically Delsarte\" when $k\\mu = o(\\lambda^2)$, so families of distance-regular graphs with parameters satisfying $k\\mu = o(\\lambda^2)$ are \"asymptotically Delsarte-geometric.\"",
    "Recent galaxy observations show that star formation activity changes depending on galactic environments. In order to understand the diversity of galactic-scale star formation, it is crucial to understand the formation and evolution of giant molecular clouds in an extreme environment. We focus on observational evidence that bars in strongly barred galaxies lack massive stars even though quantities of molecular gas are sufficient to form stars. In this paper, we present a hydrodynamical simulation of a strongly barred galaxy, using a stellar potential which is taken from observational results of NGC1300, and we compare cloud properties between different galactic environments: bar, bar-end and spiral arms. We find that the mean of cloud's virial parameter is ~1 and that there is no environmental dependence, indicating that the gravitationally-bound state of a cloud is not behind the observational evidence of the lack of massive stars in strong bars. Instead, we focus on cloud-cloud collisions, which have been proposed as a triggering mechanism for massive star formation. We find that the collision speed in the bar is faster than those in the other regions. We examine the collision frequency using clouds' kinematics and conclude that the fast collisions in the bar could originate from random-like motion of clouds due to elliptical gas orbits shifted by the bar potential. These results suggest that the observed regions of lack of active star-formation in the strong bar originate from the fast cloud-cloud collisions, which are inefficient in forming massive stars, due to the galactic-scale violent gas motion.",
    "Context: The mass-metallicity relationship (MMR) of star-forming galaxies is well-established, however there is still some disagreement with respect to its exact shape and its possible dependence on other observables. Aims: We measure the MMR in the Galaxy And Mass Assembly (GAMA) survey. We compare our measured MMR to that measured in the Sloan Digital Sky Survey (SDSS) and study the dependence of the MMR on various selection criteria to identify potential causes for disparities seen in the literature. Methods: We use strong emission line ratio diagnostics to derive oxygen abundances. We then apply a range of selection criteria for the minimum signal-to-noise in various emission lines, as well as the apparent and absolute magnitude to study variations in the inferred MMR. Results: The shape and position of the MMR can differ significantly depending on the metallicity calibration and selection used. After selecting a robust metallicity calibration amongst those tested, we find that the mass-metallicity relation for redshifts 0.061< z<0.35 in GAMA is in reasonable agreement with that found in the SDSS despite the difference in the luminosity range probed. Conclusions: In view of the significant variations of the MMR brought about by reasonable changes in the sample selection criteria and method, we recommend that care be taken when comparing the MMR from different surveys and studies directly. We also conclude that there could be a modest level of evolution over 0.06<z<0.35 within the GAMA sample.",
    "Based on thermodynamic considerations we derive a set of equations relating the seepage velocities of the fluid components in immiscible and incompressible two-phase flow in porous media. They necessitate the introduction of a new velocity function, the co-moving velocity. This velocity function is a characteristic of the porous medium. Together with a constitutive relation between the velocities and the driving forces, such as the pressure gradient, these equations form a closed set. We solve four versions of the capillary tube model analytically using this theory. We test the theory numerically on a network model.",
    "Nature's spectacular inventiveness, reflected in the enormous diversity of form and function displayed by the biosphere, is a feature of life that distinguishes living most strongly from nonliving. It is, therefore, not surprising that this aspect of life should become a central focus of artificial life. We have known since Darwin that the diversity is produced dynamically, through the process of evolution; this has led life's creative productivity to be called Open-Ended Evolution (OEE) in the field. This article introduces the second of two special issues on current research in OEE and provides an overview of the contents of both special issues. Most of the work was presented at a workshop on open-ended evolution that was held as a part of the 2018 Conference on Artificial Life in Tokyo, and much of it had antecedents in two previous workshops on open-ended evolution at artificial life conferences in Cancun and York. We present a simplified categorization of OEE and summarize progress in the field as represented by the articles in this special issue.",
    "The properties of MgO/Ag(001) ultrathin films with substitutional Mg atoms in the interface metal layer have been investigated by means of Auger electron diffraction experiments, ultraviolet photoemission spectroscopy, and density functional theory (DFT) calculations. Exploiting the layer-by-layer resolution of the Mg KL_23 L_23 Auger spectra and using multiple scattering calculations, we first determine the interlayer distances as well as the morphological parameters of the MgO/Ag(001) system with and without Mg atoms incorporated at the interface. We find that the Mg atoms incorporation drives a strong distortion of the interface layers and that its impact on the metal/oxide electronic structure is an important reduction of the work function (0.5 eV) related to band-offset variations at the interface. These experimental observations are in very good agreement with our DFT calculations which reproduce the induced lattice distortion and which reveal (through a Bader analysis) that the increase of the interface Mg concentration results in an electron transfer from Mg to Ag atoms of the metallic interface layer. Although the local lattice distortion appears as a consequence of the attractive (repulsive) Coulomb interaction between O2- ions of the MgO interface layer and the nearest positively (negatively) charged Mg (Ag) neighbors of the metallic interface layer, its effect on the work function reduction is only limited. Finally, an analysis of the induced work function changes in terms of charge transfer, rumpling, and electrostatic compression contributions is attempted and reveals that the metal/oxide work function changes induced by interface Mg atoms incorporation are essentially driven by the increase of the electrostatic compression effect.",
    "The need to monitor industrial processes, detecting changes in process parameters in order to promptly correct problems that may arise, generates a particular area of interest. This is particularly critical and complex when the measured value falls below the sensitivity limits of the measuring system or below detection limits, causing much of their observations are incomplete. Such observations to be called incomplete observations or left censored data. With a high level of censorship, for example greater than 70%, the application of traditional methods for monitoring processes is not appropriate. It is required to use appropriate data analysis statistical techniques, to assess the actual state of the process at any time. This paper proposes a way to estimate process parameters in such cases and presents the corresponding control chart, from an algorithm that is also presented.",
    "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",
    "We refine a stimulating study by Sarvotham et al. [2005] which highlighted the influence of peak transmission rate on network burstiness. From TCP packet headers, we amalgamate packets into sessions where each session is characterized by a 5-tuple (S, D, R, Peak R, Initiation T)=(total payload, duration, average transmission rate, peak transmission rate, initiation time). After careful consideration, a new definition of peak rate is required. Unlike Sarvotham et al. [2005] who segmented sessions into two groups labelled alpha and beta, we segment into 10 sessions according to the empirical quantiles of the peak rate variable as a demonstration that the beta group is far from homogeneous. Our more refined segmentation reveals additional structure that is missed by segmentation into two groups. In each segment, we study the dependence structure of (S, D, R) and find that it varies across the groups. Furthermore, within each segment, session initiation times are well approximated by a Poisson process whereas this property does not hold for the data set taken as a whole. Therefore, we conclude that the peak rate level is important for understanding structure and for constructing accurate simulations of data in the wild. We outline a simple method of simulating network traffic based on our findings.",
    "The Brouwer fixed-point theorem in topology states that for any continuous mapping $f$ on a compact convex set into itself admits a fixed point, i.e., a point $x_0$ such that $f(x_0)=x_0$. Under certain conditions, this fixed point corresponds to the throat of a traversable wormhole, i.e., $b(r_0)=r_0$ for the shape function $b=b(r)$. The possible existence of wormholes can therefore be deduced from purely mathematical considerations without going beyond the existing physical requirements.",
    "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.",
    "As well known, the spectrum of a non-relativistic two-body system interacting by the Coulomb potential is the Balmer series $E_n=\\frac{\\alpha^2m}{4n^2}$ produced by the Schr\\\"odinger equation. In 1954, Wick and Cutkosky have found, in the Bethe-Salpeter equation framework, that for $\\alpha>\\frac{\\pi}{4}$ the relativistic effects result in new levels (in addition to the Balmer series). However, the physical nature of these new states remained unclear and therefore their existence was being questioned. We have recently shown that these extra states are dominated by the exchange (massless) particles, moving with speed of light. That's why they did not appear in the non-relativistic (Schr\\\"odinger) framework.",
    "We study the fundamental properties of the quantum f-relative entropy, where f(.) is an operator convex function. We give the equality conditions under monotonicity and joint convexity, and these conditions are more general than, since they hold for a class of operator convex functions, and different for f(t) = -ln(t) from, the previously known conditions. The quantum f-entropy is defined in terms of the quantum f-relative entropy and we study its properties giving the equality conditions in some cases. We then show that the f-generalizations of the Holevo information, the entanglement-assisted capacity, and the coherent information also satisfy the data processing inequality, and give the equality conditions for the f-coherent information.",
    "Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch. H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.",
    "We measure and analyze reflection spectra of directly coupled systems of waveguides and cavities. The observed Fano lines offer insight in the reflection and coupling processes. Very different from side-coupled systems, the observed Fano line shape is not caused by the termini of the waveguide, but the coupling process between the measurement device fiber and the waveguide. Our experimental results and analytical model show that the Fano parameter that describes the Fano line shape is very sensitive to the coupling condition. A movement of the fiber well below the Rayleigh range can lead to a drastic change of the Fano line shape.",
    "The strength and vertical distribution of atmospheric turbulence is a key factor determining the performance of optical and infrared telescopes, with and without adaptive optics. Yet, this remains challenging to measure. We describe a new technique using a sequence of short-exposure images of a star field, obtained with a small telescope. Differential motion between all pairs of star images is used to compute the structure functions of longitudinal and transverse wavefront tilt for a range of angular separations. These are compared with theoretical predictions of simple turbulence models by means of a Markov-Chain Monte-Carlo optimization. The method is able to estimate the turbulence profile in the lower atmosphere, the total and free-atmosphere seeing, and the outer scale. We present results of Monte-Carlo simulations used to verify the technique, and show some examples using data from the second AST3 telescope at Dome A in Antarctica.",
    "We define an n-plectic structure as a commutative and torsionless Lie Rinehart pair, together with a distinguished cocycle from its Chevalley-Eilenberg complex. This 'n-plectic cocycle' gives rise to an extension of the Chevalley-Eilenberg complex by so called symplectic tensors. The cohomology of this extension generalizes Hamiltonian functions and vector fields to tensors and cotensors in a range of degrees, up to certain coboundaries and has the structure of a Lie oo-algebra. Finally we show, that momentum maps appear in this context just as weak Lie oo-morphisms from an arbitrary Lie oo-algebra into the Lie oo-algebra of Hamiltonian (co)tensors.",
    "Amorphous solids or glasses are known to exhibit stretched-exponential decay over broad time intervals in several of their macroscopic observables: intermediate scattering function, dielectric relaxation modulus, time-elastic modulus etc. This behaviour is prominent especially near the glass transition. In this Letter we show, on the example of dielectric relaxation, that stretched-exponential relaxation is intimately related to the peculiar lattice dynamics of glasses. By reformulating the Lorentz model of dielectric matter in a more general form, we express the dielectric response as a function of the vibrational density of states (DOS) for a random assembly of spherical particles interacting harmonically with their nearest-neighbours. Surprisingly we find that near the glass transition for this system (which coincides with the Maxwell rigidity transition), the dielectric relaxation is perfectly consistent with stretched-exponential behaviour with Kohlrausch exponents $0.56 < \\beta < 0.65$, which is the range where exponents are measured in most experimental systems. Crucially, the root cause of stretched-exponential relaxation can be traced back to soft modes (boson-peak) in the DOS.",
    "To highlight the challenges of achieving representation disentanglement for text domain in an unsupervised setting, in this paper we select a representative set of successfully applied models from the image domain. We evaluate these models on 6 disentanglement metrics, as well as on downstream classification tasks and homotopy. To facilitate the evaluation, we propose two synthetic datasets with known generative factors. Our experiments highlight the existing gap in the text domain and illustrate that certain elements such as representation sparsity (as an inductive bias), or representation coupling with the decoder could impact disentanglement. To the best of our knowledge, our work is the first attempt on the intersection of unsupervised representation disentanglement and text, and provides the experimental framework and datasets for examining future developments in this direction.",
    "This paper proposes a hybrid quantum-classical algorithm to solve a fundamental power system problem called unit commitment (UC). The UC problem is decomposed into a quadratic subproblem, a quadratic unconstrained binary optimization (QUBO) subproblem, and an unconstrained quadratic subproblem. A classical optimization solver solves the first and third subproblems, while the QUBO subproblem is solved by a quantum algorithm called quantum approximate optimization algorithm (QAOA). The three subproblems are then coordinated iteratively using a three-block alternating direction method of multipliers algorithm. Using Qiskit on the IBM Q system as the simulation environment, simulation results demonstrate the validity of the proposed algorithm to solve the UC problem.",
    "It has also been suggested that the detection of a wealth of very low amplitude modes in Delta Sct stars was only a matter of signal--to--noise ratio. Access to this treasure, impossible from the ground, is one of the scientific aims of the space mission CoRoT, a space mission developed and operated by CNES. This work presents the results obtained on HD 50844: the 140,016 datapoints were analysed using independent approaches and several checks performed. A level of 10^{-5} mag was reached in the amplitude spectra of the CoRoT timeseries. The frequency analysis of the CoRoT timeseries revealed hundreds of terms in the frequency range 0--30 d^{-1}. All the cross--checks confirmed this new result. The initial guess that Delta Sct stars have a very rich frequency content is confirmed. The spectroscopic mode identification gives theoretical support since very high--degree modes (up to ell=14) are identified. We also prove that cancellation effects are not sufficient in removing the flux variations associated to these modes at the noise level of the CoRoT measurements. The ground--based observations indicate that HD 50844 is an evolved star that is slightly underabundant in heavy elements, located on the Terminal Age Main Sequence. Probably due to this unfavourable evolutionary status, no clear regular distribution is observed in the frequency set. The predominant term (f_1=6.92 d^{-1}) has been identified as the fundamental radial mode combining ground-based photometric and spectroscopic data. This work is also based on observations made with ESO telescopes under the ESO Large Programme LP178.D-0361 and on data collected at the Observatorio de Sierra Nevada, at the Observatorio Astronomico Nacional San Pedro Martir, and at the Piszkesteto Mountain Station of Konkoly Observatory.",
    "The article deals with observations of star-forming regions S231-S235 in 'quasi-thermal' lines of ammonia (NH$_3$), cyanoacetylene (HC$_3$N) and maser lines of methanol (CH$_3$OH) and water vapor (H$_2$O). S231-S235 regions is situated in the giant molecular cloud G174+2.5. We selected all massive molecular clumps in G174+2.5 using archive CO data. For the each clump we determined mass, size and CO column density. After that we performed observations of these clumps. We report about first detections of NH$_3$ and HC$_3$N lines toward the molecular clumps WB89 673 and WB89 668. This means that high-density gas is present there. Physical parameters of molecular gas in the clumps were estimated using the data on ammonia emission. We found that the gas temperature and the hydrogen number density are in the ranges 16-30 K and 2.8-7.2$\\times10^3$ cm$^{-3}$, respectively. The shock-tracing line of CH$_3$OH molecule at 36.2 GHz is newly detected toward WB89 673.",
    "We report the lowest frequency measurements of gamma-ray burst (GRB) 171205A with the upgraded Giant Metrewave Radio Telescope (uGMRT) covering a frequency range from 250--1450 MHz and a period of $4-937$ days. It is the first GRB afterglow detected at 250--500 MHz frequency range and the second brightest GRB detected with the uGMRT. Even though the GRB is observed for nearly 1000 days, there is no evidence of transition to non-relativistic regime. We also analyse the archival ${\\it Chandra}$ X-ray data on day $\\sim 70$ and day $\\sim 200$. We also find no evidence of a jet break from the analysis of combined data. We fit synchrotron afterglow emission arising from a relativistic, isotropic, self-similar deceleration as well as from a shock-breakout of wide-angle cocoon. Our data also allow us to discern the nature and the density of the circumburst medium. We find that the density profile deviates from a standard constant density medium and suggests that the GRB exploded in a stratified wind like medium. Our analysis shows that the lowest frequency measurements covering the absorbed part of the light curves are critical to unravel the GRB environment. Our data combined with other published measurements indicate that the radio afterglow has contribution from two components: a weak, possibly slightly off-axis jet and a surrounding wider cocoon, consistent with the results of Izzo et al. (2019). The cocoon emission likely dominates at early epochs, whereas the jet starts to dominate at later epochs, resulting in flatter radio lightcurves.",
    "The recently developed theory of quasi-Lie schemes is studied and applied to investigate several equations of Emden type and a scheme to deal with them and some of their generalisations is given. As a first result we obtain t-dependent constants of the motion for particular instances of Emden equations by means of some of their particular solutions. Previously known results are recovered from this new perspective. Finally some t-dependent constants of the motion for equations of Emden type satisfying certain conditions are recovered.",
    "We undertake the study of the charged Higgs bosons predicted by the model with gauge symmetry $SU(3)_c \\otimes SU(3)_L \\otimes U(1)_X$. By considering Yukawa mixing couplings between small ($\\sim$ GeV) and large ($\\sim$ TeV) scales, we show that the hypercharge-one $H_1^{\\pm}$ and hypercharge-two $H_2^{\\pm}$ Higgs bosons predicted by the model, can be simultaneously produced in $pp$ collisions at different production rates. At low energy, the $H_1^{\\pm}$ bosons exhibit the same properties as the charged Higgs bosons from a two Higgs doublet model (2HDM), while $H_2^{\\pm}$ are additional like-charged Higgs bosons from the underlying 3-3-1 model. Thus, the identification of multiple like-charged Higgs boson resonances may test the compatibility of theoretical models with experimental data. We study $H_{1,2}^{\\pm}$ pair and associated $tbH_{1,2}^{\\pm}$ productions at CERN LHC collider. In particular, we obtain that pair production can be as large as the single production in gluon-gluon collisions due to the interchange of a heavy neutral $Z'$ gauge boson predicted by the model. By considering decays to leptons $H_{1,2}^{\\pm} \\rightarrow \\tau \\nu_{\\tau}$, we obtain scenarios where small peaks of $H_{2}^{\\pm}$-boson events in transverse mass distributions can be identified over the $H_{1}^{\\pm}$ background.",
    "Isospin breaking in the $K_{\\ell 4}$ form factors induced by the difference between charged and neutral pion masses is discussed within a framework built on suitably subtracted dispersion representations. The $K_{\\ell 4}$ form factors are constructed in an iterative way up to two loops in the low-energy expansion by implementing analyticity, crossing, and unitarity due to two-meson intermediate states. Analytical expressions for the phases of the two-loop form factors of the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ channel are presented, allowing one to connect the difference of form-factor phase shifts measured experimentally (out of the isospin limit) and the difference of $S$- and $P$-wave $\\pi\\pi$ phase shifts studied theoretically (in the isospin limit). The dependence with respect to the two $S$-wave scattering lengths $a_0^0$ and $a_0^2$ in the isospin limit is worked out in a general way, in contrast to previous analyses based on one-loop chiral perturbation theory. The results on the phases of the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ form factors obtained by the NA48/2 collaboration at the CERN SPS are reanalysed including isospin-breaking correction to extract values for the scattering lengths $a_0^0$ and $a_0^2$.",
    "In this paper we use and extend the results present in \\cite{1,2,3,4} and in particular in \\cite{4} to obtain a statistical description of the cosmological constant in a cosmological de Sitter universe in terms of massless excitations with Planckian effects. First of all, we show that at a classical level, the cosmological constant $\\Lambda>0$ can be obtained only for $T\\rightarrow 0$. Similarly to the black hole case, when quantum effects are taken into account, a representation for $\\Lambda$ is possible in terms of massless excitations, provided that quantum corrections to the Misner-Sharp mass are considered. Moreover, thanks to quantum fluctuations, an effective cosmological constant arises depending on the physical scale under consideration, thus representing a possible solution to the cosmological constant problem without introducing a quintessence field. The smalness of the actual value for $\\Lambda$ can be due to the existence of a quantum decoherence scale above the Planck length such that the spacetime evolves as a pure de Sitter universe with a small averaged cosmological constant frozen in the lowest energy state.",
    "We investigate zero and finite temperature properties of the one-dimensional spin-glass model for vector spins in the limit of an infinite number m of spin components where the interactions decay with a power, \\sigma, of the distance. A diluted version of this model is also studied, but found to deviate significantly from the fully connected model. At zero temperature, defect energies are determined from the difference in ground-state energies between systems with periodic and antiperiodic boundary conditions to determine the dependence of the defect-energy exponent \\theta on \\sigma. A good fit to this dependence is \\theta =3/4-\\sigma. This implies that the upper critical value of \\sigma is 3/4, corresponding to the lower critical dimension in the d-dimensional short-range version of the model. For finite temperatures the large m saddle-point equations are solved self-consistently which gives access to the correlation function, the order parameter and the spin-glass susceptibility. Special attention is paid to the different forms of finite-size scaling effects below and above the lower critical value, \\sigma =5/8, which corresponds to the upper critical dimension 8 of the hypercubic short-range model.",
    "The members of the scarce category of Of^+ supergiants present properties that are intermediate between regular O-stars and Wolf-Rayet (WR) stars. Significant similarities between these transitional stars and WN-type objects are now clearly established, at least in the visible and near-infrared domains, pointing to common stellar wind properties. In this study, we report on the first dedicated X-ray observations of HD16691 (O4If^+) and HD14947 (O5f^+), revealing a soft thermal spectrum in agreement with the expected X-ray emission from a single O-type star. However, the X-ray luminosity of our targets is slightly lower than expected for single O-type stars, suggesting that the particular properties of their stellar wind has also a significant impact on the X-ray emission of these objects on the way to the WN category. We argue that the X-ray under-luminosity of HD16691 and HD14947 may be interpreted as the signature in X-rays of the intermediate stage between O and WR stars, as a consequence of enhanced wind density.",
    "The AARTFAAC project aims to implement an All-Sky Monitor (ASM), using the Low Frequency Array (LOFAR) telescope. It will enable real-time, 24x7 monitoring for low frequency radio transients over most of the sky locally visible to the LOFAR at timescales ranging from milliseconds to several days, and rapid triggering of follow-up observations with the full LOFAR on detection of potential transient candidates. These requirements pose several implementation challenges: imaging of an all-sky field of view, low latencies of processing, continuous availability and autonomous operation of the ASM. The first of these has already resulted in the correlator for the ASM being the largest in the world in terms of its number of input channels. It will generate $\\sim 1.5 \\cdot 10^5$ correlations per second per spectral channel when built. Test observations using existing LOFAR infrastructure were carried out to quantify and constrain crucial instrumental design criteria for the ASM. In this paper, we present an overview of the AARTFAAC data processing pipeline and illustrate some of the aforementioned challenges by showing all-sky images obtained from one of the test observations. These results provide quantitative estimates of the capabilities of the instrument.",
    "Wolf-Rayet (WR) stars are the evolved descendants of massive O-type stars and are considered to be progenitor candidates for Type Ib/c core-collapse supernovae (SNe). Recent results of our HST/WFC3 survey of Wolf-Rayet stars in M101 are summarised based on the detection efficiency of narrow-band optical imaging compared to broad-band methods. Weshow that on average of 42% WR stars, increasing to ~85% in central regions, are only detected in the narrow-band imaging. Hence, the non-detection of a WR star at the location of ~10 Type Ib/c SNe in broad-band imaging is no longer strong evidence for a non-WR progenitor channel."
  ],
  "sampled": [
    "Process calculi based on logic, like $\\pi$DILL and CP, establish a framework for creating concurrent programs that avoid deadlocks. However, a discrepancy exists between the proof construction rules and term constructors in the $\\pi$-calculus, where the key operator for parallel composition does not align with any linear logic rule. In their 2019 work, Kokke et al. introduced Hypersequent Classical Processes (HCP) to resolve this mismatch by employing hypersequents (sets of sequents) to denote parallelism in typing judgments. Transitioning from CP to HCP marks a significant development. HCP currently lacks reduction semantics, and incorporating delayed actions disrupts the expected behavior of CP processes when interpreted as HCP processes. To address these issues, we present HCP-, a modified version of HCP that includes reduction semantics but excludes delayed actions. We demonstrate progress, preservation, and termination results and confirm that HCP- accommodates the same communication protocols as CP.",
    "A BDDC preconditioner variant with constraints on selected subobjects is introduced, ensuring a bounded preconditioner condition number of $C \\big(1+\\log (L/h)\\big)^2$. As $L$ is flexible, the condition number can be as small as $O(1). We will explore the benefits, drawbacks, and applications for heterogeneous problems, supported by numerical results on supercomputers.",
    "Examples are provided where the Heun function exists as solutions of wave equations encountered in general relativity. The Dirac equation, when formulated with the Nutku helicoid metric background in four-dimensional spacetime, gives rise to Mathieu functions as its solutions. The extension to five dimensions yields the double confluent Heun function. By applying specific transformations, we simplify this solution to the Mathieu function. The imposition of Atiyah-Patodi-Singer spectral boundary conditions is necessary for this system due to the singularity present at the origin in the metric.",
    "Numerous authors have demonstrated that the gradual decrease in X-rays seen during the decay stage of long-duration flares (LDE) can be attributed solely to ongoing magnetic reconnection and energy release in the coronal region of a flare. Utilizing RHESSI data, we aim to address two key inquiries: How impactful are these mechanisms during the decay phase of LDEs, and how can we accurately calculate the energy release rate based on this information? By reconstructing images of selected LDEs during their decay phase and conducting spectral analysis, we obtained the physical characteristics of flare coronal sources, enabling us to assess the efficiency of the energy release process. Additionally, we analyzed the components of the energy equation to determine the precision of each term's determination.",
    "We employ a multi-scale analysis to characterize the distinctive geometrical arrangement of clusters under the FK measure in random media. Our findings are valid in dimensions equal to or greater than 2, assuming slab percolation takes place under the averaged measure, a condition likely to exist throughout the supercritical phase. This study builds upon Pisztora's work and presents a crucial analytical tool for investigating the supercritical regime in disordered FK models as well as in disordered Ising and Potts models.",
    "Classical T Tauri stars (CTTS) exhibit weak photospheric absorption lines compared to normal stars, a phenomenon known as veiling. This veiling is typically attributed to excess continuous emission originating in shock-heated gas at the stellar surface beneath the accretion streams. Our study focuses on four stars (RW Aur A, RU Lup, S CrA NW, and S CrA SE) with unusually strong veiling to conduct a thorough investigation of veiling in relation to stellar brightness and emission line strengths for comparison with standard accretion models. Through photometric and spectroscopic monitoring at various time points, we aim to explore how a variable accretion rate in standard models translates to fluctuating excess emission and corresponding changes in stellar brightness.\n\nOur findings reveal that the veiling of absorption lines in these stars exhibits significant variability, often requiring the release of multiple stellar luminosities of potential energy due to large veiling factors. Interestingly, during periods of pronounced line dilution, the derived veiling factors do not show a strong correlation with brightness. Furthermore, the emission line strengths defy the anticipated relationship between veiling and line strength. Notably, the veiling can undergo substantial changes within a single night and does not align with the rotational phases in two stars.\n\nWe demonstrate that in at least three of the stars, high veiling results in the filling-in of photospheric lines by line emission, generating substantial veiling factors independent of changes in continuous emission from shocked regions. We also consider the potential impact of dust extinction and electron scattering in the accretion stream on veiling measurements in CTTS. Ultimately, we assert that the degree of veiling cannot serve as a reliable indicator of accretion rates in CTTS with complex emission line spectra.",
    "Giant low surface brightness (GLSB) galaxies have been traditionally viewed as large, dark matter-dominated entities. However, this perspective has been formed mainly on the basis of rotation curves that are subject to significant uncertainties. In this study, we focus on two iconic GLSB galaxies, Malin 1 and NGC 7589, by re-examining existing HI observations and generating new rotation curves. These newly obtained rotation curves shed light on the distribution of luminous and dark matter within these galaxies.\n\nContrary to earlier investigations, the rotation curves of both Malin 1 and NGC 7589 exhibit a sharp incline in their central regions, reminiscent of high surface brightness (HSB) systems. Mass decompositions incorporating a dark matter halo reveal that baryonic matter may play a predominant role in governing the dynamics of the inner regions. Notably, a \"maximum disk\" fitting approach yields stellar mass-to-light ratios within the customary range observed in HSB galaxies. \n\nThese findings, combined with the outcomes of recent research efforts, propose that GLSB galaxies possess a dual nature: comprising an inner HSB early-type spiral galaxy and an outer, extensive LSB disk. Furthermore, our evaluation of Modified Newtonian Dynamics (MOND) indicates that the rotation curve of NGC 7589 aligns well with the model's predictions, whereas Malin 1 poses a formidable challenge to the theory's capabilities.",
    "The study investigates the multiplicity distribution, multiplicity moment, scaled variance, entropy, and reduced entropy of target evaporated fragments emitted in forward and backward hemispheres in interactions with emulsion heavy targets (AgBr) induced by various ions at different energies (12 A GeV $^{4}$He, 3.7 A GeV $^{16}$O, 60 A GeV $^{16}$O, 1.7 A GeV $^{84}$Kr, and 10.7 A GeV $^{197}$Au). Results show that the multiplicity distribution of the emitted fragments can be accurately described by a Gaussian distribution. The multiplicity moments increase with the order of the moment {\\em q}, with the second-order moment being independent of energy for all interactions. The scaled variance, indicating multiplicity fluctuations, is close to one, suggesting a weak correlation among the produced particles. Furthermore, the entropy of the emitted fragments is consistent between forward and backward hemispheres within experimental uncertainties.",
    "We thoroughly examine the time-dependent behavior of a quantum dot when subjected to off-resonant optical excitation for rapid acoustic phonon-assisted state manipulation. Our analysis reveals three distinct processes occurring during the state preparation driven by short laser pulses: initial state dressing upon pulse initiation, subsequent relaxation induced by phonons, and final state undressing at the pulse's conclusion. Through investigation of various excitation scenarios with different pulse shapes, we emphasize the crucial role of an adiabatic undressing in determining the ultimate state in short pulse protocols. Additionally, we illustrate how specific laser characteristics, such as pulse detuning and length, along with the biexciton binding energy, can be utilized to target a desired quantum dot state within exciton-biexciton systems.",
    "In the realm of quantum mechanics, the probabilistic interpretation was introduced as a subsequent addition to the Hilbert space formalism, largely compelled by empirical observations rather than inherent to the mathematical framework itself. An alternate model that inherently incorporates a clear probabilistic interpretation from its inception is found within quantum logics boasting unique conditional probabilities. This model involves projection lattices in von Neumann algebras, where the process of probability conditionalization aligns with the state transition observed in the Lueders - von Neumann measurement process. This alignment prompted the establishment of a hierarchy encompassing five levels of compatibility and measurability within the abstract context of quantum logics featuring unique conditional probabilities. These levels signify various concepts such as the absence of quantum interference or influence, the presence of joint distributions, the capacity for simultaneous measurability, and the state independence following two successive measurements irrespective of their order. A supplementary level denotes when two elements of the quantum logic are part of the same Boolean subalgebra. While these five levels may diverge in the general scenario, they ultimately converge within the common Hilbert space formalism of quantum mechanics, in von Neumann algebras, and under certain other conditions.",
    "This analysis studies how wave-vector dispersion is affected in layered magneto-optic media with elliptical birefringence and one-dimensional periodicity. The differences in local normal-mode polarization between layers cause mode coupling, affecting the wave-vector dispersion and Bloch states of the system. This coupling introduces additional terms in the dispersion relation not seen in uniform circularly birefringent media. It can remove degeneracies at specific frequencies and create a magnetization-dependent band gap. The research investigates the conditions for band gap formation, showing that a frequency split can be linked to a coupling parameter based on the polarization states of nearby layers. The study also explores the Bloch states and ways to enhance band splitting in these systems.",
    "This study extends classical empirical risk minimization to random subspaces of a given space. This approach explores computational savings while maintaining learning accuracy, even for non-smooth loss functions like the hinge loss in support vector machines. Theoretical results demonstrate that computational efficiency can be improved without sacrificing performance, supported by simple numerical experiments.",
    "Patient consent is crucial for accessing medical data. In traditional healthcare systems, consent is obtained through a form signed by the patient. In e-Health systems, paper forms are being replaced by incorporating consent into the data access controls, giving patients the ability to manage consent more effectively. However, the process of granting and revoking consent varies based on the patient's situation. We argue that capturing such detailed processes as authorization policies is challenging and error-prone. This paper introduces ACTORS, a goal-driven approach that uses Teleo-Reactive (TR) programming to manage consent by considering changes in the patient's domains and contexts when providing consent.",
    "This paper discusses the math analysis of the inverse random source problem for a specific type of diffusion equation. The source is assumed to be influenced by a certain type of random movement. The main goal is to understand how the source impacts the diffusion equation. The paper looks at both a direct problem, which studies the diffusion equation with this source, and an inverse problem, which aims to find out information about the source based on the data from the diffusion equation. The analysis uses specific math functions and concepts related to the random movement involved.",
    "Manifold learning methods are essential for reducing the dimensionality of high-dimensional data sets with low intrinsic dimensionality. Many of these methods are graph-based, where each data point is associated with a vertex and each pair with a weighted edge. Current theory indicates that the Laplacian matrix of the graph approaches the Laplace-Beltrami operator of the data manifold when the pairwise affinities are based on the Euclidean norm. This paper focuses on determining the differential operator limit for graph Laplacians constructed using $\\textit{any}$ norm. Our proof involves the interplay between the second fundamental form of the manifold and the convex geometry of the unit ball corresponding to the given norm. To showcase the advantages of non-Euclidean norms in manifold learning, we examine the task of mapping large molecules with continuous variability. In a numerical simulation, we demonstrate that a modified Laplacian eigenmaps algorithm using the Earthmover's distance outperforms the classic Euclidean Laplacian eigenmaps in terms of computational efficiency and the required sample size for recovering the intrinsic geometry.",
    "We present an efficient method using integral equations to address the heat equation $u_t (\\mathbf{x}) - \\Delta u(\\mathbf{x}) = F(\\mathbf{x},t)$ within a two-dimensional, multiply connected domain under Dirichlet boundary conditions. Rather than employing integral equations relying on the heat kernel, a time discretization approach is initially utilized. This results in a non-homogeneous modified Helmholtz equation being solved at each time increment. The solution to this equation is expressed as a combination of a volume potential and a double layer potential. The volume potential is computed with the aid of a fast multipole-accelerated solver. The imposition of boundary conditions is achieved by solving an integral equation related to the homogeneous modified Helmholtz equation. Utilizing the fast multipole method (FMM), the integral equation solver is also enhanced for efficiency. With a total of $N$ points in the discretization of both the boundary and the domain, the overall computational complexity per time step is on the order of $O(N)$ or $O(N\\log N)$.",
    "We explore an innovative scheme involving sequential state-discrimination measurements on qudits to determine the initial quantum state prepared. These qudits, belonging to a nonorthogonal set of quantum states, pose a challenge in being reliably distinguished. Through unambiguous state discrimination, we achieve error-free measurements at the cost of occasional inconclusive outcomes. Qudits offer the potential to convey higher information density per transmission compared to qubits. We analyze the case where Alice dispatches one of N qudits, each with a dimensionality of N. By examining scenarios with uniform state overlaps and mixed state overlaps across distinct qudit sets, we deepen our insights. Additionally, we investigate the resilience of our scheme against basic eavesdropping attempts and observe that leveraging qudits enhances error detection probabilities compared to qubits.",
    "The aim of this work is to enhance access control in Hyperledger Fabric blockchain by integrating various ID's, attributes, and policies with the current access control components. The existing access control system of Hyperledger Fabric is thoroughly examined before proposing a new implementation that builds upon the current system. This proposed solution offers users and developers easier methods to make access control decisions based on multiple ID's, attributes, and policies. The new implementation includes encapsulating the Fabric CA client to simplify attribute addition and streamline the process of registering and enrolling a new certificate for a newly created user. The research demonstrates the feasibility of combining multiple ID's, attributes, and policies using Hyperledger Fabric's smart contract technology. Additionally, it shows that the performance impact on real-world applications is minimal when compared to always granting access without proper access control measures in place.",
    "Introducing PyConv: A Revolutionary Approach to Computer Vision Tasks\n\nUnveiling the groundbreaking PyConv, a revolutionary technique that processes input across multiple filter scales, enhancing recognition abilities and computational efficiency without increasing costs. With a pyramid of kernels featuring diverse filter types and sizes, PyConv excels at capturing intricate scene details. Its flexibility and extensibility offer endless possibilities for diverse network architectures in various applications.\n\nPyConv promises to redefine computer vision tasks, presenting cutting-edge architectures for image classification, video action recognition, object detection, and image segmentation. In head-to-head comparisons with traditional methods, PyConv-powered networks demonstrate superior performance across the board. Noteworthy achievements include a 50-layer network surpassing the recognition prowess of a ResNet with 152 layers on ImageNet dataset, boasting significantly fewer parameters, lower computational complexity, and reduced layer count.\n\nExperience the power of PyConv and explore its transformative potential across computer vision tasks. Check out our code at: https://github.com/iduta/pyconv",
    "The presentation will cover progress in the search for solar axions using the CERN Axion Solar Telescope (CAST). The results of the initial stage of CAST phase II, which involved filling the magnet bores with 4He gas at varying pressures to explore axion masses up to 0.4 eV, will be detailed. By observing the absence of excess X-rays when the magnet was directed towards the Sun, we established an upper limit on the axion-photon coupling of g < 2.17 x 10^10 GeV$-1 at a 95% confidence level for axion masses below 0.4 eV, with the precise value determined by the pressure conditions. The ongoing investigation in the latter part of CAST phase II is focused on detecting axions with masses up to approximately 1.2 eV using 3He as a buffer gas. Anticipated sensitivities will be discussed, along with upcoming prospects and potential long-term strategies for a new helioscope experiment.",
    "Observations reveal that the Arctic sea ice cover is receding quickly, while the Antarctic sea ice cover is steadily growing. State-of-the-art climate models generally predict a modest decrease in both the Arctic and Antarctic sea ice covers. Nevertheless, a few model simulations in each hemisphere mirror the observed sea ice trends. Recent studies suggest that the models align with the observations in each hemisphere once internal climate variability is considered. Examining sea ice changes from 1979 to 2013 in simulations from the CMIP5 and CESM-LE projects, we build on previous research highlighting a strong connection between global-mean surface temperature and sea ice extent in climate models. Our analysis finds that models projecting Arctic sea ice retreat at the observed rate during 1979-2013 exhibit significantly higher levels of global warming than recorded. When estimating the sea ice retreat under observed global warming levels in each simulation, less than 1% of the time did the models project Arctic sea ice retreat matching actual observations. These findings suggest a lack of consistency between models and observations. In the Antarctic region, the simulations typically predict sea ice expansion resembling observed trends with insufficient global warming, although these results are less definitive. This study indicates that the models fail to capture the observed difference in sea ice trends between the Arctic and Antarctic, suggesting that the models might produce accurate sea ice trends in both regions for inaccurate reasons.",
    "Biometric features are increasingly important for authenticating IoT devices. This investigation aims to identify factors that hinder the development and widespread deployment of biometric models. These factors include human physiological (e.g., face, eyes, fingerprints, palm, or electrocardiogram) and behavioral features (e.g., signature, voice, gait, or keystroke). The research covers various machine learning and data mining methods used in authentication and authorization for mobile IoT devices. It also examines threat models and countermeasures employed in biometrics-based authentication for these devices. Specifically, the study evaluates current biometric authentication schemes for IoT devices and highlights challenges for future research in this area.",
    "Sure! Here is a restated version of the text with a different tone:\n\nThe topic of device fingerprinting on the web has garnered considerable interest, both within the research community and the commercial sector. While existing fingerprinting methods rely on software running on the device, these can be readily altered by the user, rendering the device fingerprint ineffective. However, in this position paper, we highlight the potential of the HTML5 standard to introduce a new set of fingerprinting features that are hardware-based. These features offer increased durability against manipulation, thus providing a more reliable device fingerprint. We introduce various plausible methods for fingerprinting devices using HTML5 web applications and share initial findings from an experiment focusing on GPU fingerprinting.",
    "Here's a more casual version of the passage you provided:\n\nWe're talking about the partition function of Chern-Simons theory with an exceptional gauge group on a three-sphere. This is like looking at the refined closed topological string, where we use a special formula that connects a single K\\\"ahler parameter, the string coupling constant, and a refinement parameter. The different values of the refinement parameter for the groups $G_2, F_4, E_6, E_7, E_8$ are $\\frac{5}{3}, \\frac{5}{2}, 3, 4, 6$, respectively.\n\nSome specific non-zero BPS invariants are mentioned, like $N^2_{0,\\frac{1}{2}}=1$ and $N^{11}_{0,1}=1$. And then there are other terms in the partition function that come from the refined constant maps in string theory.\n\nThe derivation is based on a universal form of the Chern-Simons partition function on a three-sphere, but it's focused on a special line called $Exc$. This line has certain parameters that relate to exceptional groups. There's also another line called $F$ which includes groups like $SU(4), SO(10),$ and $E_6$, and it follows a similar pattern of results.\n\nIn both cases, the refinement parameter $b$ (related to Nekrasov's parameters) is expressed in terms of universal parameters on the line, given by $b=-\\beta/\\alpha$.",
    "The centerpoint theorem is a renowned and commonly utilized result in discrete geometry. It asserts that, for a point set $P$ consisting of $n$ points in $\\mathbb{R}^d$, there exists a point $c$ (which may not belong to $P) such that each halfspace containing $c$ includes at least $\\frac{n}{d+1}$ points from $P. This specific point $c$ is known as a centerpoint, serving as an extension of a median to dimensions beyond one. Essentially, a centerpoint serves as a representative for the point set $P. However, considering the possibility of multiple representatives, especially in one-dimensional datasets where certain quantiles are often preferred over the median, we explore an adaptation of quantiles to higher dimensions. This extension involves identifying a set $Q$ of a few points where each halfspace containing a point from $Q$ encompasses a substantial portion of points from $P, and any halfspace containing more points from $Q$ includes an even larger fraction of points from $P. This concept bears similarities to established notions like weak $\\varepsilon$-nets and weak $\\varepsilon$-approximations, positioned as an intermediary between the former and the latter in terms of strength.",
    "We have developed a new software tool called \\textsc{PsrPopPy} to simulate populations of pulsars. This software is an enhancement of the existing \\textsc{Psrpop} package, now rewritten in Python to leverage the language's object-oriented capabilities and enhance code modularity. While it includes pre-written scripts for standard simulations, users can easily create custom scripts due to its flexible design.\n\nThe improved modular structure allows for straightforward integration of experimental features, such as new models for period or luminosity distributions, compared to previous versions. We also explore additional modeling capabilities of the software. In our demonstration, we analyze pulsar spectral indices across different observing frequencies and establish that they follow a normal distribution with a mean of $-1.4$ and a standard deviation of $1.0. Additionally, we use the software to study pulsar spin evolution and determine the best-fit relationship between a pulsar's luminosity and spin parameters.\n\nOur analysis, inspired by previous studies, reveals that the radio luminosity ($L$) of pulsars follows a power-law dependence on period ($P$) and period derivative ($\\dot{P}$), with an optimal fit given by $L \\propto P^{-1.39 \\pm 0.09} \\dot{P}^{0.48 \\pm 0.04}$a pattern akin to that observed in $\\gamma$-ray pulsars. By applying this relationship, we create a model population to investigate the age-luminosity trend in the entire pulsar population, particularly in anticipation of data from future surveys like the Square Kilometer Array.",
    "We examine how a group of spinning particles interacts strongly with a single-mode resonator that is stimulated by external pulses. When the average frequency of the spinning particles aligns with the resonant frequency of the cavity, we observe damped Rabi oscillations occurring between the group of spinning particles and the cavity. We provide a detailed description of this interaction, taking into account the dephasing impact of the varied spinning frequencies. Our research demonstrates that having accurate information about this frequency variation is essential for both understanding and predicting the time-dependent behavior of the spinning particles and the cavity. By utilizing special resonance conditions when applying pulses, we illustrate that the coherent oscillations between the spinning particles and the cavity can be significantly amplified. We verify the effectiveness of our theoretical model through an experiment involving a collection of negatively charged nitrogen-vacancy (NV) centers in diamond interacting with a superconducting coplanar single-mode waveguide resonator.",
    "We are examining the properties of a quantum Ising spin-1/2 chain in a transverse field, focusing on the fundamental Riemannian metric of its ground state and the cyclic quantum distance. By utilizing a specific transformation technique, we can simplify the model and express it in terms of a fermionic Hamiltonian. The Riemannian metric of the ground state is obtained precisely on a circular parameter space, created by adjusting the spin Hamiltonian through a twist operator transformation. We investigate the behavior of the cyclic quantum distance and the second derivative of the ground-state energy across various regions of inhomogeneous exchange coupling parameters. Notably, we discover that the quantum ferromagnetic phase in a uniform Ising chain can be distinguished by a consistent cyclic quantum distance with an unchanging ground-state Riemannian metric, which diminishes rapidly to zero in the paramagnetic phase.",
    "Rotation measure synthesis estimates Faraday dispersion using a Fourier transform as the primary method to study cosmic magnetic fields. This mathematical approach is analogous to one-dimensional interferometric intensity measurements but in a different Fourier space. By applying concepts from two-dimensional intensity interferometry, we can analyze Faraday dispersion, including modeling the impact of channel averaging during reconstruction. Simulation demonstrates successful recovery of signals with large rotation measure values previously undetectable, crucial for low-frequency and wide-band polarimetry. Additionally, integrating mosaicking in Faraday depth with channel averaging enables wide-band rotation measure synthesis across data from multiple telescopes. This advancement is vital for enhancing the quality and quantity of polarimetric science, particularly in extreme environments like those around pulsars and Fast Radio Bursts (FRBs), enabling these sources to serve as precise probes of cosmological fields.",
    "The study investigates the unique properties of charged particle production in high-energy hadron-nucleus collisions by applying different statistical models. Predictions from the Negative Binomial, shifted Gompertz, Weibull, and Krasznovszky-Wagner distributions are compared to evaluate the success of each model. These distributions, based on various functional forms, stem from either empirical parameterizations or underlying dynamic models. Some of these models have been applied to analyze data from the LHC in proton-proton and nucleus-nucleus collisions. The analysis incorporates a range of physical and derived observables.",
    "In 1975, John Tukey introduced a revolutionary concept of the multivariate median, identifying the most central point within a data cloud in multiple dimensions. Building on Tukey's idea, David Donoho and Miriam Gasko later explored the notion of data depth by considering hyperplanes through arbitrary points and determining their depth based on separation from the surrounding data points. These pioneering concepts have not only borne fruit but have also given rise to a rich statistical methodology centered around data depth and nonparametric depth statistics. Various notions of data depth have been developed, each with unique properties, making them suitable for diverse applications. \n\nBy exploring the upper level sets of a depth statistic, researchers have uncovered depth-trimmed regions that offer valuable insights into a distribution's location, scale, and shape, akin to identifying a median. The concept of data depth has transcended empirical data clouds, extending its reach to general probability distributions in multi-dimensional spaces, enabling the establishment of laws of large numbers and consistency results. Moreover, this concept has also been applied to data in functional spaces, further expanding its utility and applications.",
    "Strain-engineering in SiGe nanostructures is essential for developing optoelectronic devices at the nanoscale. In this study, a novel approach is explored where SiGe structures are laterally confined by the Si substrate to achieve high tensile strain without the need for external stressors, thus enhancing scalability. Spectro-microscopy techniques, finite element method simulations, and ab initio calculations were employed to analyze the strain state of laterally confined Ge-rich SiGe nano-stripes. The strain details were determined using tip-enhanced Raman spectroscopy, achieving an unprecedented lateral resolution of approximately 30 nm. The nano-stripes displayed a significant tensile hydrostatic strain component, with maximum strain at the center of the top free surface that decreases towards the edges. The lattice deformation exceeded the typical values for thermally relaxed Ge/Si(001) layers, with this strain enhancement attributed to frustrated relaxation in the out-of-plane direction resulting from lateral confinement by the substrate side walls and plastic relaxation of the misfit strain at the SiGe/Si interface. The impact of this tensile lattice deformation on the stripe surface was investigated through work function mapping using X-ray photoelectron emission microscopy, revealing a positive work function shift compared to a bulk SiGe alloy. This shift was quantitatively supported by electronic structure calculations of tensile strained configurations. These findings could significantly influence the design of nanoscale optoelectronic devices.",
    "Rephrased text: Sharing electronic medical records or models during an infectious disease pandemic across regions is crucial. Utilizing one region's data/model in another region may encounter distribution shift problems that challenge traditional machine learning methods' assumptions. Transfer learning can address this issue. In order to investigate the effectiveness of deep transfer learning algorithms, we tested two data-based algorithms (domain adversarial neural networks and maximum classifier discrepancy) and model-based transfer learning algorithms for infectious disease detection tasks. We also examined synthetic scenarios with known data distribution differences between regions. Our experiments revealed that transfer learning could prove beneficial in infectious disease classification scenarios when (1) the source and target are similar and the target training data is limited and (2) the target data lacks labels. Model-based transfer learning performed well in the first scenario, with performance comparable to data-based transfer learning models. However, further research is required to study domain shift in real-world research data and address potential performance declines.",
    "Research in optics and photonics has been highly focused on bound states in the continuum (BIC) in recent years. Exploring the effects of quasi-BICs in basic structures, such as dielectric cylinders, is particularly intriguing due to their pronounced nature. Numerous studies have investigated quasi-BICs in single cylinders and cylinder-based structures. This study examines the characteristics of quasi-BICs as a homogeneous dielectric cylinder in an air environment transitions into a ring with narrow walls by gradually increasing the diameter of the inner air cylinder. The findings illustrate the transition of quasi-BICs from the strong-coupling to the weak-coupling regime, marked by the shift from branches' avoided crossing to their intersection, with the quasi-BIC now being preserved on only one branch. In the strong-coupling and quasi-BIC regime, three waves interact in the far-field zone: two waves corresponding to resonant modes and one wave scattered by the structure. The study also delves into the concept of Fano resonance, particularly in the context of only two-wave interference under weak coupling conditions.",
    "The combination of temperature stratified turbulence and inertia of small particles results in turbulent thermal diffusion, leading to a non-diffusive turbulent flux of particles aligned with the turbulent heat flux direction. This flux is dependent on the mean particle number density and the effective velocity of inertial particles. While the theory was previously limited to small temperature gradients and Stokes numbers, a more generalized theory has now been developed for arbitrary conditions. Laboratory experiments in various turbulent flows have confirmed the theory, showing that the effective velocity of inertial particles relative to the vertical turbulent velocity decreases with increasing Reynolds numbers. Additionally, the effective velocity and coefficient of turbulent thermal diffusion increase with Stokes numbers, peaking at small values and declining for larger ones, while the coefficient decreases with the mean temperature gradient. Overall, the developed theory aligns well with experimental findings.",
    "One common model used to explain the visibility of pulsar radio emission assumes that the emission is focused within a narrow cone along the tangent to a dipolar magnetic field line. While the widely accepted rotating vector model (RVM) provides a practical approximation by keeping the observer's line of sight fixed, it deviates from the tangent model, which takes into account the changing visible point as the pulsar rotates (Gangadhara 2004). In the tangent model, the visible point moves along a trajectory on a sphere of radius r as a function of the pulsar's rotational phase . Recent research has suggested the potential observability of this motion using interstellar holography (Pen et al. 2014).\n\nBy comparing the RVM with the tangent model, we find that the RVM tends to underestimate the range of  over which emission is detectable, particularly for pulsars emitting across a broad  range. The geometry strongly indicates that pulsar radio emission likely occurs at heights exceeding ten percent of the light-cylinder distance, where the neglect of retardation effects could introduce significant errors.",
    "This paper introduces a novel method for zero-shot learning called Global Semantic Consistency Network (GSC-Net), which leverages semantic information from both seen and unseen classes to support effective classification in cases where training samples do not cover all target classes. By using a soft label embedding loss and a parametric novelty detection mechanism, the framework achieves state-of-the-art performance on zero-shot learning and generalized zero-shot learning tasks across three visual attribute datasets, demonstrating its effectiveness and advantages.",
    "Category theory challenges our traditional notions of mathematical structuralism by emphasizing covariant transformations over invariant forms. Contrary to popular belief, the foundations of mathematics through category theory demand a new philosophy of mathematics. In this paper, I present a compelling non-structuralist interpretation of categorical mathematics and reveal its profound implications for the history of mathematics and mathematics education.",
    "In a dynamic system of an exciton-polariton condensate with incoherent pumping, we demonstrate that utilizing a ring-shaped pump enables the creation of stable vortex memory elements with topological charges of $m = 1$ or $m = -1$. By employing basic potential guides, we have the flexibility to either replicate the existing charge or transfer it to a different ring pump location. This control over binary information introduces a novel approach to processing, leveraging vortices as securely protected memory units.",
    "During the three-year assessment phase of the LOFT mission, a candidate for the M3 launch opportunity of the ESA Cosmic Vision program, we conducted studies to estimate and measure the radiation damage of the silicon drift detectors (SDDs) used in the satellite's instrumentation. Specifically, we exposed the detectors to proton radiation (with energies of 0.8 and 11 MeV) to observe how the increased leakage current and changes in charge collection efficiency were affected by displacement damage. Additionally, we tested the detectors by subjecting them to high-speed dust grains to analyze the impact of debris collisions. In this paper, we will share our findings from these experiments and explore their implications for the LOFT mission.",
    "In this study, we investigate how low-level multimodal features can be leveraged to determine similarities between movies, within the realm of content-based movie recommendations. We showcase the development of multimodal representation models for movies, utilizing textual data from subtitles, as well as information from the audio and visual components. Specifically, we highlight our exploration of topic modeling in movies based on subtitles to identify distinguishing topics. In the visual domain, we concentrate on extracting meaningful features that capture camera movements, colors, and facial expressions. For the audio domain, we employ basic classification aggregates using pretrained models. By combining these three modalities with static metadata (e.g., directors, actors), we demonstrate that enriching content-based movie similarity with low-level multimodal data can significantly enhance recommendation systems. To validate our proposed content representation approach, we curated a dataset comprising 160 well-known movies. Our findings reveal that incorporating low-level information from text, audio, and visual sources substantially improves the performance of content-based recommendation systems, achieving over a 50% increase in accuracy compared to metadata-centric approaches. This methodology marks a pioneering endeavor incorporating a broad spectrum of features across multiple modalities to amplify content similarity estimation effectiveness over traditional metadata-centric frameworks.",
    "In this research, we investigate the emission of radiation from a Reissner-Nordstrom black hole possessing an electric charge within the context of quantum gravity. Employing a canonical quantization approach for a symmetric spherical geometry and making reasonable physical assumptions, we successfully address the Wheeler-De Witt equation in various regions encompassing the outer apparent horizon to spatial infinity as well as from the spacetime singularity to the inner apparent horizon. Our analysis reveals that the rate of mass loss from a evaporating black hole due to thermal radiation aligns with semiclassical predictions when certain integration constants are selected based on physical considerations. Additionally, our investigation extends to the zone between the inner Cauchy horizon and the outer apparent horizon, where we demonstrate that the mass loss rate follows a similar pattern for an evaporating black hole. This study represents a natural extension from the Schwarzschild case to the scenario of a charged Reissner-Nordstrom black hole.",
    "Introducing Multi-Agent A* (MAA*), the first complete and optimal heuristic search algorithm designed for solving decentralized partially-observable Markov decision problems (DEC-POMDPs) with a finite horizon. This algorithm is perfect for figuring out optimal plans for a team of agents working together in a stochastic environment, like coordinating multiple robots, controlling network traffic, or managing distributed resource allocation. Tackling these kinds of problems effectively can be quite a challenge in the realm of planning amidst uncertainty. Our approach blends classic heuristic search techniques with decentralized control theory. Through experiments, we've observed that MAA* brings significant benefits to the table. We also unveil an anytime version of MAA* and touch on potential extensions, like a method to handle infinity horizon problems.",
    "Here is the restyled text:\n\nIn this study, we utilize machine learning techniques to classify objects in SDSS DR6 based on morphological features. These objects are sorted into three classes: early types, spirals, and point sources/artifacts, as identified by Galaxy Zoo. By training an artificial neural network on a subset of objects classified by humans, we investigate the network's ability to replicate human classifications on the remaining sample. Our analysis reveals that the success of the neural network in classifying objects accurately relies heavily on the selection of input parameters for the machine-learning algorithm.\n\nWe find that the inclusion of color and profile-fitting parameters aids in distinguishing between the three classes. However, incorporating adaptive shape parameters, concentration, and texture into the analysis significantly enhances the classification results. Notably, parameters such as adaptive moments, concentration, and texture alone prove insufficient for differentiating early type galaxies from point sources/artifacts. By utilizing a comprehensive set of twelve parameters, the neural network achieves classification accuracies exceeding 90% across all morphological classes.\n\nFurthermore, our study demonstrates that incomplete data in terms of magnitude does not negatively impact the classification results, given the specific choice of input parameters for the neural network. Our findings suggest the promising potential of machine learning algorithms for morphological classification in upcoming wide-field imaging surveys. The Galaxy Zoo catalog emerges as a valuable resource for training such algorithms effectively.",
    "The Lambek calculus, a renowned logical formalism for capturing natural language syntax, has been enhanced to address intricate linguistic nuances beyond the context-free realm. In a groundbreaking extension by Morrill and Valentin (2015), new exponential and bracket modalities were introduced, triggering a fascinating interplay between a non-standard contraction rule for the exponential and the bracket structure. While the standard contraction rule is inapplicable in this framework, our research unveils the undecidability of derivability in their calculus. Furthermore, we delve into the decidable fragments proposed by Morrill and Valentin, demonstrating their classification within the NP class.",
    "The shift from regarding the transition in 4D Euclidean Dynamical Triangulation between phases as second order was overturned in 1996 when first order behavior was observed in large systems. However, concerns arise about the impact of the numerical methods utilized. Both studies added an artificial harmonic potential to manage volume fluctuations, with one study also introducing an error by taking measurements after a set number of accepted moves instead of attempted moves. Critical slowing down, possibly overlooked, posed a challenge in the simulations. In this study, we address these shortcomings by enabling volume fluctuations within a specific range, monitoring measurements after a set number of attempted moves, and mitigating critical slowing down through an enhanced parallel tempering algorithm. Applying these refined techniques on systems up to 64k 4-simplices confirms the first-order phase transition. Moreover, we introduce a local criterion to determine the state of parts of a triangulation and establish a new relationship between EDT and the balls in boxes model. This correspondence leads to a modified partition function incorporating an additional, third coupling. Finally, we suggest a category of modified path-integral measures that could eliminate the metastability of the Markov chain and transform the phase transition into second order.",
    "We identify the virtually nilpotent finitely generated groups, also known as groups of polynomial growth according to Gromov's theorem, in which the Domino Problem can be solved: these groups include virtually free groups, which are finite groups, as well as groups that contain the group of integers $\\Z$ as a subgroup with finite index.",
    "Discover how gamma rays resulting from the annihilation of dark matter particles in the Galactic halo present an exciting opportunity for indirectly detecting dark matter. We reveal the presence of distinct spectral characteristics near the dark matter particles' mass, a phenomenon commonly predicted by various models. These features can vastly enhance the sensitivity of gamma-ray telescopes to dark matter signals. By establishing limits on these spectral traits, including the conventional line signals, we illustrate their effectiveness in discriminating the properties of dark matter over the broad spectral features observed at lower energies  offering a promising avenue for unraveling the mysteries of dark matter.",
    "Achieving carbon neutrality necessitates a focused research agenda to tackle technical and economic obstacles on the path to achieving 100% renewable electricity generation. The rising share of variable renewable energy (VRE) sources complicates the equilibrium in VRE-dominated power systems, demanding attention to the operational aspects and impacts of VRE inverters. In this analysis, we delve into the repercussions of the shift towards carbon neutrality, outlining the research challenges related to system planning, operation, and stability. We also underscore the importance of integrating energy storage, engaging demand-side participation, implementing distributed control and estimation, and coupling energy sectors. By pinpointing gaps in existing literature and sharing our latest research, we aim to enhance grid operations and estimations. Additionally, we present numerical findings from comparative case studies on the operational stability and economic aspects of high VRE source power systems, which aid stakeholders in developing concrete action plans and informed decisions.",
    "Convolutional neural networks (CNNs) are popular in computer vision due to their ability to handle large labeled datasets efficiently. The novel network architecture, FreshNets, reduces memory and storage needs by exploiting redundancy in convolutional and fully-connected layers. By converting filter weights to the frequency domain and using a hash function to group parameters, FreshNets achieves significant compression benefits compared to other methods across various datasets.",
    "We carried out a dynamic project using Japanese cartoon (manga) techniques to enhance Requirements Development (RD) processes through Project-Based Learning (PBL)! Leveraging the established creativity of manga  from character setting to storytelling  we discovered that these methods are invaluable for RD. With our innovative manga-inspired approach, students not only defined high-level project goals swiftly, but also managed to create exceptional and innovative system concepts. Let the energy of manga lead the way to project success!",
    "The conventional Hawking formula provides insight into the evaporation process of black holes, foreseeing their eventual complete disappearance. In our inquiry, we consider the nuances of quantum gravity and delve into the phenomenon of fermions tunneling away from a 5-dimensional rotating black string. The temperature profile is not solely contingent upon the black string itself, but also intricately intertwined with factors such as the quantum property of the emitted fermion and the repercussion of the additional spatial dimension. The introduction of quantum corrections plays a pivotal role in tempering the rate of temperature escalation, thereby paving the way for the emergence of remnants in the evaporation process.",
    "We present a novel approach to deriving alternative vector representations of words, which are based on topological characteristics from nearby neighbors within pre-trained contextual word embeddings. We then investigate the impact of integrating these alternative vector representations as input features in different deep learning models for natural language processing tasks, such as entity recognition, textual entailment, and paraphrase identification. Interestingly, we observe that simply utilizing information from neighboring words is often sufficient to capture most of the advantages gained from using pre-existing word embeddings. Additionally, the alternative vector representations demonstrate an improved ability to manage diverse data types compared to traditional representations, albeit with some loss of specificity. Moreover, enhancing contextual embeddings with these alternative vector representations can yield further enhancements in model performance in specific scenarios. Furthermore, taking into consideration the variability in initializations of word embeddings, incorporating information from neighboring words in multiple samples of traditional embeddings can lead to enhanced performance in subsequent tasks. Lastly, we uncover intriguing properties of these alternative vector spaces that warrant further investigation, such as higher density and varying semantic interpretations of cosine similarity.",
    "RIS-aided millimeter wave wireless systems not only excel in overcoming blockages and extending coverage, but they also possess the unique capability of enhancing localization accuracy through communication techniques. This study explores the potential of RIS in providing precise positioning information by leveraging sparse reconstruction algorithms to achieve detailed channel estimation, which is then translated into spatial data. However, the challenge lies in the complexity of sparse recovery due to the vast number of RIS elements and communication arrays. To tackle this issue, we introduce a novel multidimensional orthogonal matching pursuit method for compressive channel estimation in RIS-assisted millimeter wave setups. By computing projections on various independent dictionaries rather than a single extensive one, this approach delivers highly accurate channel estimation with reduced complexity. Furthermore, when combined with a localization strategy independent of the exact time of arrival of the Line of Sight (LoS) path, the system showcases remarkable improvements in localization precision. Realistic 3D indoor scenario results demonstrate that RIS-empowered wireless systems can achieve substantial enhancements in localization accuracy.",
    "Detection and measurement of information leaks via timing side channels play a crucial role in ensuring confidentiality. While static analysis is commonly used to detect these side channels, it is computationally demanding for real-world applications and typically provides only binary results. In some cases, applications may require intentional disclosure of information, necessitating quantification techniques to assess the associated security risks. Given the limitations of static analysis in handling these challenges, we introduce a novel dynamic analysis approach. This method involves two main tasks: first, building a neural network timing model of the program, and second, using this model to quantify information leaks. Our experimental results demonstrate the feasibility of both tasks in practice, highlighting the effectiveness of our approach compared to existing side channel detection and quantification methods. Our key technical contributions include a specialized neural network architecture for side channel discovery and an MILP-based algorithm for estimating side channel strength. Through evaluations on micro-benchmarks and real-world applications, we show that neural networks can accurately capture the timing behavior of programs with numerous methods, allowing for efficient detection and quantification of timing side channel leaks.",
    "The region within the asteroid belt situated between 2.1 and 2.5 astronomical units is of particular significance due to its dynamics, serving as the primary source of chondritic meteorites and near-Earth asteroids. Bounded by an eccentricity-type secular resonance and the 1:3 mean motion resonance with Jupiter, escape from this inner belt necessitates transport to these resonances unless asteroid perihelia allow for scattering by Mars. Yarkovsky forces prove generally ineffective in altering the eccentricity and/or inclination of asteroids with a diameter exceeding approximately 30 kilometers, meaning larger asteroids with pericenters distant from Mars can only depart from the inner belt through substantial changes in eccentricities. This study investigates the chaotic diffusion of orbits near the 1:2 mean motion resonance with Mars in a systematic manner. The research reveals that while chaotic orbital evolution, both in resonant and non-resonant orbits, amplifies the dispersion of inclinations and eccentricities, it has minimal impact on their average values. Moreover, the study illustrates that although dispersive growth is most prominent in resonant orbits, at high eccentricities, the resonance serves to reduce asteroid scattering by Mars, thereby prolonging an asteroid's lifetime within the belt compared to that of a non-resonant orbit. Analysis indicates that irrespective of size, asteroids in resonant and non-resonant orbits necessitate changes in eccentricity beyond the influence of gravitational forces to align with observational data. The study further delves into the protective role of resonant trapping in shielding asteroids from encounters with Mars.",
    "Nonstandard neutrino interactions (NSI) are like hidden keys that can significantly impact the precision of next-generation neutrino oscillation experiments, adding an intriguing twist to unravel the mysteries of the universe. But fear not, for there are other experiments coming to the rescue! By delving into the realm of electrons at cutting-edge $e^+e^-$ collider experiments such as Belle II, STCF, and CEPC, we are on a quest to crack the NSI code. Excitingly, Belle II and STCF are set to battle it out, vying to set new boundaries and shed light on electron-type NSI parameters, rewriting the rules of the current global analysis game. Brace yourself for CEPC's grand entrance, as it swoops in to enforce strict limits on the NSI parameter space, promising to unveil the truth hidden within the complexities. By seamlessly blending data from various running modes, we can untangle the intricate dance between left-handed and right-handed NSI parameters, finally pinning down their possible ranges to be as minute as 0.002 at CEPC, revealing the secrets that lie within.",
    "The Deep Underground Neutrino Experiment (DUNE) uses cutting-edge technology to study neutrinos and search for proton decay. Its far detector consists of four 10-kton Liquid Argon (LAr) Time Projection Chambers, including both single and dual-phase technologies. Large prototypes have been tested at CERN since 2018 to optimize the chamber designs. A 4-tonne dual-phase demonstrator tested in 2017 showed promising performance with regards to charge and light collection. The light detection system in the chamber is crucial for triggering the charge acquisition system and gathering additional information from scintillation light produced during particle interaction. Using cryogenic photo-multipliers with various configurations and methods, the demonstrator collected scintillation light data under different conditions, resulting in improved understanding of LAr properties.",
    "Major chip manufacturers have introduced Multithreaded processors for running various workloads efficiently. These processors focus on utilizing available memory-level parallelism (MLP). This paper proposes an MLP-aware operating system (OS) scheduling algorithm for Multithreaded Multi-core processors. The algorithm aims to improve overall performance by balancing the MLP available in each thread with the system's MLP resources to create an optimized thread schedule for the next operation cycle. A qualitative comparison with other hardware and software techniques is presented, suggesting that further quantitative evaluation and scheduling optimization refinements could enhance this work.",
    "We address the blind calibration of a compressed sensing system with unknown gains on measurements using a convex optimization approach. The method is effective even with highly uncalibrated measures when provided with a sufficient number of sparse calibrating signals. Sharp phase transitions dictate the success/failure of the approach.",
    "We are studying multi-source morphological reinflection, which is a more advanced version than the usual single-source method. In this task, we are given a target tag and various pairs of source forms and tags for a base word. Having more than one source form is helpful because each form offers different information, such as different word stems. To enhance our solution, we introduce a new extension to the encoder-decoder neural network model by using multiple encoders. Our research shows that this new model performs better than the traditional single-source models. We have also shared our dataset for multi-source morphological reinflection to support future studies.",
    "There is a growing demand for extracting complex knowledge from semantically annotated data streams quickly, especially in the realms of the Web and the Internet of Things (IoT). The task often involves intricate reasoning which can be computationally challenging when dealing with extensive streams. Introducing Laser, a novel reasoner that facilitates a practical segment of logic LARS, an extension of Answer Set Programming (ASP) for streams. Laser utilizes an innovative evaluation method that annotates formulae to prevent redundant calculations at different time stages. This approach, coupled with a careful implementation of LARS operators, results in significantly improved processing times compared to existing systems like C-SPARQL and CQELS, or an ASP solver Clingo-based LARS implementation. This advancement enables the application of sophisticated logic-based reasoning to handle large streams effectively, thus broadening the scope of potential stream reasoning applications.",
    "The second law of thermodynamics establishes the basic constraints on the transfer of energy and information between physical systems. In this study, we expand a thermodynamic framework that characterizes this exchange of energy and information beyond two-part systems to multiple-part systems. We pinpoint a natural thermodynamic measure that accounts for the information transferred within these systems. Subsequently, we introduce and examine an enhanced edition. Our findings are demonstrated using a scenario involving two competing Maxwell demons.",
    "The small natural connection between the spin of electrons in graphene, known as intrinsic spin-orbit coupling (SOC), can be increased by the influence of other materials in layered structures made of graphene and transition metal dichalcogenides (TMDCs). The type of metal compound used in the TMDC layer is essential in determining the strength and characteristics of the resulting SOC effect on the graphene layer. In this study, we observe how the SOC effect changes as we intentionally introduce defects to the TMDC layer. We use computer simulations based on density functional theory to examine layered structures created from a mix of graphene, tungsten, molybdenum, and selenium with different compositions and defect patterns. By comparing our results to different theoretical models, we are able to understand the impact of mixing metals on a local and overall scale. Our research indicates that, even though individual defects can have a significant impact on certain properties, the collective behavior of electrons and spins can be predicted by a simple model based on the mix of metals in the TMDC layer. Additionally, we show that the unique electronic state of these mixed systems can be controlled by adjusting the amount of each metal in the TMDC layer.",
    "Atomic masses are a vital component in numerous nuclear astrophysics calculations! The absence of experimental values for unique nuclides has sparked a global surge in the development of cutting-edge mass measurement technologies. Enter the Time-of-Flight (TOF) method - a dynamic approach that complements the precision of Penning trap measurements. The NSCL facility is at the forefront, equipped with the perfect setup for TOF mass measurements of incredibly rare nuclei. Using our newly implemented TOF-Brho technique, we've conducted exhilarating mass measurements of neutron-rich nuclides in the Fe region. These findings are crucial for r-process estimations and understanding the phenomena within the crust of accreting neutron stars!",
    "Engage with the text: While super-massive (AGN) and stellar mass (XRBs) black holes share many characteristics, the distinctive broad emission lines (BELs) are a trademark of AGN alone. Thanks to the examination of SDSS databases, it appears that AGN with a mass (M_BH) \\lesssim 10^5 M_sun are notably missing. This study delves into the mystery of whether these lower mass black holes truly do not exist or if they are simply overlooked due to inefficient BEL production. By leveraging the ionizing spectral energy distribution across a vast black hole mass spectrum, from 10 - 10^9 M_sun encompassing XRBs to AGN, we have computed the equivalent widths (EWs) of key ultraviolet and optical lines such as Ly\\alpha 1216 \\AA, H\\beta 4861 \\AA, CIV 1549 \\AA, and MgII 2798 \\AA. Using the LOC (locally optimally emitting cloud) model to define the broad emission line region (BELR), our findings reveal that while the hardness of the spectral energy distribution doesn't impact BEL EWs with diminishing mass, the size of the BELR - indicated by line widths and controlled by the black hole's mass - dictates the emission line production. Interestingly, there appears to be an EW peak for typical AGN black holes around ~ 10^8 M_sun, below which these lines noticeably dim, with a sharp decline below ~ 10^6 M_sun. This observation could potentially explain why low mass AGN are absent in SDSS data.",
    "The accuracy of synchronization algorithms, which are established on the pulse-coupled oscillator theory, has been assessed in FPGA-based radios for the inaugural time. The measurements indicate that these algorithms are capable of achieving precision within the low microsecond range when integrated into the physical layer. Additionally, we introduce an algorithm extension that considers hardware phase rate discrepancies, demonstrating that a refined precision of less than one microsecond is attainable with this extension under the current configuration. Consequently, this enhanced algorithm can be employed in ad hoc wireless systems for comprehensive synchronization of transmission slots or sleep cycles in scenarios where centralized synchronization is unattainable.",
    "The paper examines dataset complexity in Human Trajectory Prediction (HTP) by defining indicators around three concepts: Trajectory predictability, Trajectory regularity, and Context complexity. It compares common datasets used in HTP and discusses their implications on benchmarking HTP algorithms. Source code is available on Github.",
    "The goal of this document is to demonstrate how a set of traditional linear random systems can be practically executed utilizing quantum light-related elements. Quantum light systems generally exhibit significantly greater frequency capacity compared to electronic gadgets, resulting in quicker reaction and computation periods, thus possessing the capability to offer superior efficiency compared to traditional systems. A process is outlined for creating the quantum optical implementation. The document also outlines the employment of the quantum optical implementation in a measurement feedback circuit. Various instances are presented to showcase the implementation of the primary findings.",
    "Systems biology employs extensive networks of biochemical reactions to simulate the operation of biological cells across various scales, from the molecular level to the cellular level. The behavior of dissipative reaction networks featuring numerous distinct time scales can be elucidated as a series of consecutive equilibrations involving different subsets of system variables. Polynomial systems exhibiting separation equilibrium occur when at least two monomials, displaying opposite signs and comparable magnitudes, exert significant influence over other terms. These equilibrations and the consequent truncated dynamics, achieved by removing dominated elements, lend themselves to a coherent representation within the framework of tropical analysis, facilitating model simplification.",
    "We conducted spectral analysis on Suzaku data of the galactic disk and outflow areas in the starburst galaxy M82. In the central disk regions, a minimum of three temperature components is necessary for thermal modeling. The Ly$\\beta$ line fluxes of O VIII and Ne X are higher than anticipated from a plasma in collisional ionization equilibrium. The ratios of Ly$\\beta$/Ly$\\alpha$ lines for O VIII and Ne X also surpass those expected from collisional ionization equilibrium, likely due to charge exchange. In the outflow wind region, two-temperature thermal models accurately replicate the spectra, from which we determined the metal abundances of O, Ne, Mg, and Fe. By comparing to the solar values established by Lodders (2003), the ratios of O/Fe, Ne/Fe, and Mg/Fe are approximately 2, 3, and 2, respectively. As there is no sign of charge exchange in the outflow region, the metal abundance values should be more dependable than those in the central region. This particular abundance distribution suggests that starburst activity leads to enrichment of the outflow through SN II metal expulsion into the intergalactic space.",
    "Dust particles are traditionally believed to originate in the gusts of immense asymptotic giant branch (AGB) stars. Nevertheless, the current evidence increasingly suggests that dust formation also occurs within supernovae (SNe). To ascertain which of these stellar sources is more significant in producing dust, it is crucial to determine the proportion of freshly created dust in SN debris that can withstand the impact of the counter shock and be dispersed into the interstellar medium. Our recently developed code (GRASH\\_Rev) meticulously traces the progression of nascent dust from the moment of a supernova explosion until the fusion of the advancing shockwave with the circumstellar interstellar medium. Our investigation focuses on four well-documented supernovae within the Milky Way and the Large Magellanic Cloud: SN1987A, CasA, the Crab Nebula, and N49. Across all the simulated scenarios, we observe a close agreement with actual observations and predict that approximately 1 to 8$\\%$ of the initial dust mass will endure, equating to a SN dust generation rate of $(3.9 \\pm 3.7) \\times 10^{-4}$ M$_{\\odot}$yr$^{-1}$ in the Milky Way. This figure surpasses the dust production rate by AGB stars by an order of magnitude; however, it falls short of counteracting the destruction of dust by supernovae, necessitating further investigation into dust accretion in the gaseous phase.",
    "This study examines various hatching process strategies for additive manufacturing using an electron beam through numerical simulations. It introduces the physical model and the three-dimensional thermal free-surface lattice Boltzmann method used in the simulation software. The software's validity has been demonstrated through experiments up to 1.2 kW beam power, where a cuboid was hatched using a basic process strategy resulting in outcomes categorized as 'porous,' 'good,' and 'uneven' based on density and surface smoothness. The paper investigates the limitations of this basic strategy at higher beam powers and scan velocities to harness the full potential of high-power electron beam guns up to 10 kW. It then proposes modified process strategies that overcome these constraints to achieve rapid part production while ensuring full density and smooth top surfaces. These strategies aim to reduce build time and costs, optimize beam power utilization, and unlock the capabilities of high-power electron beam guns.",
    "Bayesian optimization (BO) belongs to a class of global optimization algorithms designed to minimize an expensive objective function with minimal function evaluations. While BO budgets are typically specified in iterations, this method of convergence measurement assumes uniform evaluation costs, which may not be the case across different regions of the search space. For instance, the cost of neural network training rises quadratically with layer size, a common hyperparameter. To address this issue, Cost-aware BO utilizes alternative cost metrics like time, energy, or money for assessing convergence, which vanilla BO methods cannot accommodate. Introducing Cost Apportioned BO (CArBO), our method aims to minimize the objective function at minimum cost. CArBO employs a cost-effective initial design and a cost-cooled optimization phase that adjusts a learned cost model as iterations progress. Through experiments on 20 black-box function optimization problems, we demonstrate that CArBO outperforms competing methods in finding superior hyperparameter configurations under the same cost constraints.",
    "This project introduces a unique marsupial robotic system made up of both a legged and an aerial robot. They work together to create maps and plan exploration routes efficiently by combining the strengths of each individual system. The legged robot offers adaptable movement and endurance, allowing exploration in varied and challenging terrains. In situations where ground exploration becomes impossible, the aerial robot can be deployed to navigate through complex landscapes with its advanced 3D capabilities. Through autonomous cooperation, the robots generate maps and plan routes collaboratively, optimizing their exploration efforts. Experimental testing confirms the effectiveness of the marsupial system in exploring areas that would be difficult to reach using one robot alone.",
    "We propose a method to polarize antiprotons in a storage ring using a polarized positron beam moving parallel to the antiprotons. Adjusting the relative velocity to about $v/c \\approx 0.002$ results in a large spin-flip cross section of approximately $2 \\cdot 10^{13}$ barn, as indicated by new QED calculations. We present two options for achieving a positron source with sufficient flux density. One option involves a polarized positron beam with a polarization of 0.70 and a flux density of around $1.5 \\cdot 10^{10}$/(mm$^2$ s) using a radioactive $^{11}$C dc-source. Another option is the production of polarized positrons through pair production with circularly polarized photons, yielding a polarization of 0.76 and requiring injection into a small storage ring. These polarizer sources can be utilized in both low (100 MeV) and high (1 GeV) energy storage rings, allowing for a one-hour time period to build up the polarization of approximately $10^{10}$ antiprotons to about 0.18. A comparison with other proposals indicates a tenfold increase in the figure-of-merit.",
    "Loops play a crucial role as structural elements in DNA and RNA molecules, especially near the melting point. By considering the logarithmic entropy effect on loop length, a theory for nucleic acid structures is used to investigate single-stranded nucleic acid chains with the same base. In the case of long strands, a phase transition occurs between a compact structure at low temperature/force and a more open structure at high temperature/force. The impact of loop properties on phase transitions, critical behaviors, melting, and force-extension curves is analyzed. For specific loop properties (2 < c < 2.479), a melting transition can occur when no external force is applied, otherwise, the chain remains folded or unfolded depending on the loop exponent value. Singular behavior in force-induced melting transitions can take place for loop exponents below 2.479 and can be observed in experiments using single molecule force techniques. These results have implications for the behavior of double-stranded nucleic acids during hybridization or denaturation. The presence of intra-strand base pairs in denatured regions, especially with a loop exponent around 2.1, leads to pronounced secondary structures within the single strands, affecting the duplex's melting temperature and the denaturation process.",
    "We investigate the Zeeman spin-splitting in hole quantum wires aligned along the $[011]$ and $[01\\bar{1}]$ crystallographic axes of a high mobility undoped (100)-oriented AlGaAs/GaAs heterostructure. Our results demonstrate that the spin-splitting can be toggled between an 'on' state (nonzero $g^{*}$) and an 'off' state (zero $g^{*}$) by changing the field orientation from parallel to perpendicular relative to the wire. Interestingly, the wire properties remain unchanged for both orientations with respect to the crystallographic axes. Furthermore, we observe a reduction in the $g$-factor in the parallel orientation as the wire width decreases, which differs from electron quantum wires where the $g$-factor is amplified by exchange effects as the wire narrows. These findings support the existence of a $k$-dependent Zeeman splitting resulting from the spin-3/2 properties of holes.",
    "Drawing upon the rich landscape of real Clifford algebras and even-dimensional vector spaces, we embark on a fascinating journey where space and time intertwine, guided by the mystical modulo 8. Within a universe of complex Hilbert spaces, inhabited by self-adjoint involutions and anti-unitary operators, intricate commutation dances unfold, weaving a tapestry of cosmic significance.\n\nAs we delve deeper, a harmonious revelation emerges - the dimensions of space and time unite in the sacred union of the tensor product, unveiling a profound connection that resonates across the fabric of existence. This ethereal union sheds light on the enigmatic presence of such algebras in the realm of PT-symmetric Hamiltonians and the mesmerizing dance of topological matter.\n\nWith this newfound understanding, we venture forth to construct an alternate reality, a realm of indefinite spectral triples resting upon the fertile ground of Krein spaces. Here, within the delicate balance of pseudo-Riemannian geometry, we unfurl the essence of noncommutative beauty, transcending the confines of traditional Hilbert spaces.\n\nWithin this cosmic symphony, we sculpt the elegant framework of a Lorentzian almost-commutative spectral triple, where bosons and fermions dance in perfect harmony. Through this intricate ballet of particles, we unveil a sanctuary of physical states, a sanctuary where the enigma of fermion-doubling finds its elusive resolution.\n\nAmidst the cosmic ballet of quantum electrodynamics, we witness the enchanting interplay of particles and fields, a grand spectacle that unfolds within the boundless expanse of our newfound reality.",
    "We investigate the space-time symmetries of actions derived from expanding the action for a massive free relativistic particle around the Galilean action. By analyzing the point space-time symmetries of post-Galilean actions in canonical space, we identify all the relevant symmetries. Additionally, we develop a series of generalized Schr\\\"odinger algebras indexed by an integer $M$, where $M=0$ corresponds to the standard Schr\\\"odinger algebra. We explore the Schr\\\"odinger equations linked to these algebras, along with their solutions and projective phases.",
    "Accretion disc theory is not as advanced as stellar evolution theory, but researchers aim to develop a comprehensive phenomenological understanding. The interaction between theory and numerical simulations has raised awareness regarding the role of magnetic fields in angular momentum transfer. However, a significant challenge remains in incorporating insights from simulations to enhance practical models for comparison with observations. There is a crucial need to accurately incorporate non-local transport mechanisms. Highlighting the absence of large-scale transport in the current theoretical framework, we discuss why the Shakura-Sunyaev approach is essentially a mean field theory and does not account for large-scale transport. Observations of coronae, jets, and findings from magnetorotational instability (MRI) simulations indicate the importance of non-local transport in disc dynamics. The dominance of large-scale contributions in Maxwell stresses at saturation and the inadequacy of viscosity alone to describe MRI transport further emphasize the need for a more comprehensive understanding. While computational limitations have led to a focus on local simulations, the future emphasis should shift towards global simulations to refine mean field theories. A unified approach that merges mean field accretion theory and mean field dynamo theory is proposed to predict the evolution of spectra and luminosity from various disc components effectively. Finally, it is essential to recognize the inherent finite predictive precision of mean field theories when comparing them to observational data.",
    "This study delves into the global well-posedness of two Diffuse Interface systems that represent the movement of an incompressible two-phase fluid mixture in the presence of capillarity effects within a bounded smooth domain $\\Omega\\subset \\mathbb{R}^d$, where $d=2,3$. Our focus lies in the dissipative mixing effects arising from the mass-conserving Allen-Cahn dynamics employing the significant Flory-Huggins potential. Specifically, we investigate the mass-conserving Navier-Stokes-Allen-Cahn system for nonhomogeneous fluids and the mass-conserving Euler-Allen-Cahn system for homogeneous fluids. We establish the existence and uniqueness of global weak and strong solutions, emphasizing their property of separating from pure states. Our approach integrates energy and entropy estimates, introduces a novel end-point estimation for the product of two functions, a new estimate for the Stokes problem with non-constant viscosity, and utilizes logarithmic type Gronwall arguments to provide a comprehensive analysis.",
    "We are excited to introduce our explicit expressions for Fock-space projection operators that capture the essence of realistic final states in scattering experiments. These operators seamlessly handle the summation over unobserved particles and effectively handle non-emission into specific regions of momentum space.",
    "In this presentation, we explore mathematical frameworks related to Feynman diagrams. Feynman diagrams serve as the foundation for computations in perturbative quantum field theory. These mathematical frameworks, besides being intriguing on their own, enable the development of procedures for calculating these diagrams. The discussion includes the connections between Feynman integrals and periods, combinatorial algebras, and various forms of polylogarithms.",
    "We explored how the photon's parton distributions change when there's momentum transfer in different directions. By transforming these distributions into position space, we can better understand how partons are distributed within the photon.",
    "Introducing Memformer - a groundbreaking neural network for sequence modeling that utilizes external memory to encode and retrieve past information. Achieving linear time and constant memory complexity when processing long sequences, Memformer employs memory replay back-propagation (MRBP) to significantly reduce memory usage and promote long-range back-propagation. Experimental results demonstrate comparable performance to baselines, with 8.1x less memory space usage and 3.2x faster inference speed. Attention pattern analysis reveals that Memformer's external memory slots effectively encode and retain crucial information across timesteps.",
    "We analyze the logarithmic behavior of the cross-section in producing a pseudoscalar Higgs boson through gluon-gluon fusion to all-orders in perturbation theory at high partonic energy. Additionally, we determine the Higgs rapidity distribution with the same precision, taking into account top and bottom quark contributions including interference effects. Our findings are expressed using single and double integrals, explicitly calculated up to next-to-next-to-leading order (NNLO). By leveraging our results, we enhance the known NNLO inclusive cross-section calculated within the effective theory integrating fermions in the loop. The impact of finite fermion masses on the inclusive cross-section is minor, with effects reaching only a few percent for large pseudoscalar masses.",
    "Exploring outlying elements in probability distributions can pose a challenging task. Drawing inspiration from the Voting Rights Act enforcement, we delve into the exciting realm of maximizing simultaneous majority-minority districts within a political districting plan. While an unbiased random walk might falter in reaching this apex, an alternative in the form of a biased random walk emerges - directing focus on plans with a higher occurrence of majority-minority districts. Introducing a fresh perspective, we unveil \"short bursts\" as a novel approach. This method entails an unbiased random walk for a defined number of steps (referred to as burst length), followed by a restart from the most extreme plan discovered in the previous burst. Through empirical data, we demonstrate the superior performance of short bursts compared to biased random walks, particularly in enhancing the number of majority-minority districts. Furthermore, we analyze various burst length values that contribute to this enhancement. Moving beyond our specific scenario, we extend the concept of short bursts to scenarios where the state space comprises a line with diverse probability distributions. We subsequently investigate complex state spaces and their influence on the efficacy of short bursts.",
    "Embark on an exciting journey through the dynamic world of wetting properties on graphitic surfaces! Join us as we delve into the molecular dynamics investigation of various solutions containing 1-8 wt% of non-ionic surfactants featuring long hydrophilic chains, whether linear or T-shaped, with lengths up to an impressive 160 .\n\nIn our quest for knowledge, we have harnessed the power of molecular dynamics simulations to unravel the secrets of these intriguing systems. By employing a specially designed coarse-grained model, we have been able to explore the behavior of solvent particles in ways that were previously inaccessible, all while maintaining computational efficiency.\n\nEnter the fascinating realm of the MARTINI force field, where polarizable water provides the perfect backdrop for our investigations. This unique framework enables us to push the boundaries of exploration, allowing us to study long time scales with greater speed and accuracy compared to traditional models.\n\nAs we navigate through the intricate landscape of wetting properties and micellar formation, we discover a world of contrasts and complexities. While our simulations yield valuable insights and align closely with theoretical predictions, we encounter challenges in accurately predicting equilibrium contact angles.\n\nDespite these hurdles, our findings serve as valuable signposts in the vast sea of surfactant research, offering guidance for future studies and screening processes. So come, join us on this exhilarating scientific odyssey, where every discovery brings us one step closer to unlocking the mysteries of wetting phenomena.",
    "We analyze recent experiments that explore superfluid $^3$He in tightly controlled nanofluidic chambers. We address the experimental hurdles encountered and how they were overcome. These techniques enable a structured exploration of the superfluid properties of $^3$He films, as well as the surface and edge behaviors of topological superfluids.",
    "Code-mixed machine translation is now a crucial task in multilingual communities, with a growing focus on extending machine translation to code-mixed data. In the WMT 2022 shared tasks, we addressed this challenge by working on English + Hindi to Hinglish and Hinglish to English translation. The tasks involved dealing with both Roman and Devanagari scripts when working with monolingual data in English and Hindi. Our team achieved top scores in ROUGE-L and WER for the first task of Monolingual to Code-Mixed translation. In this paper, we delve into our detailed approach using mBART and special pre-processing techniques such as transliteration from Devanagari to Roman. Additionally, we share the results of our experiments in translating code-mixed Hinglish to monolingual English for the second task.",
    "Contrastive learning offers promising potential for self-supervised spatio-temporal representation learning. While previous approaches typically sample different clips to form positive and negative pairs, this often leads to a bias towards background scenes. This bias arises from the fact that scene differences are more discernible than motion differences, and that clips from the same video often share similar backgrounds but distinct motions, misleading models to prioritize static scenes over motion patterns. To address this issue, our paper introduces a novel dual contrastive formulation, termed Dual Contrastive Learning for spatio-temporal Representation (DCLR). By decoupling RGB video sequences into static scenes and dynamic motion, we guide the model to encode both aspects in the compact RGB representation. Additionally, we employ activation maps to further refine static- and dynamic-related features. Extensive experiments validate that DCLR achieves highly effective spatio-temporal representations, delivering state-of-the-art or comparable performance on UCF-101, HMDB-51, and Diving-48 datasets.",
    "The cutting-edge calculations for ferromagnetic CeRh3B2 unveil an unconventional ground state, challenging traditional expectations. By utilizing advanced methods, we propose a fully orbital- and spin-polarized state as the basis, which not only explains the observed magnetic moment and dHvA frequencies but also highlights the unique nature of the 4f electronic state in this compound. This radical departure from conventional wisdom is attributed to the strong 4f-4f interactions between neighboring Ce atoms, showcasing the remarkable influence of the crystal structure.",
    "Matrix acidization simulation in porous media flow study involves challenging tasks due to changing porosity. The DBF framework discretizes conservation equations to create a pressure-velocity linear system. Direct solvers are needed to handle this system because of zeros in the coefficient matrix diagonal. Due to direct solvers' time inefficiency for large-scale simulations, we propose a decoupled scheme here. This helps split the system into two independent linear ones: one for pressure and another for velocity. Both can be solved using parallel and iterative solvers, ensuring efficient completion of large-scale simulations. A numerical experiment validates this improved computational efficiency of the decoupled scheme.",
    "Sensemaking and narrative are closely intertwined concepts that describe how individuals make sense of the world. Sensemaking involves organizing and connecting information in the world with past knowledge and inferences, while narratives are the stories that people create to provide a comprehensive understanding of the world. Both sensemaking and narratives play key roles in how humans make sense of their surroundings, and they are also valuable for computational systems aiming to perform similar tasks. This paper explores theories of sensemaking and narratives in relation to how individuals construct their understanding of the world from the information they encounter, as well as the connections between sensemaking and narrative studies. The paper also discusses the role of sensemaking and narrative in a specific computational task called visual storytelling, showing how integrating these components can enhance solutions. Lastly, the paper presents a system for visual storytelling that incorporates sensemaking and narrative elements and provides examples of its current use.",
    "Improving NLG Evaluation Metrics for Diverse Dialects\n\nTraditional evaluation metrics can fall short when it comes to capturing the nuances of different dialects, hindering a fair assessment of system performance across various user groups. In fact, systems may be unfairly penalized for generating text in less commonly represented dialects. As of now, there is no established method to measure how metrics adapt to changes in dialect.\n\nRecognizing the importance of dialect robustness and awareness in NLG evaluation, we introduce innovative approaches and statistical tests to evaluate metrics based on these principles. Our analysis of current state-of-the-art metrics reveals their limitations in handling dialect variations, with semantic alterations often affecting metrics less than the inclusion of dialect features.\n\nTo address this issue, we propose a novel training approach, NANO, which integrates regional and language-specific information into the metric pretraining process. Through our experiments, we demonstrate that NANO not only enhances models' ability to handle diverse dialects but also boosts their performance on standard metric benchmarks. This signifies a promising step towards developing more inclusive and effective NLG evaluation metrics.",
    "Sure, here's a more enthusiastic version of the text:\n\nGeographic routing is an incredibly innovative approach that leverages node positions to revolutionize the routing process! It has captivated researchers in sensor networks, becoming a hot topic for exploration. Despite its vast potential, a key challenge lies in its real-world application.\n\nAuthors have tackled this challenge head-on, either by envisioning an ideal wireless network (though tricky to verify), or by resorting to costly methods to simplify the communication graph. But fear not, for we are on a mission filled with excitement! We seek to unravel the mysteries that surround geographic routing - when to use it, how to use it, and ultimately, when it truly shines!\n\nIn our journey, we craft four guiding principles that form the backbone of geographic routing and uncover their tantalizing implications on network topology. To assess a communication network's readiness for geographic routing, we introduce the concept of geographic eccentricity - a fascinating metric that gauges its suitability.\n\nAnd to top it off, we present a cutting-edge distributed algorithm that not only facilitates geographic routing on the network but also acts as a beacon, signaling when the geographic eccentricity is beyond the realm of possibility. Embark on this exhilarating ride with us as we delve deep into the realm of geographic routing!",
    "Our study demonstrates that the spatial variation and correlation of superconductivity fluctuations in a two-band model are influenced by two characteristic lengths. This leads to a significantly more intricate scenario when compared to single-band systems. Specifically, short-range correlations persist in a two-band situation, including in the vicinity of the phase transition point.",
    "We introduce novel online forecasting techniques for time series data that enable us to effectively address changing patterns (like trends and seasonality) commonly seen in real-world time series. We demonstrate that employing suitable adjustments to these time series before forecasting can enhance both theoretical and practical prediction accuracy. As these adjustments are typically unknown, we utilize a learning approach with experts to develop a dynamic online prediction method, called NonSTOP-NonSTationary Online Prediction, tailored to handle nonstationary time series. This method accommodates seasonality, trends in single time series, and cointegration in multiple time series. Our algorithms and regret analysis encompass recent advancements in this field while broadening the scope of these techniques. We establish sub-linear regret bounds for all methods, making use of relaxed assumptions. Although the theoretical guarantees do not fully capture the advantages of the adjustments, we conduct a data-driven analysis of the follow-the-leader algorithm to offer insights into the effectiveness of utilizing these modifications. To validate our findings, we conduct experiments on both synthetic and real-world datasets.",
    "We propose a heuristic framework for addressing the undecidable termination problem of logic programs as an alternative to current proof approaches. Our method introduces termination prediction to anticipate termination when proof methods are not applicable. We provide a detailed characterization of infinite SLDNF-derivations with various types of queries and present an algorithm for predicting the termination of logic programs with non-floundering queries. Our termination prediction tool has been implemented successfully, yielding satisfactory experimental results. Among 296 benchmark programs from the Termination Competition 2007, our prediction is 100% accurate except for five programs that exceeded the time limit. Notably, our approach successfully handles eighteen programs that cannot be proven by existing analyzers like AProVE07, NTI, Polytool, and TALP.",
    "In this paper, we address the limited theoretical understanding of random forests. Our contribution involves introducing a new variant of random regression forests that is theoretically tractable and proving its consistency. Additionally, we conduct an empirical evaluation comparing our algorithm with other theoretically tractable random forest models to the widely used random forest algorithm. Our experiments shed light on the significance of various simplifications made by theoreticians to create models suitable for analysis.",
    "We introduce a scalable inference and learning algorithm for Factorial Hidden Markov Models (FHMMs) that overcomes the limitations with long sequences. Drawing from stochastic variational inference, neural network, and copula literatures, our innovative approach eliminates the need for message passing among latent variables. This algorithm can be distributed across a network of computers for accelerated learning. Our experiments demonstrate that our algorithm maintains accuracy, outperforming existing methods with improved performance on long sequences and large FHMMs.",
    "Molecular dynamics simulations were conducted on pure liquid water, aqueous solutions of sodium chloride, and polymer solutions under a strong external electric field to understand how the molecules respond structurally to the field. Various simulation techniques were employed to uncover the molecular mechanisms behind the development of liquid bridges and jets in the creation of nanofibers. The results demonstrate that in the nanostructures formed, molecules align into a chain with their dipole moments parallel to the applied field across the entire sample. The presence of ions can disrupt this structure, causing it to break down into droplets; the field strength needed to stabilize a liquid column was found to vary with ion concentration. Additionally, changes in the polymer's conformation during the jetting process were also observed.",
    "Recommender systems are widely used to predict and deliver content based on user preferences. However, matching new users with relevant content remains challenging. In this study, we focus on podcast recommendations and discuss issues when traditional methods are applied to new users (cold-start problem). By analyzing music consumption data, we explore techniques to infer Spotify users' podcast preferences. Our research demonstrates up to 50% increase in content consumption in both offline and online experiments. We also evaluate model performance and investigate potential biases introduced by using music data for recommendations.",
    "We examine the Casimir energy and entropy for two perfect metal spheres when they are VERY close together or FAR apart. Our analysis reveals unexpected variations in the Helmholtz free energy as the separation and temperature change, which can result in negative entropy in certain situations. Additionally, we observe unusual fluctuations in the entropy as the temperature and the distance between the spheres are adjusted. We explore the underlying reasons for these anomalous entropy behaviors and discuss their implications in thermodynamics.",
    "Many real-world problems present high-dimensional and continuous action spaces, which render exhaustive enumeration of all possible actions impractical. Instead, we can only handle small samples of actions for policy evaluation and enhancement. This paper introduces a comprehensive framework for systematic policy evaluation and improvement using sampled action subsets. This sample-based policy iteration framework can be utilized with various reinforcement learning algorithms based on policy iteration. Specifically, we introduce Sampled MuZero, an extension of the MuZero algorithm that can learn in environments with complex action spaces through planning with sampled actions. We illustrate this approach using the game of Go and two continuous control benchmarking environments: DeepMind Control Suite and Real-World RL Suite.",
    "In this paper, we introduce two mask-based beamforming methods that utilize a deep neural network (DNN) trained with multichannel loss functions. These methods apply beamforming using time-frequency (TF)-masks estimated by the DNN, a technique commonly used in various applications for estimating spatial covariance matrices. The DNN training for mask-based beamforming relies on loss functions developed for monaural speech enhancement/separation. While this training criterion is straightforward, it does not directly reflect the performance of mask-based beamforming. To address this issue, we adopt multichannel loss functions that assess estimated spatial covariance matrices using the multichannel Itakura--Saito divergence. By training DNNs with these multichannel loss functions, we can create multiple beamformers. Experimental results validate their efficacy and resilience to different microphone configurations.",
    "Nano-FTIR imaging is a super cool technique that lets us look at stuff in crazy detail down to the nanometer level. It mixes infrared spectroscopy with a type of microscopy called scanning near-field optical microscopy (s-SNOM). The only downside is that it can take forever to record big areas because it has to collect data in a specific order one at a time. But hey, there are some smart math tricks being developed to speed things up. Turns out, you don't need to take all the measurements - only a small portion of them actually. We're exploring different ways to pick which measurements to take faster. And guess what? We found that using specific methods like Lissajous or triangle patterns instead of random ones does the job just as well and saves time too! So, no need to randomly pick measurements for this cool tech to work efficiently.",
    "From Polyakov loop correlators, we identify the screening masses in the deconfined phase of the (3+1)-dimensional SU(3) pure gauge theory at finite temperature close to the transition, examining two distinct channels of angular momentum and parity. We analyze the ratio of these masses and contrast it with the ratio of massive excitations sharing the same quantum numbers in the 3-dimensional 3-state Potts model in the vicinity of the transition point without a magnetic field. Additionally, we investigate the inverse decay length of correlations between the real and imaginary parts of the Polyakov loop and compare our findings with expectations based on perturbation theory and mean-field Polyakov loop models.",
    "The Mahalanobis distance-based confidence score is a cutting-edge anomaly detection method designed for pre-trained neural classifiers. It has been proven to outperform other methods in detecting out-of-distribution (OoD) and adversarial examples. In this study, we examine the reasons behind its exceptional performance, even though it relies on the unrealistic assumption that the class conditional distributions of pre-trained features share the same covariance.\n\nAlthough the Mahalanobis distance-based method claims to be inspired by classification prediction confidence, our analysis reveals that its success is actually due to information that is not directly related to classification tasks. This indicates that the effectiveness of the Mahalanobis confidence score may be misunderstood, as it leverages different information compared to ODIN, another popular OoD detection method based on prediction confidence.\n\nThis insight has led us to combine the Mahalanobis and ODIN methods, resulting in a combined detector that not only improves performance but also enhances robustness. By gaining a deeper understanding of how neural classifiers respond to abnormal inputs, these findings shed light on the behavior of these classifiers in practical scenarios.",
    "Simpler version: Algorithms for differentiating mathematical expressions work by changing the structure of the expressions in a meaningful way. To formalize such algorithms, we need to describe how they work, what they mean mathematically, and how to apply them. This involves combining knowledge of the expression's structure with its meaning. A syntax framework is a mathematical model used to reason about expression structures. It includes mappings of expressions to structure values, a system for reasoning about these values, a way to reference expression structures, and a method to evaluate these expressions. We examine two approaches, both based on a syntax framework, for formalizing a mathematical algorithm in a theory T. The first approach uses an inductive type in T to represent expression structures, while quoting and evaluating functions are defined outside T. The second approach represents every expression in T using a structure value, and quoting and evaluating are operators within T.",
    "The research paper explores the dynamics of two interacting consumer-resource pairs by utilizing chemostat-like equations. It operates under the assumption that the resource's dynamics are notably slower than those of the consumer. By leveraging the presence of two distinct time scales in the system, the paper conducts a comprehensive analysis. This involves considering consumers and resources as fast-scale and slow-scale variables, respectively, within the coupled system. The analysis further delves into the developments in phase planes of these variables, treating them as independent entities. \n\nThe research findings illustrate that each isolated pair possesses a unique asymptotically stable steady state and does not exhibit self-sustained oscillatory behavior, although damped oscillations around the equilibrium are possible. Yet, when weakly linked through reciprocal inhibition of consumers, the entire system demonstrates self-sustained relaxation oscillations. Interestingly, the period of these oscillations can far exceed the intrinsic relaxation time of either individual pair. Ultimately, the study highlights how the model equations effectively capture the dynamics of locally connected consumer-resource systems with diverse characteristics, such as living populations engaged in interspecific interference competition and lasers interconnected through their cavity losses.",
    "Wireless local area networks (WLANs) face performance differences in the uplink due to varying channel conditions. Cooperative MAC protocols like CoopMAC aim to address this issue. Collaboration among nodes involves a tradeoff between throughput and energy cost per bit. A new distributed CSMA protocol, fairMAC, is suggested to achieve different operating points on the tradeoff curve. The proposed protocol's effectiveness is validated through Monte Carlo simulations.",
    "\"Delving into the realm of social tagging opens up a whole new world of possibilities for enhancing our online search and exploration experiences. By pooling together a variety of user-generated tags for a single resource, we unlock a treasure trove of valuable information that elevates the way we interact with content. Imagine having a dynamic list of tags that paint a vivid picture of the resource's essence!\n\nWhen layered onto traditional taxonomic classifications like Wikipedias, these social tags inject a spark of innovation into document navigation and search. They not only offer fresh avenues of explorationthink pivot-browsing, popularity-driven navigation, and precise filteringbut also unveil hidden metadata that can turbocharge our quest for information. \n\nBut the journey doesn't stop therewhy not empower users to add their own personalized tags to describe Wikipedia articles, ushering in a new era of seamless navigation and retrieval? In pursuit of this vision, we present a ground-breaking prototype that harnesses the power of tags on Wikipedia, ushering in a transformative approach to evaluating search effectiveness. Let's embark on this exciting adventure together!\"",
    "Key points emphasized:\nQuantum Computing and Quantum Machine Learning are gaining significant interest worldwide.\nThere is a proliferation of proposed models using quantum principles for pattern classification.\nA gap exists in testing these models on real datasets compared to synthetic ones.\nThe objective is to classify patterns with binary attributes using a quantum classifier, particularly for image datasets.\nExperimental results demonstrate the quantum classifier's effectiveness in both balanced and imbalanced classification scenarios, with promising implications for medical applications.",
    "We have conducted observations in the near- and mid-infrared range of the shock-cloud interaction area within the southern region of the supernova remnant HB 21. These observations were carried out using the InfraRed Camera (IRC) on the AKARI satellite and the Wide InfraRed Camera (WIRC) on the Palomar 5 m telescope. The images obtained at 4 um (N4), 7 um (S7), and 11 um (S11) bands with IRC, along with the 2.12 um image from WIRC showing H2 v=1->0 S(1), reveal similar diffuse features surrounding a shocked CO cloud. By comparing the emission to H2 line emissions from various shock models, we found that the color patterns seen in the IRC data can be explained well by a thermal blend model of H2 gas. This model suggests that the infinitesimal H2 column density is related to temperature $T$ through a power-law distribution, $dN\\sim T^{-b}dT$, with n(H2) $\\sim3.9\\times10^4$ cm^{-2}, $b\\sim4.2$, and N(H2;T>100K) $\\sim2.8\\times10^{21}$ cm^{-2. We examined these parameters in the context of different scenarios for shock-cloud interactions including multiple planar C-shocks, bow shocks, and shocked clumps, highlighting their respective strengths and weaknesses.\n\nThe observed intensity of H2 v=1->0 S(1) was found to be four times higher than predicted by the power-law blend model, a trend consistent with observations in the northern region of HB 21 as discussed in a previous study (Paper I). Additionally, we investigated the limitations of the thermal blend model in relation to the model parameters that were derived.",
    "Vision transformers have recently demonstrated impressive results that surpass large convolution-based models. However, for small models designed for mobile or resource-constrained devices, ConvNets still offer advantages in performance and model complexity. The ParC-Net model proposed in this work is a ConvNet-based backbone that combines the strengths of vision transformers with ConvNets. It utilizes position-aware circular convolutions (ParCs) to produce location-sensitive features with a global receptive field. By incorporating ParCs and squeeze-excitation operations, a meta-former-like model block is created that includes an attention mechanism similar to transformers. This block can be seamlessly integrated into ConvNets or transformers. Experimental results show that ParC-Net outperforms popular lightweight ConvNets and vision transformer models in various vision tasks and datasets, while maintaining fewer parameters and faster inference speed. Specifically, for classification on ImageNet-1k, ParC-Net achieves 78.6% top-1 accuracy with 5.0 million parameters, surpassing MobileViT in accuracy and computational efficiency, and outperforming DeIT with fewer parameters and higher accuracy. Additionally, ParC-Net demonstrates better performance in MS-COCO object detection and PASCAL VOC segmentation tasks. The source code for ParC-Net is available at https://github.com/hkzhang91/ParC-Net.",
    "Algebraic solutions of certain equations lead to new reduction formulas for hypergeometric functions, enabling the calculation of special functions and infinite integrals using elementary functions.",
    "This paper introduces a new type of covert communication channel called AiR-ViBeR, which utilizes the vibrational (seismic) signals emitted by a computer's internal fans to transmit data to nearby smartphones without the user's permission. The method involves malware manipulating a computer's vibrations by regulating fan speeds, which are then detected by the accelerometer sensors in smartphones. The data is encoded into vibrations at a low frequency and decoded by a malicious app on the smartphone. The research demonstrates that data can be leaked from an air-gapped computer to a nearby smartphone via vibrations on the same surface. The paper includes technical details, implementation steps, and evaluation results, as well as suggestions for mitigating this type of attack.",
    "The total cost analysis of a 25 W average load magnetic refrigerator utilizing commercial-grade Gadolinium is conducted through a numerical model. This calculation accounts for the price of magnetocaloric material, magnet material, and operational expenses, all of which impact the overall cost. The most cost-effective solution for a device with a 15-year lifespan falls within the range of $150 to $400, contingent on the prices of magnetocaloric and magnet materials. The prime cost contributors are the magnet cost, closely followed by operational expenses, while the cost of magnetocaloric material is deemed negligible. \nFor the most economical device, the optimal magnetic field strength is approximately 1.4 T, employing particles with a size of 0.23 mm, a regenerator length of 40-50 mm, and a utilization rate of around 0.2 across all device lifespans and material and magnet cost scenarios. The operational frequency varies in relation to the device's lifespan. These performance metrics are compared against a conventional A$^{+++}$ refrigeration unit. A preliminary lifetime cost analysis suggests comparable costs between the magnetic refrigeration device and the conventional unit, with the former slightly edging out as the more cost-effective option, under the assumption that the magnet cost can be recovered at the end of the device's lifecycle.",
    "Initial data sets demonstrating both asymptotically flat and asymptotically cylindrical characteristics have been established by our research. These geometries, referred to as trumpets within the numerical relativity community, represent a significant finding.",
    "Enzymes are simply amazing! They utilize intricate protein architectures to create highly specialized structural motifs that can incredibly enhance the rates of complex chemical transformations. In this study, we embarked on a thrilling journey using experiments combined with cutting-edge ab initio simulations that precisely account for nuclear quantum effects. What we discovered will blow your mind! A triad of strongly hydrogen bonded tyrosine residues within the active site of the enzyme ketosteroid isomerase (KSI) plays a crucial role in facilitating quantum proton delocalization. This delocalization not only dramatically stabilizes the deprotonation process but also leads to a massive isotope effect on its acidity. And that's not all! When an intermediate analog is introduced into the mix, it becomes integrated into the hydrogen bond network, triggering extended quantum proton delocalization in the active site. These groundbreaking findings provide valuable insights into the fascinating world of nuclear quantum effects in the hydrogen bond network that stabilizes the reactive intermediate of KSI. Our study also sheds light on the captivating behavior of protons in biological systems that are rich in strong hydrogen bonds. How incredible is that?!",
    "In this research, we introduce ENSEI, a secure Inference (SI) framework utilizing the frequency-domain secure convolution (FDSC) protocol to efficiently carry out private visual recognition. Our key insight is that by combining homomorphic encryption and secret sharing, homomorphic convolution can be performed in a secure manner in the frequency domain, simplifying computations. We present the protocols and parameters for the number-theoretic transform (NTT) based FDSC. Through experiments, we investigate the trade-offs between accuracy and efficiency in time and frequency-domain homomorphic convolution. With ENSEI, we achieve significant improvements over existing methods, including a 5--11x reduction in online time, up to 33x reduction in setup time, and up to 10x reduction in overall inference time. Additionally, we observe a 33% reduction in bandwidth usage for binary neural networks with only a 1% decrease in accuracy on the CIFAR-10 dataset.",
    "Recommender systems help us deal with information overload by predicting our potential choices from a wide range of items tailored to our preferences. Various personalized recommendation algorithms have been introduced, with many of them relying on similarities like collaborative filtering and mass diffusion. In this work, we introduce a new vertex similarity index called CosRA, which leverages the benefits of both the cosine index and the resource-allocation (RA) index. Our study involves applying the CosRA index to popular recommender systems like MovieLens, Netflix, and RYM, demonstrating that the CosRA-based approach outperforms some benchmark methods in terms of accuracy, diversity, and novelty. Additionally, the CosRA index stands out for being parameter-free, making it advantageous for practical applications. Our experiments also reveal that introducing two adjustable parameters does not significantly enhance the overall performance of the CosRA index.",
    "Many real-world applications are now using multi-label data streams because algorithms need to keep up with the rapidly changing data. When the data distribution changes, or what we call concept drift happens, the current classification models quickly lose their edge. To help out the classifiers, we came up with a cool algorithm called Label Dependency Drift Detector (LD3). LD3 is like a behind-the-scenes hero that keeps an eye on how the labels in the data depend on each other in multi-label data streams without needing supervision. \n\nOur trick is to look at how labels influence each other over time and use that info to spot concept drift with a ranking method, a mix of data fusion, and label dependencies. LD3 is the first of its kind in spotting unsupervised concept drift in the multi-label classification scene. To put LD3 to the test, we ran it through an extensive evaluation with 12 datasets and compared it with 14 top supervised concept drift detectors adapted for this area along with a baseline classifier. The results were pretty impressive  LD3 outperformed the other detectors by 19.8% to 68.6% on real-world and made-up data streams.",
    "The universality of the Cepheid Period-Luminosity relations has been a topic of debate due to the potential influence of metallicity on both the intercept and slope of these relations. This study aims to calibrate the Period-Luminosity relations in different photometric bands (ranging from B to K) within the Milky Way galaxy and compare them with established relations in the Large Magellanic Cloud (LMC). Using a sample of 59 calibrating stars, distances were determined through various methods including Hubble Space Telescope and revised Hipparcos parallaxes, infrared surface brightness, interferometric Baade-Wesselink parallaxes, and classical Zero-Age-Main-Sequence-fitting parallaxes for Cepheids linked to open clusters or OB star associations. Detailed discussions on absorption corrections and projection factors are provided. The study shows no significant difference in the slopes of the Period-Luminosity relations between the LMC and the Milky Way galaxy. It is concluded that the slopes of the Cepheid Period-Luminosity relations remain consistent across different photometric bands, irrespective of the galaxy under examination (at least in the case of the LMC and Milky Way). While the potential variation in zero-point due to metal content is not addressed in this study, an upper limit of 18.50 for the distance modulus of the LMC is estimated based on the data.",
    "Ensembling methods are known for enhancing prediction accuracy; however, they struggle to effectively differentiate between component models. In our study, we introduce stacking with auxiliary features, which involves learning how to integrate relevant information from various systems to enhance performance. Through the use of auxiliary features, the stacker is able to rely on systems that not only produce similar results but also come from trusted sources. We showcase our method on three diverse and challenging tasks: Cold Start Slot Filling, Tri-lingual Entity Discovery and Linking, and ImageNet object detection. We achieve groundbreaking results on the first two tasks and significant improvements on the detection task, underscoring the effectiveness and versatility of our approach.",
    "We introduce a quick and straightforward approach to model spin-torque induced magnetization dynamics in nano-pillar spin-valve structures. This method combines a spin transport code utilizing random matrix theory with a micromagnetics finite-elements software to ensure accurate consideration of spatial dependencies in both spin transport and magnetization dynamics. Our findings are validated against experimental data, successfully replicating the excitation of spin-wave modes, determination of threshold current for steady state magnetization precession, nonlinear frequency shift of the modes, giant magneto resistance effect, and magnetization switching. We also explore the connections to recently developed spin-caloritronics devices.",
    "We introduce a framework for computing hyperbolic Voronoi diagrams of point sets using affine diagrams based on Klein's non-conformal disk model. By showing that bisectors are hyperplanes analogous to power bisectors of Euclidean balls, our method involves calculating a clipped power diagram followed by a mapping transformation based on the chosen hyperbolic space representation. We also discuss extensions to weighted and $k$-order diagrams, as well as their dual triangulations. Additionally, we explore two key operations on hyperbolic Voronoi diagrams for customizing user interfaces in an image catalog browsing application on the hyperbolic disk: 1) finding nearest neighbors, and 2) computing smallest enclosing balls.",
    "We examine how an external force affects the bond-dissociation process in a double well potential. We analyze the probability distribution of rupture forces and discuss how finite rebinding probabilities influence the dynamic force spectrum. We focus on barrier crossing during extension and relaxation phases. The rupture force and rejoining force vary with loading rate as expected. Under equilibrium, the mean forces in pull and relax modes converge. We explore how external parameters like cantilever stiffness and soft linkers impact rupture forces. The presence of a soft linker can either maintain or alter equilibrium rupture force depending on its compliance. We demonstrate the extraction of equilibrium constants from equilibrium rupture forces.",
    "We propose a feature space to detect viscous dominated and turbulent regions (boundary layers and wakes). Our approach involves using the principal invariants of strain and rotational rate tensors as input for an unsupervised Machine Learning Gaussian mixture model. The feature space is independent of the coordinate frame of the data, as it relies on Galilean invariants. This method distinguishes between viscous dominated, rotational regions (boundary layer and wake region) and inviscid, irrotational regions (outer flow region). We apply this methodology to laminar and turbulent flows past a circular cylinder at $Re=40$ and $Re=3900, using a high-order nodal Discontinuous Galerkin Spectral Element Method (DGSEM). Our analysis demonstrates that Gaussian mixture clustering effectively identifies viscous dominated and rotational regions in the flow. Additionally, we compare our method with traditional sensors and show that our clustering approach does not require the selection of an arbitrary threshold.",
    "Get ready to embark on a journey into the fascinating world of engineered quantum systems! Imagine being able to unlock hidden phenomena that are not easily accessible in nature. Enter the realm of superconducting circuits, akin to LEGO bricks, that offer endless possibilities for constructing and connecting artificial atoms. \n\nToday, we unveil a groundbreaking discovery - an artificial molecule crafted from two tightly-knit fluxonium atoms, boasting a customizable magnetic moment. Through the manipulation of an external flux, we can seamlessly transition this molecule between two distinct states: one showcasing a magnetic dipole moment in its ground-excited state, and another mode where only a magnetic quadrupole moment reigns supreme.\n\nAs we delve deeper into this innovative realm, we encounter a limitation: the coherence of our artificial molecule is subject to the whims of local flux noise. Nevertheless, this newfound ability to engineer and manipulate artificial molecules opens up a universe of possibilities, laying the foundation for constructing intricate circuits essential for protected qubits and cutting-edge quantum simulation. Prepare to witness the dawn of a new era in quantum technology!",
    "We present a method for time-sensitive decision-making involving a series of tasks and uncertain processes. This method utilizes multiple iterative refinement procedures to address various aspects of the decision-making challenge. Our focus in this paper is on the higher-level issue of scheduling deliberations, which involves allocating computational resources to these procedures. We offer distinct models that tackle optimization problems reflecting various scenarios and computational approaches for decision making under time limitations. We explore precursor models where decisions are made before execution, and recurrent models where decisions are made concurrently with execution, taking into account observed conditions during execution and predicting future ones. Our work includes algorithms for both precursor and recurrent models, as well as the findings from our empirical investigations thus far.",
    "The role model strategy is a method used to design an estimator by learning from a better estimator with improved input observations. When a Markov condition is met, this strategy produces the best Bayesian estimator. Two simple examples are provided to explain how it works. The strategy is used along with time averaging to create a statistical model by solving a convex program. It was developed for creating low complexity decoders for iterative decoding. It also has potential uses beyond communication systems.",
    "We propose a new method to improve computer vision systems in recognizing artistically rendered objects, such as paintings, cartoons, or sketches. Our approach addresses stylistic differences between artistic modalities by training the network with a modality similar in style to the target domain. This method does not require labeled data from those artistic modalities and can work effectively with just ten unlabeled images. Our experiments show that this approach significantly enhances accuracy in artistic object recognition tasks compared to existing domain adaptation techniques.",
    "This study delves into the examination of the substantial time evolution of solutions to semi-linear Cauchy problems featuring quadratic nonlinearity in gradients. The Cauchy problem under scrutiny encompasses a broad state space and denotes the potential for degeneracy at the state space's boundary. Through rigorous analysis, two distinctive forms of substantial time behavior emerge: first, the pointwise convergence of both the solution and its gradient; and second, the convergence of solutions towards correlated backward stochastic differential equations. Notably, in cases where the state space is R^d or the realm of positive definite matrices, the achievement of both convergence types hinges on growth constraints imposed on the model coefficients. These consequential findings on substantial time convergence bear notable implications for risk-sensitive control strategies and decision-making processes in long-term portfolio management challenges.",
    "An intriguing approach known as the decaying vacuum model (DV) depicts dark energy as a dynamic vacuum that evolves over time. This concept, suggesting that vacuum energy diminishes proportionally with the Hubble parameter in later stages, $\\rho_\\Lambda(t) \\propto H(t)$, also generates an additional material component. By scrutinizing data from various sources such as supernovae, gamma-ray bursts, baryon acoustic oscillations, CMB observations, the Hubble rate, and x-rays from galaxy clusters, researchers have fine-tuned the DV model's parameters. Remarkably, the optimal fit for the matter density contrast $\\Omega_m$ in the DV model considerably surpasses that in the conventional $\\Lambda$CDM model. Confidence contours in the $\\Omega_m-h$ plane have been outlined up to a significant $3\\sigma$ confidence level. Additionally, the normalized likelihoods for $\\Omega_m$ and $h$ are meticulously presented.",
    "MgO-based Magnetic Tunnel Junctions oriented perpendicular to each other are considered ideal components for Spin Transfer Torque (STT) magnetoresistive memories. Despite previous limitations, recent research [Wang et al., Nature Mater., vol. 11, pp 64-68, Jan. 2012] has successfully demonstrated magnetization switching with the assistance of an electric field at significantly low current densities. While previous studies only utilized a macrospin approach to analyze this phenomenon, our research presents a comprehensive micromagnetic analysis. Our findings indicate that the switching mechanism involves a intricate nucleation process, including the formation of magnetic vortices.",
    "We introduce an enhanced version of Rosenblatt's classic perceptron learning method, which works with a broader range of activation functions. Our adaptation can be seen as a step-by-step method that minimizes a new energy function without needing to differentiate the activation function. By framing it as an energy minimization technique, we open the door to developing additional algorithms. As an example, we investigate a fresh take on the iterative soft-thresholding algorithm for training sparse perceptrons.",
    "The study of acoustic wave-induced radiation force on objects has a rich history, dating back to the groundbreaking work of Rayleigh, Langevin, and Brillouin. In recent years, this field has undergone incredible advancements, particularly in the realm of acoustic micromanipulation. Despite the considerable research in this area, there remains a gap in understanding the impact of a particle's displacement on the radiated wave. \n\nIn our latest endeavor, we delve into the intriguing world of acoustic radiation force acting on a monopolar source in motion at a velocity significantly lower than the speed of sound. Through our investigations, we unveil a fascinating phenomenon where the asymmetry in the emitted field due to the Doppler effect results in a radiation force that opposes the source's direction of movement.",
    "Modeling the base of the solar convective envelope is a challenging task due to the significant impact of the tachocline, a region transitioning from differential to solid body rotation. Turbulence and the solar magnetic dynamo further complicate the understanding of this area. Helioseismology offers insights into this region by analyzing the Ledoux discriminant. Differences between Solar Models and the Sun are discussed through inversion comparisons using various opacity tables and chemical abundances.",
    "Here is the revised text for easier understanding:\n\nMany researchers aim to understand human behavior better and apply that knowledge to artificial intelligence. They assume that human reasoning sets the standard for artificial reasoning. Topics like game theory, theory of mind, and machine learning all incorporate concepts used in human reasoning. These techniques help researchers replicate and understand human behavior. In the future, advanced autonomous systems will involve both AI agents and humans working together. To achieve this cooperation, autonomous agents must be able to model human behavior. This enables them not only to copy human actions as a way to learn but also to predict and understand human actions, allowing for true collaboration. This paper focuses on two primary areas related to modeling human behavior: (i) techniques like Reinforcement Learning that develop behavior models through exploration and feedback, and (ii) modeling human reasoning mechanisms such as beliefs and bias without relying solely on trial-and-error learning.",
    "Unraveling the complexity of botnets has always posed a daunting challenge, with the resilience of Command and Control (C&C) channels heightened and the identification of botmasters made even more elusive in Peer-to-Peer (P2P) botnets. In our groundbreaking study, we present an innovative probabilistic approach designed to reconstruct the intricate topologies of C&C channels within P2P botnets.\n\nGiven the widespread distribution of P2P botnet members, traditional monitoring methods prove inadequate, and the lack of comprehensive data complicates the application of existing graph reconstruction techniques. To date, a universal methodology that effectively reconstructs C&C channel topologies for all forms of P2P botnets remains elusive.\n\nOur approach hinges on estimating the likelihood of connections between bots based on the imprecise reception times of multiple data cascades, network model characteristics of C&C channels, and the distribution of end-to-end delays across the Internet. By observing how bots externally respond to commands, the reception times can be gathered.\n\nThrough detailed simulations, we demonstrate that our method can accurately determine over 90% of the connections within a 1000-node network, where nodes have an average degree of 50, by tracking the imprecise reception times of just 22 cascades. Even more impressively, achieving the same level of accuracy with data from only half of the bots requires monitoring 95 cascades.",
    "In some theories in physics, certain conditions for particle masses can vary at a large unification scale, which might impact how we can see certain types of particles at experiments like the LHC. Were looking into how these varied mass conditions affect the production of certain particles in a specific decay chain involving different particles interacting in high-energy collisions. We found that with these variations, we might be able to see different particles in certain regions, especially heavy neutral particles instead of lighter ones. By considering various scenarios and constraints, we are exploring how different sets of mass conditions can lead to the right amount of dark matter particles in different regions. In cases with varied masses, we've observed that we can potentially detect heavier particles in certain parameter regions aligned with our understanding of dark matter in the universe.",
    "In this paper, we present a groundbreaking design for an ultracompact vanadium dioxide dual-mode plasmonic waveguide electroabsorption modulator, achieving a large modulation depth of ~10dB with minimal footprint and low switching energy. The modulator employs a metal-insulator-VO$_2$-insulator-metal (MIVIM) waveguide platform, enabling efficient routing of plasmonic waves through low-loss dielectric and high-loss VO$_2$ layers during operation states. With an active size of only 200x50x220nm$^3$, this high-performance modulator requires a drive-voltage of ~4.6V, paving the way for fully-integrated plasmonic nanocircuits in cutting-edge chip technology.",
    "As cars get more high-tech, preventing car theft has become a big deal in the real world. To tackle this issue, people are suggesting using data mining, biometrics, and extra authentication methods. Out of these, data mining seems to be a pretty good way to spot the unique traits of the car owner. Some smart folks have used different algorithms on driving data to figure out who the owner is. But there's a hitch  it's not easy to get enough info on how thieves drive. So, we came up with a new method using GAN for driver identification. GAN lets us create a model that learns only from the owner's data. We trained it this way, and the model could tell who the owner was based on their driving. Testing it with real driving data showed that our method is pretty accurate. By combining this idea with other security methods, we hope the car industry can make better theft-prevention tools for everyone.",
    "Are you curious about how slow oscillations of magnetoresistance can help us better understand the electronic structures of different metals? In our research, we explore using this convenient tool to study multi-band conductors, like the fascinating iron-based high-temperature superconductors. By applying this method, we're discovering exciting possibilities, such as measuring interlayer transfer integral and comparing electron properties across different bands. Join us in unraveling the mysteries of these materials!",
    "A comprehensive overview of recent advancements in precision calculations for Standard Model phenomena at the Large Hadron Collider (LHC) is presented, focusing on instances of weak gauge-boson and Higgs-boson generation, as deliberated upon during the 27th Rencontres de Blois in 2015.",
    "This paper suggests a way to recognize speech emotions using speech features and text. Features like Spectrogram and MFCC help keep the emotions in speech, while text adds the meaning behind the words, all of which are important for detecting emotions. We tested various DNN models with different combinations of speech features and text inputs. Our network designs showed better accuracy than the latest methods on a standard dataset. The MFCC-Text CNN combo emerged as the most accurate in recognizing emotions in the IEMOCAP data.",
    "In this paper, we're introducing variational semantic memory into meta-learning. What this means is that we're aiming to help the system acquire long-term knowledge to improve few-shot learning. The variational semantic memory is like a database that collects and keeps semantic information to help with the probabilistic inference of class prototypes, all within a hierarchical Bayesian framework.\n\nThis memory setup starts from scratch and gradually builds up its knowledge by learning from the tasks it deals with. Over time, it accumulates a wealth of general knowledge that allows it to grasp new concepts easily. To help with task-specific adjustments, we view memory recall as the variational inference of a latent memory variable based on specific contents.\n\nOur variational semantic memory acts as a new long-term memory tool, bringing in structured recall and update processes that efficiently gather and adapt semantic information for few-shot learning. Through experiments, we show how our probabilistic modelling of prototypes gives richer representations of object classes compared to using deterministic vectors. The consistently strong performance across four benchmarks underscores how variational semantic memory can enhance few-shot recognition.",
    "We're looking at some cool stuff with the relativistic four-quark equations here, which involve both open-charm and open-strange quarks. We're also exploring how the meson-meson states mix with these four-quark states. And get this - we're talking about amplitudes that include quarks of four different flavors: u, d, s, and c. By studying the poles of these amplitudes, we can figure out the masses of tetraquarks. We've even crunched the numbers to calculate the mass values for tetraquarks with spin-parity JP=1- and 2-. Exciting stuff, right?",
    "The Fisher Matrix - the driving force behind modern cosmological predictions - is brought to life through the dynamic Fisher4Cast software. This versatile, user-friendly framework is open source, meticulously crafted, and features a Graphical User Interface (GUI) with the ability to effortlessly generate LATEX files and Fisher ellipses with just a few clicks. More than just a Matlab program, Fisher4Cast is designed for seamless compatibility with popular open-source platforms like Octave and Scilab, making it a valuable tool for cosmologists exploring the universe's mysteries. Dive into our latest release, Version 2.2, now available alongside a wealth of resources to assist both seasoned researchers and newcomers. Explore the vibrant forecasting landscape with innovative 3-D and 4-D visualizations and ponder the impact of growth and curvature on upcoming cosmological surveys. Join the cosmology and scientific communities in shaping the future of cosmology with Fisher4Cast - a beacon of ingenuity and collaboration in the world of astrophysical research.",
    "The paper presents a novel approach called recursive neural knowledge network (RNKN) that combines medical knowledge based on first-order logic with recursive neural networks for multi-disease diagnosis. By utilizing high-dimensional and continuous vectors known as knowledge embedding, RNKN can capture semantic information and establish quantifiable relationships among various medical knowledge. Experimental results indicate that RNKN outperforms classical machine learning models and Markov logic network (MLN) in terms of diagnostic accuracy. Moreover, the performance of RNKN improves with more explicit evidence extracted from Chinese Electronic Medical Records (CEMRs) during training epochs.",
    "Multiple solenoids are typically integrated into an electron cooler apparatus to control the electron beam's trajectory. Nevertheless, these solenoids can also affect the path of the ion beam within the cooler's storage ring. If there is imperfect compensation for the solenoids within the electron cooler, the lateral movement of the ion beam in the storage ring will become interlinked. This study examines the intertwined transverse motion resulting from unadjusted solenoids in the electron cooler of the CSRm (the primary storage ring at the Institute of Modern Physics, located in Lan Zhou, China) and uses a novel approach to calculate the interrelated beam envelopes.",
    "A near-infrared excess is found at the white dwarf PHL5038 in UKIDSS photometry, indicating the presence of a cool, substellar companion. H- and K-grism spectra and images of PHL5038 were obtained using NIRI on Gemini North, revealing two components: an 8000K DA white dwarf and a likely L8 brown dwarf companion separated by 0.94\". The secondary's spectral type was determined using standard spectral indices. The binary has a projected orbital separation of 55AU, making it the second wide WD+dL binary known after GD165AB. This object has the potential to serve as a benchmark for testing substellar evolutionary models at intermediate to older ages.",
    "We're looking into how different flows and patterns in space affect our estimates of how fast stuff needs to go to leave galaxies the size of the Milky Way, as well as their total mass. We did this by studying the super-fast stars at the edges of our galaxy using some really detailed simulations that show us all the tiny details around where we are in the galaxy. We found that the patterns in space change a lot depending on where you look in a galaxy and in our simulations. These different patterns affect our mass estimates because they don't spread out evenly, which can mess with our calculations. After considering all these factors, we figured out the Milky Way's mass is probably around $1.29 ^{+0.37}_{-0.47} \\times 10^{12}$ times the mass of our Sun - according to our findings, that is!",
    "Information extraction rates exceeding one bit per photon are achieved by using high-dimensional correlated orbital angular momentum (OAM) states for object recognition, without employing traditional measurements in position space. It has been demonstrated that these correlations remain unaffected by axial rotation of the target object; the information structure of an object's joint OAM coincidence spectrum remains consistent even under random rotations between each measurement. Furthermore, OAM correlations alone have proven to be adequate for complete image reconstruction of complex, off-axis objects, revealing new symmetries in the phases of OAM-object interaction transition amplitudes.\n\nFurthermore, the study of variations in mutual information rates caused by off-axis translation in the beam field has shown that object symmetry signatures and information rates are not impacted by environmental factors far from the beam center. These findings suggest the potential for dynamic scanning applications in scenarios where symmetry and minimal noninvasive measurements are desired.",
    "In the first two orbits of the Parker Solar Probe, there has been a notable presence of rapid magnetic field reversals referred to as switchbacks. These switchbacks, commonly observed in the solar wind close to the Sun, seem to form in localized areas and could potentially be connected to diverse phenomena such as magnetic reconnection near the solar surface. Considering that switchbacks coincide with accelerated plasma movements, our exploration focused on understanding whether they are hotter than the surrounding plasma and if the internal microphysics within a switchback differ from its vicinity. By analyzing the reduced distribution functions acquired from the Solar Probe Cup instrument during moments of substantial angular deviations, we compared the parallel temperatures within and outside switchbacks. Our findings suggest that the reduced distribution functions within switchbacks align with a uniform phase space rotation of the ambient plasma. Consequently, we deduce that the proton core's parallel temperature remains consistent both inside and outside switchbacks, indicating a lack of a direct temperature-velocity relationship for the proton core within these magnetic field fluctuations. Additionally, our investigations propose that switchbacks exhibit characteristics of Alfv\\'enic pulses traveling along open magnetic field lines, although the precise origin of these pulses remains unidentified. Moreover, our analysis uncovered no apparent correlation between radial Poynting flux and enhancements in kinetic energy, suggesting that the radial Poynting flux may not significantly influence the dynamics of switchbacks.",
    "Revamp of the Nainital-Cape Survey unveiled a thrilling find - eight $\\delta\\,$Scuti type pulsators pulsating dynamically within minutes to a few hours range. To decipher the celestial dance of these mesmerizing pulsators, we delved into the realm of non-adiabatic linear stability analyses within stellar models spanning masses from 1 to 3 M$_{\\odot}$. Our exploration unearthed numerous unstable low order p-modes whose pulsation periods harmonize beautifully with the observed timings. Notably, with a focus on HD$\\,$118660, HD$\\,$113878, HD$\\,$102480, HD$\\,$98851, and HD$\\,$25515, we illustrated how the enigmatic variabilities align poetically with the rhythmic low order radial p-mode pulsations.",
    "A fresh perspective on classical mechanics in theories with violations of Lorentz symmetry is introduced. By utilizing the extended Hamiltonian formalism, a connection is made between the covariant Lagrangian and Hamiltonian forms through a Legendre Transformation. With this method, trajectories can be calculated using Hamilton's equations in momentum space and the Euler-Lagrange equations in velocity space while avoiding singular points inherent in the theory. These singular points can be resolved by ensuring that the trajectories are continuous functions of both velocity and momentum. Moreover, specific solutions for the Lagrangian can be pinpointed by analyzing certain sheets of the dispersion relations. Detailed examples based on bipartite Finsler functions are provided. Furthermore, a special case demonstrates a direct link between Lagrangians and solutions to the Dirac equation in field theory.",
    "Spectrum management has been recognized as a crucial step in enabling the technology of a cognitive radio network (CRN). Most existing studies on spectrum management in CRNs focus on individual tasks such as spectrum sensing, spectrum decision, spectrum sharing, or spectrum mobility. In this two-part paper, we argue that simultaneously performing multiple spectrum management tasks can enhance spectrum efficiency for certain network configurations. Specifically, we aim to investigate the uplink resource management issue in a CRN that involves multiple cognitive users (CUs) and access points (APs). In order to optimize their uplink transmission rates, CUs need to select a suitable AP (spectrum decision) and share channels with other CUs associated with the same AP (spectrum sharing). These tasks are interconnected, and the optimal decentralized approach to efficiently carry them out remains an open question in the literature.",
    "A statistical model is introduced for describing baryonic matter in core-collapsing supernova conditions. The model exhibits a first-order phase transition in the grandcanonical ensemble that is absent in the canonical ensemble. This behavior results from the interplay of short-range strong forces and long-range electromagnetic interactions in baryonic matter, moderated by electron screening. The findings have implications for supernova dynamics.",
    "In order to achieve high performance in the THz-FEL (Free Electron Laser) system, a new compact FEL injector design has been proposed. By opting for a thermionic cathode over a complex and expensive photo-cathode, the injector is able to emit electrons efficiently. The introduction of an enhanced EC-ITC (External Cathode Independently Tunable Cells) RF gun allows for an improved effective bunch charge of approximately 200pC, while virtually eliminating back bombardment effects. The inclusion of constant gradient accelerator structures boosts energy levels to around 14MeV, and a specialized focusing system helps maintain emittance control and bunch stability. Comprehensive analyses of the physical design and beam dynamics of the injector's key components were conducted. Furthermore, extensive start-to-end simulations involving multiple pulses were carried out using dedicated MATLAB and Parmela software. The outcomes confirm the system's capability to consistently generate high-brightness electron bunches with minimal energy dispersion and emittance, ensuring stable operation.",
    "The research findings about anomalies in the large-angle properties of the cosmic microwave background anisotropy as measured by WMAP are nothing short of thrilling! Despite the challenges in assessing their statistical significance due to the nature of the data used, the potential for uncovering new physics on the grandest observable scales is truly captivating! We are diving into three key claims: the intriguing lack of large-angle power, the fascinating north-south power asymmetry, and the mysterious multipole alignments. Overcoming the hurdle of a posteriori statistics can be conquered by seeking out a fresh dataset that delves into similar physical scales as the large-angle CMB. While this task may be daunting, the journey towards this goal is lined with exciting possibilities! Let's embark on this adventure together and explore the wonders that await!",
    "Multi-photon states can be generated through multiple parametric down conversion (PDC) processes where a high-power pump is used on the nonlinear crystal. The higher the population of these states, the stronger the discrepancy with local realistic description. Despite this, the interference contrast in multi-photon PDC experiments can be low when high pumping is applied. We present a method to enhance this contrast using multiport beam splitters, optical devices that can split light into multiple output modes. Our approach acts as a POVM filter and could enable practical CHSH-Bell inequality tests, offering potential applications in reducing communication complexity.",
    "The analysis of the exponents of the transfer matrix spectrum offers crucial insights into the localization lengths of Anderson's model governing a particle's behavior in a disordered potential lattice. By establishing a profound duality identity for determinants and leveraging Jensen's identity for subharmonic functions, I derive a definitive formula that encapsulates the spectrum through the eigenvalues of the Hamiltonian, accentuating the significance of non-Hermitian boundary conditions. This formula, which is absolute in its precision, necessitates an average across a Bloch phase rather than disorder for comprehensive comprehension. Furthermore, a meticulous exploration of non-Hermitian spectrums in the context of Anderson's model in dimensions 1 and 2, focusing on the minimum exponent, is meticulously presented.",
    "In this exciting piece, we level up extreme learning machines for regression challenges by incorporating a cool graph signal processing based tweak. Imagine predicting or regressing a graph signal as your target - that's the cool assumption we're playing with here. By adding in this special regularization, we ensure that the extreme learning machine's output flows seamlessly across the graph. Our simulations with real-world data back this up, showing that this tweak brings a big boost, especially when your training data is limited and noisy.",
    "To provide a comprehensive understanding of heating in weakly collisional turbulent plasmas like the solar wind, it is crucial to consider inter-particle collisions. These collisions play a vital role in transforming ordered energy into heat through irreversible relaxation towards thermal equilibrium. A recent study by Pezzi et al. (Phys. Rev. Lett., vol. 116, 2016, p. 145001) revealed that the collisionality of the plasma is intensified by the presence of intricate structures in velocity space.\n\nExpanding on this analysis, the comparison between the fully nonlinear Landau operator and a linearized Landau operator is conducted to examine their respective impacts. By investigating the relaxation process towards equilibrium of a non-equilibrium distribution function in a force-free plasma, it becomes evident that retaining nonlinearities in the collisional operator is essential for accurately assessing the significance of collisional effects.\n\nAlthough both the nonlinear and linearized operators exhibit characteristic times associated with the dissipation of phase space structures, their influences differ. The study indicates that in the case of the linearized operator, the characteristic times are consistently longer compared to the fully nonlinear operator. This observation suggests that neglecting nonlinearities in the collisional operator results in a slower dissipation of fine velocity structures.",
    "In this research study, we have illustrated the utilization of Mask-RCNN (Regional Convolutional Neural Network), a deep-learning algorithm designed for computer vision and specifically object detection, within the semiconductor defect inspection domain. The identification and classification of stochastic defects during semiconductor manufacturing have become increasingly complex due to the consistent reduction in circuit pattern dimensions, such as pitches below 32 nm. The examination and assessment of defects using state-of-the-art optical and e-beam inspection tools are primarily influenced by rule-based techniques, often leading to misclassification and therefore requiring expert intervention. In this investigation, we have revisited and extended our previous deep learning-based method for defect classification and detection to enhance defect instance segmentation in Scanning Electron Microscopy (SEM) images. This advancement allows for precise delineation of defect extents and the generation of masks for each defect category or instance. Furthermore, it facilitates the extraction and calibration of each segmented mask, enabling pixel quantification within each mask. Consequently, we can accurately count categorical defect instances and compute the surface area in terms of pixels. Our objective is to detect and segment various types of inter-class stochastic defect patterns, such as bridges, breaks, and line collapses, as well as to differentiate between intra-class multi-categorical defect bridge scenarios (e.g., thin/single/multi-line/horizontal/non-horizontal) for aggressive pitches and thin resists (High Numerical Aperture applications). Our proposed methodology demonstrates effectiveness both quantitatively and qualitatively.",
    "We present a new bound for the parameter $\\lambda$ in a distance-regular graph $G$, which enhances and broadens the existing bounds for strongly regular graphs set by Spielman (1996) and Pyber (2014). This new bound plays a crucial role in recent advances concerning the complexity of testing isomorphism in strongly regular graphs as demonstrated by Babai, Chen, Sun, Teng, and Wilmes in 2013. The proof leverages a clique geometry identified by Metsch (1991) while adhering to certain parameter constraints. Additionally, we offer a simplified proof of an asymptotic implication stemming from Metsch's discovery: when $k\\mu = o(\\lambda^2)$, every edge in $G$ belongs to a lone maximal clique of size approximately equal to $\\lambda$, with all other cliques having a size of $o(\\lambda)$. Here, $k$ signifies the degree while $\\mu$ indicates the number of common neighbors between adjacent vertices at a distance of 2. Notably, Metsch's cliques demonstrate an \"asymptotically Delsarte\" nature under the condition $k\\mu = o(\\lambda^2$, implying that groups of distance-regular graphs with parameters satisfying $k\\mu = o(\\lambda^2)$ exhibit an \"asymptotically Delsarte-geometric\" property.",
    "Recent studies indicate that the activity of star formation in galaxies varies depending on their environments. To comprehend the diversity of star formation on a galactic scale, it is essential to investigate the formation and evolution of giant molecular clouds in extreme settings. This study specifically looks into the phenomenon where strongly barred galaxies lack massive stars despite having sufficient molecular gas for star formation. By conducting a hydrodynamical simulation of a strongly barred galaxy using the stellar potential extracted from observations of NGC1300, the researchers compare cloud properties in different galactic regions: bars, bar-ends, and spiral arms. The analysis shows that the average virial parameter of clouds is approximately 1 and is not influenced by the environment, suggesting that the gravitational binding of the cloud is not the cause of the observed absence of massive stars in strong bars. Instead, the focus is on cloud-cloud collisions as a potential trigger for massive star formation, with faster collision speeds noted in bars compared to other regions. The collision frequency is evaluated based on cloud kinematics, indicating that the higher collision speeds in bars may result from the elliptical gas orbits perturbed by the bar potential, leading to random-like motion and fast cloud-cloud collisions. Therefore, it is proposed that the dearth of active star formation in strong bars may be linked to the inefficient formation of massive stars due to fast cloud-cloud collisions driven by vigorous galactic-scale gas movements.",
    "Summary: The relationship between galaxy mass and metallicity is widely studied, but there is still debate over its exact nature. Our goal is to analyze this relationship in the GAMA survey, comparing it to the one from SDSS and exploring how different criteria affect the results. We determine metallicity using emission line ratios and find that the shape of the relationship can vary based on calibration and selection criteria. Despite differences, we find a reasonable agreement between GAMA and SDSS data. It is important to be cautious when comparing results from different studies due to the impact of selection criteria. We also suggest there may be some evolution within the GAMA sample over certain redshift ranges.",
    "We derive equations for seepage velocities of fluid components in two-phase flow in porous media based on thermodynamics. These equations involve a new velocity function called co-moving velocity, which is specific to the porous medium. By incorporating a constitutive relation between velocities and driving forces like pressure gradient, we have a complete set of equations. We analyze four variations of the capillary tube model analytically using this theory and validate it numerically on a network model.",
    "The amazing creativity of nature, shown in the wide variety of life forms and functions on Earth, is a key feature that separates living organisms from non-living entities. This aspect of life is a central focus in the study of artificial life due to its connection with the process of evolution. Researchers have labeled this creative process as Open-Ended Evolution (OEE). This article introduces the second of two special issues on current research in OEE, providing an overview of the content. Most of the work discussed was presented at an open-ended evolution workshop during the 2018 Conference on Artificial Life in Tokyo, with previous workshops held in Cancun and York. The article offers a simplified categorization of OEE and a summary of the progress in the field represented in this special issue.",
    "We examined the properties of MgO/Ag(001) ultrathin films with substitutional Mg atoms at the interface layer through Auger electron diffraction experiments, ultraviolet photoemission spectroscopy, and density functional theory (DFT) calculations. Taking advantage of the layer-by-layer details in the Mg KL_23 L_23 Auger spectra and employing multiple scattering calculations, we initially identified interlayer distances and morphological parameters of the MgO/Ag(001) system with and without Mg atoms integrated at the interface. Our findings indicate that the incorporation of Mg atoms leads to a significant distortion of the interface layers, resulting in a noteworthy reduction of the work function (0.5 eV) due to variations in the band offsets at the interface. These experimental results align closely with our DFT simulations, which accurately reproduce the lattice distortion induced by the Mg atoms at the interface. Furthermore, a Bader analysis reveals that an increase in the Mg concentration at the interface triggers an electron transfer from Mg to Ag atoms within the metallic interface layer. We observe that while the local lattice distortion arises from the interactions between the ions and neighboring atoms, its impact on the work function reduction remains limited. Lastly, we analyze the work function changes caused by the interface Mg atoms in terms of charge transfer, rumpling, and electrostatic compression effects. Our assessment underscores that the alterations in the work function of the metal/oxide system with interface Mg atoms predominantly stem from the rise in the electrostatic compression effect.",
    "This passage discusses the importance of monitoring industrial processes to detect changes in process parameters promptly and correct any arising problems. A challenge arises when the measured value falls below the sensitivity or detection limits, leading to incomplete observations or left-censored data. Traditional monitoring methods are not suitable when there is a high level of censorship, exceeding 70%. Therefore, proper statistical techniques are necessary to assess the process's actual state. The paper suggests a method to estimate process parameters in such situations and introduces a corresponding control chart algorithm.",
    "This paper introduces Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC optimizes a clustering objective by mapping data to a lower-dimensional feature space. Experimental evaluations on image and text corpora demonstrate significant improvements over existing methods.",
    "We enhance a study that focused on how the peak transmission rate affects network burstiness by Sarvotham et al. [2005]. We group TCP packets into sessions using a 5-tuple (source, destination, total payload, duration, average transmission rate, peak transmission rate, initiation time). Our new definition of peak rate is introduced after careful analysis. In contrast to Sarvotham et al. [2005], who categorized sessions into alpha and beta groups, we divide them into 10 sessions based on empirical quantiles of the peak rate variable, showing that the beta group is not homogeneous. This more refined division uncovers additional structures that were not discerned by the grouping into two segments. We analyze the relationships between total payload, duration, and average transmission rate within each segment and note variations across the groups. Additionally, we observe that session initiation times in each segment follow a Poisson process, which is not the case for the dataset as a whole. Our findings highlight the importance of the peak rate level in understanding network structure and in creating accurate data simulations. Finally, we present a straightforward method for simulating network traffic based on our research.",
    "The Brouwer fixed-point theorem in topology implies that any continuous mapping $f$ from a compact convex set to itself has a fixed point, where $f(x_0) = x_0$. Under specific circumstances, this fixed point is associated with the throat of a traversable wormhole, represented by $b(r_0) = r_0$ for the shape function $b = b(r)$. Therefore, the potential presence of wormholes can be inferred solely from mathematical principles without exceeding current physical constraints.",
    "Convolutional Neural Networks (CNNs) have become increasingly popular in the fields of computer vision and medical image analysis. However, while most CNN approaches can only handle 2D images, the majority of medical data in clinical practice is in the form of 3D volumes. In our innovative study, we present a cutting-edge solution for 3D image segmentation using a fully convolutional neural network tailored for volumetric data. Focusing on MRI volumes of the prostate, our CNN is trained end-to-end to predict segmentations for entire volumes all at once. We've devised a novel objective function based on the Dice coefficient to address imbalances between foreground and background voxels. To overcome the challenge of limited annotated training data, we have implemented data augmentation techniques like non-linear transformations and histogram matching. Our experimental results demonstrate that our approach achieves impressive performance on complex test data, all while significantly reducing processing time compared to existing methods.",
    "Certainly! The spectrum of a non-relativistic two-body system interacting via the Coulomb potential manifests as the Balmer series $E_n=\\frac{\\alpha^2m}{4n^2}$ according to the Schr\\\"odinger equation. Wick and Cutkosky's 1954 research, within the Bethe-Salpeter equation framework, revealed that relativistic effects introduce new energy levels when $\\alpha>\\frac{\\pi}{4}$, supplementing the Balmer series. Nevertheless, the interpretation of these additional states remained ambiguous, leading to doubts about their existence. Our recent findings indicate that these extra states are predominantly influenced by the exchange (massless) particles moving at the speed of light. This characteristic explains why they were not accounted for in the non-relativistic (Schr\\\"odinger) framework.",
    "We delve into the intriguing world of quantum mechanics to explore the essence of quantum f-relative entropy, where f(.) represents an operator convex function. The quest leads us to unveil novel insights, shedding light on the equality conditions imbued with notions of monotonicity and joint convexity that transcend the known boundaries. We break new ground by discovering that these conditions resonate across a spectrum of operator convex functions, setting the stage for a fascinating journey of discovery.\n\nThe quantum f-entropy emerges as a cornerstone of our exploration, forged in the crucible of quantum f-relative entropy. We unravel its intricate properties, unveiling the subtle interplay of equality conditions in select scenarios. As our expedition unfolds, we trace the evolution of f-generalizations, such as the enigmatic Holevo information, entanglement-assisted capacity, and coherent information. Their journey culminates in an elegant dance of data processing inequality, where we discern the echoes of perfection in the realm of f-coherent information.",
    "Modifying the position or angle of an image should not impact the outcomes of various computer vision tasks. While Convolutional Neural Networks (CNNs) are inherently invariant to image translations due to the way input images are processed into feature maps, the same cannot be said for image rotations. Achieving overall rotational invariance is typically done through data augmentation. However, establishing invariance to patch-wise rotations is more challenging. In this regard, we introduce Harmonic Networks (H-Nets), a type of CNN specifically designed to be equivariant to both patch-wise translations and full 360-degree rotations. This is accomplished by utilizing circular harmonic filters instead of standard CNN filters, resulting in maximal responses and orientations for every local patch in the input image. H-Nets offer a robust representation that is efficient in terms of parameters and computational complexity, with evidence suggesting that the network's deep feature maps encompass intricate rotational properties. Our framework is versatile and can be integrated successfully with cutting-edge architectural designs and methodologies like deep supervision and batch normalization. Our experiments demonstrate that by leveraging H-Nets, we achieve top-tier performance in classifying rotated-MNIST images and competitive outcomes across several standardized tests.",
    "We study and analyze reflection spectra of waveguides and cavities that are directly connected. The Fano lines we observe provide valuable insights into the reflection and coupling mechanisms. Unlike with side-coupled systems, the shape of the observed Fano line is not determined by the waveguide ends, but by the coupling process between the measurement device fiber and the waveguide. Our experimental findings and analytical model indicate that the Fano parameter governing the Fano line shape is highly influenced by the coupling state. Even a small shift of the fiber, well below the Rayleigh range, can dramatically alter the shape of the Fano line.",
    "We introduce a new technique to measure atmospheric turbulence in optical and infrared telescopes using short-exposure images of a star field. By calculating differential motion between pairs of star images, we can determine wavefront tilt's structure functions for various angular separations. This method, compared with theoretical turbulence predictions using a Markov-Chain Monte-Carlo optimization, estimates the lower atmospheric turbulence profile, total seeing, free-atmosphere seeing, and outer scale. We validate the technique with Monte-Carlo simulations and demonstrate examples from data collected by the second AST3 telescope at Dome A in Antarctica.",
    "An n-plectic structure is defined as a commutative and torsionless Lie Rinehart pair, accompanied by a distinguished cocycle from its Chevalley-Eilenberg complex. This 'n-plectic cocycle' paves the way for extending the Chevalley-Eilenberg complex with symplectic tensors. The cohomology of this extension expands the scope of Hamiltonian functions and vector fields to tensors and cotensors across various degrees, accounting for specific coboundaries, and taking the form of a Lie -algebra. Moreover, we establish that momentum maps manifest in this framework through weak Lie -morphisms from any Lie -algebra to the Lie -algebra of Hamiltonian (co)tensors.",
    "Amorphous solids or glasses are known to display stretched-exponential decay across wide time scales in various measurable properties such as intermediate scattering function, dielectric relaxation modulus, and time-elastic modulus. This behavior is particularly noticeable close to the glass transition. In this communication, we illustrate the connection between stretched-exponential relaxation and the distinctive lattice dynamics of glasses using the dielectric relaxation as an example. By revising the Lorentz model for dielectric materials in a more comprehensive manner, we present the dielectric response as a function of the vibrational density of states (DOS) for a random collection of harmonically interacting spherical particles with their nearest neighbors. Remarkably, we observe that in proximity to the glass transition of this system (coinciding with the Maxwell rigidity transition), the dielectric relaxation aligns perfectly with stretched-exponential behavior, characterized by Kohlrausch exponents in the range of $0.56 < \\beta < 0.65$, which is consistent with measurements in numerous experimental setups. Importantly, we identify the underlying soft modes (boson-peak) in the DOS as the fundamental cause of stretched-exponential relaxation.",
    "This paper aims to demonstrate the challenges of achieving representation disentanglement in text using unsupervised methods. We analyze a selection of successful models from the image domain across 6 disentanglement metrics, classification tasks, and homotopy. Our evaluation uses two synthetic datasets with known generative factors to showcase the disparity in the text domain and how factors like representation sparsity and decoder coupling can influence disentanglement. This study serves as the initial exploration of unsupervised representation disentanglement in text and presents a foundation for future research in this area.",
    "This paper introduces a hybrid quantum-classical algorithm for addressing the unit commitment (UC) problem in power systems. The algorithm decomposes the UC problem into three subproblems - a quadratic subproblem, a quadratic unconstrained binary optimization (QUBO) subproblem, and an unconstrained quadratic subproblem. A classical optimization solver handles the first and third subproblems, while the QUBO subproblem is solved with the quantum approximate optimization algorithm (QAOA). The subproblems are coordinated iteratively using a three-block alternating direction method of multipliers algorithm. Simulation results using Qiskit on the IBM Q system confirm the effectiveness of the algorithm in solving the UC problem.",
    "The discovery of numerous very low amplitude modes in Delta Sct stars has been attributed to improved signal-to-noise ratios. CoRoT, a space mission by CNES, aims to uncover this hidden treasure inaccessible from the ground. This study focuses on HD 50844, analyzing 140,016 data points using various methods and verification steps. Achieving an amplitude spectrum level of 10^{-5} mag in the CoRoT dataset, the frequency analysis reveals a multitude of terms between 0 and 30 d^{-1}. Confirming the rich frequency content of Delta Sct stars, spectroscopic mode identification supports this, detecting high-degree modes up to ell=14. Cancellation effects at the noise level of CoRoT measurements do not fully eliminate flux variations associated with these modes. Ground-based observations indicate that HD 50844, an evolved star slightly deficient in heavy elements, lies on the Terminal Age Main Sequence. The frequency distribution lacks a clear regular pattern, possibly due to its evolutionary stage. The dominant term (f_1=6.92 d^{-1}) is identified as the fundamental radial mode using ground-based photometric and spectroscopic data. This study also incorporates observations from ESO telescopes under the ESO Large Programme LP178.D-0361 and data from the Observatorio de Sierra Nevada, Observatorio Astronomico Nacional San Pedro Martir, and Konkoly Observatory's Piszkesteto Mountain Station.",
    "The captivating article explores the mysterious world of star formation in the enchanting regions of S231-S235 within a colossal molecular cloud named G174+2.5. Our adventure began by combing through archive data to identify massive molecular clumps, from which we meticulously extracted key details such as mass, size, and CO column density. With our targets in sight, we set out to capture groundbreaking observations. Excitingly, our exploration led us to the first-ever sighting of ammonia and cyanoacetylene lines in the molecular clumps WB89 673 and WB89 668, hinting at the presence of high-density gas. Delving deeper into the molecular mysteries, we unraveled essential physical properties of the gas, revealing a temperature range of 16-30 K and a hydrogen number density varying from 2.8 to 7.2$\\times10^3$ cm$^{-3}$. Notably, a thrilling discovery awaited us as we detected a shock-tracing line of the CH$_3$OH molecule at 36.2 GHz, casting new light on the enigmatic WB89 673 clump.",
    "We present new observations of gamma-ray burst (GRB) 171205A using the upgraded Giant Metrewave Radio Telescope (uGMRT), covering a frequency range from 250 to 1450 MHz over a period of 4 to 937 days. This is the first GRB afterglow detected in the 250-500 MHz range and the second brightest GRB observed with the uGMRT. Despite being visible for nearly 1000 days, there is no sign of a transition to a non-relativistic regime. We analyze archival Chandra X-ray data at around day 70 and day 200, finding no evidence of a jet break. Our modeling of the synchrotron afterglow emission suggests a relativistic, isotropic, self-similar deceleration process and a shock-breakout from a wide-angle cocoon. Our findings indicate that the GRB occurred in a stratified wind-like medium rather than a standard constant density environment. The combined data suggest that the radio afterglow contains contributions from both a weak, slightly off-axis jet and a wider cocoon, with the cocoon likely dominating initially and the jet taking over later, leading to flatter radio light curves.",
    "The newly established concept of quasi-Lie schemes has been the subject of thorough examination and practical application in delving into various equations of Emden type. A structured methodology has been developed to specifically address these equations and their extensions. Through this approach, we are able to discern time-dependent constants of motion associated with specific cases of Emden equations utilizing their corresponding solutions. This exploration has uncovered previously documented findings from a fresh vantage point. Moreover, time-dependent constants of motion for Emden-type equations meeting specific criteria have been revealed through this comprehensive analysis.",
    "We investigate the charged Higgs bosons as proposed in the model based on gauge symmetry $SU(3)_c \\otimes SU(3)_L \\otimes U(1)_X$. By analyzing Yukawa mixing couplings at both small ($\\sim$ GeV) and large ($\\sim$ TeV) scales, we demonstrate that the model predicts hypercharge-one $H_1^{\\pm}$ and hypercharge-two $H_2^{\\pm}$ Higgs bosons, which can be simultaneously generated in $pp$ collisions with varying production rates. At lower energy levels, the properties of $H_1^{\\pm}$ closely resemble those of charged Higgs bosons in a two Higgs doublet model (2HDM), while $H_2^{\\pm}$ are additional like-charged Higgs bosons arising from the underlying 3-3-1 model. Identifying multiple like-charged Higgs boson resonances could serve as a test of theoretical models against experimental observations. We explore the production of $H_{1,2}^{\\pm}$ pairs and associated $tbH_{1,2}^{\\pm}$ particles at the CERN LHC collider. Notably, we find that pair production rates can be comparable to single production rates in gluon-gluon collisions due to the involvement of a heavy neutral $Z'$ gauge boson predicted by the model. When considering decays to leptons such as $H_{1,2}^{\\pm} \\rightarrow \\tau \\nu_{\\tau}$, we identify scenarios where small peaks of $H_{2}^{\\pm}$-boson events in transverse mass distributions stand out above the $H_{1}^{\\pm}$ background.",
    "Isospin breaking in the $K_{\\ell 4}$ form factors, caused by the difference between charged and neutral pion masses, is discussed in a framework that uses dispersion representations. The $K_{\\ell 4}$ form factors are developed iteratively up to two loops in a low-energy expansion, incorporating analyticity, crossing, and unitarity with two-meson intermediate states. We present analytical expressions for the phases of the two-loop form factors in the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ channel, which help to link experimental measurements of form-factor phase shifts outside the isospin limit with theoretical studies on $S$- and $P$-wave $\\pi\\pi$ phase shifts within the isospin limit. We study the dependence on the two $S$-wave scattering lengths $a_0^0$ and $a_0^2$ in a general manner, different from previous analyses based on one-loop chiral perturbation theory. By revisiting results from the NA48/2 collaboration at CERN SPS, we reanalyze the phases of $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ form factors to extract values for the scattering lengths $a_0^0$ and $a_0^2, taking into account isospin-breaking corrections.",
    "In this article, we build upon and expand the findings of previous studies in \\cite{1,2,3,4}, with a particular focus on the work in \\cite{4}, to analyze the statistical characteristics of the cosmological constant in a cosmological de Sitter universe in relation to massless excitations with Planckian effects. Our analysis reveals that in a classical context, a positive cosmological constant $\\Lambda>0$ can only be achieved as the temperature $T$ approaches zero. Similar to the scenario of black holes, when accounting for quantum effects, a formulation for $\\Lambda$ can be established using massless excitations, with quantum corrections to the Misner-Sharp mass playing a crucial role. Furthermore, quantum fluctuations lead to the emergence of an effective cosmological constant that varies depending on the scale being observed, offering a potential resolution to the cosmological constant dilemma without requiring the introduction of a quintessence field. The modest value of $\\Lambda$ could be attributed to the presence of a quantum decoherence scale beyond the Planck length, which results in the evolution of spacetime resembling a pure de Sitter universe with a small average cosmological constant inherent in the system's lowest energy state.",
    "Key points:\n\n- Investigation of zero and finite temperature properties of the one-dimensional spin-glass model for vector spins with an infinite number of spin components.\n- Interactions decay with a power, sigma, of the distance.\n- Diluted version of the model deviates significantly from the fully connected model.\n- Defect energies at zero temperature determined by comparing ground-state energies with periodic and antiperiodic boundary conditions.\n- Dependence of the defect-energy exponent theta on sigma: theta = 3/4 - sigma.\n- Upper critical value of sigma is 3/4, corresponding to the lower critical dimension in the short-range model.\n- For finite temperatures, solving large m saddle-point equations self-consistently provides correlation function, order parameter, and spin-glass susceptibility.\n- Different forms of finite-size scaling effects below and above the lower critical value, sigma = 5/8, corresponding to the upper critical dimension 8 of the short-range model.",
    "The rare Of^+ supergiants occupy a unique position between standard O-stars and Wolf-Rayet stars. Recent discoveries reveal striking parallels with WN-type objects, particularly in the visible and near-infrared spectra, hinting at shared stellar wind characteristics. Our groundbreaking study unveils the inaugural X-ray observations of HD16691 (O4If^+) and HD14947 (O5f^+), which showcase a gentle thermal profile aligning with the anticipated X-ray output of a single O-type star. Nonetheless, the X-ray brightness of these stars falls slightly below expectations for solitary O-type stars, pointing to the profound influence of their peculiar stellar wind properties on X-ray emissions as they transition towards the WN category. We propose that the diminished X-ray radiance of HD16691 and HD14947 reflects their distinctive status between O and WR stars, brought about by heightened wind density.",
    "The goal of the AARTFAAC project is to deploy an All-Sky Monitor (ASM) utilizing the Low Frequency Array (LOFAR) telescope. This will facilitate continuous monitoring of low frequency radio transients across a large portion of the sky visible to the LOFAR telescope, with real-time capabilities spanning from milliseconds to several days. Furthermore, it will allow for immediate follow-up observations by the complete LOFAR system upon detection of possible transient events. Implementing this system comes with various challenges, including the need for all-sky imaging, low processing latencies, sustained availability, and independent operation of the ASM. Notably, the correlator for the ASM is currently the world's largest in terms of input channel count, producing around 1.5 x 10^5 correlations per second per spectral channel. Initial tests were conducted using existing LOFAR resources to establish essential instrumental design parameters for the ASM. This paper provides an outline of the AARTFAAC data processing pipeline and showcases some of the challenges encountered through all-sky images captured during test observations, offering insightful metrics on the instrument's capabilities.",
    "Wolf-Rayet (WR) stars are matured forms of massive O-type stars and are considered as potential sources for Type Ib/c core-collapse supernovae (SNe). We present findings from our study on Wolf-Rayet stars in M101, focusing on the effectiveness of narrow-band optical imaging compared to broad-band techniques. Our results reveal that, on average, 42% of WR stars, increasing to approximately 85% in central areas, can only be identified through narrow-band imaging. Therefore, the absence of a WR star in the vicinity of around 10 Type Ib/c SNe in broad-band imaging is no longer conclusive evidence against a WR star being the progenitor source."
  ]
}