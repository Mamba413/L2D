{
  "original": [
    "Process calculi based on logic, such as $\\pi$DILL and CP, provide a foundation for deadlock-free concurrent programming. However, in previous work, there is a mismatch between the rules for constructing proofs and the term constructors of the $\\pi$-calculus: the fundamental operator for parallel composition does not correspond to any rule of linear logic. Kokke et al. (2019) introduced Hypersequent Classical Processes (HCP), which addresses this mismatch using hypersequents (collections of sequents) to register parallelism in the typing judgements. However, the step from CP to HCP is a big one. As of yet, HCP does not have reduction semantics, and the addition of delayed actions means that CP processes interpreted as HCP processes do not behave as they would in CP. We introduce HCP-, a variant of HCP with reduction semantics and without delayed actions. We prove progress, preservation, and termination, and show that HCP- supports the same communication protocols as CP.",
    "A simple variant of the BDDC preconditioner in which constraints are imposed on a selected set of subobjects (subdomain subedges, subfaces and vertices between pairs of subedges) is presented. We are able to show that the condition number of the preconditioner is bounded by $C \\big(1+\\log (L/h)\\big)^2$, where $C$ is a constant, and $h$ and $L$ are the characteristic sizes of the mesh and the subobjects, respectively. As $L$ can be chosen almost freely, the condition number can theoretically be as small as $O(1)$. We will discuss the pros and cons of the preconditioner and its application to heterogeneous problems. Numerical results on supercomputers are provided.",
    "We give examples of where the Heun function exists as solutions of wave equations encountered in general relativity. While the Dirac equation written in the background of Nutku helicoid metric yields Mathieu functions as its solutions in four spacetime dimensions, the trivial generalization to five dimensions results in the double confluent Heun function. We reduce this solution to the Mathieu function with some transformations. We must apply Atiyah-Patodi-Singer spectral boundary conditions to this system since the metric has a singularity at the origin.",
    "As it was shown by many authors, a slow decrease in X-rays observed during the decay phase of long duration flares (LDE) can be explained only by a magnetic reconnection and energy release ceaselessly ongoing in the coronal part of a flare. Using RHESSI data we try to answer two following questions. How effective are these processes at the LDEs decay phase and how can precisely the energy release rate be calculated based on these data? To answer the questions images of the selected LDEs during their decay phase were reconstructed. Physical parameters of flare coronal sources obtained from image spectral analysis allowed us to study the efficiency of the energy release process. We also examined terms included in the energy equation to find out what is the accuracy of determination of each term.",
    "By means of a multi-scale analysis we describe the typical geometrical structure of the clusters under the FK measure in random media. Our result holds in any dimension greater or equal to 2 provided that slab percolation occurs under the averaged measure, which should be the case in the whole supercritical phase. This work extends the one of Pisztora and provides an essential tool for the analysis of the supercritical regime in disordered FK models and in the corresponding disordered Ising and Potts models.",
    "Photospheric absorption lines in classical T Tauri stars (CTTS) are weak compared to normal stars. This so-called veiling is normally identified with an excess continuous emission formed in shock-heated gas at the stellar surface below the accretion streams. We have selected four stars (RW Aur A, RU Lup, S CrA NW and S CrA SE) with unusually strong veiling to make a detailed investigation of veiling versus stellar brightness and emission line strengths for comparisons to standard accretion models. We have monitored the stars photometrically and spectroscopically at several epochs. In standard accretion models a variable accretion rate will lead to a variable excess emission. Consequently, the stellar brightness should vary accordingly. We find that the veiling of absorption lines in these stars is strongly variable and usually so large that it would require the release of several stellar luminosities of potential energy. At states of very large line dilution, the correspondingly large veiling factors derived correlate only weakly with brightness. Moreover, the emission line strengths violate the expected trend of veiling versus line strength. The veiling can change dramatically in one night, and is not correlated with the phase of the rotation periods found for two stars. We show that in at least three of the stars, when the veiling becomes high, the photospheric lines become filled-in by line emission, which produces large veiling factors unrelated to changes in any continuous emission from shocked regions. We also consider to what extent extinction by dust and electron scattering in the accretion stream may affect veiling measures in CTTS. We conclude that the degree of veiling cannot be used as a measure of accretion rates in CTTS with rich emission line spectra.",
    "Giant low surface brightness (GLSB) galaxies are commonly thought to be massive, dark matter dominated systems. However, this conclusion is based on highly uncertain rotation curves. We present here a new study of two prototypical GLSB galaxies: Malin 1 and NGC 7589. We re-analysed existing HI observations and derived new rotation curves, which were used to investigate the distributions of luminous and dark matter in these galaxies. In contrast to previous findings, the rotation curves of both galaxies show a steep rise in the central parts, typical of high surface brightness (HSB) systems. Mass decompositions with a dark matter halo show that baryons may dominate the dynamics of the inner regions. Indeed, a \"maximum disk\" fit gives stellar mass-to-light ratios in the range of values typically found for HSB galaxies. These results, together with other recent studies, suggest that GLSB galaxies are systems with a double structure: an inner HSB early-type spiral galaxy and an outer extended LSB disk. We also tested the predictions of MOND: the rotation curve of NGC 7589 is reproduced well, whereas Malin 1 represents a challenging test for the theory.",
    "The multiplicity distribution, multiplicity moment, scaled variance, entropy and reduced entropy of target evaporated fragment emitted in forward and backward hemispheres in 12 A GeV $^{4}$He, 3.7 A GeV $^{16}$O, 60 A GeV $^{16}$O, 1.7 A GeV $^{84}$Kr and 10.7 A GeV $^{197}$Au induced emulsion heavy targets (AgBr) interactions are investigated. It is found that the multiplicity distribution of target evaporated fragments emitted in forward and backward hemispheres can be fitted by a Gaussian distribution. The multiplicity moments of target evaporated particles emitted in forward and backward hemispheres increase with the order of the moment {\\em q}, and second-order multiplicity moment is energy independent over the entire energy for all the interactions in the forward and backward hemisphere respectively. The scaled variance, a direct measure of multiplicity fluctuations, is close to one for all the interactions which may be said that there is a feeble correlation among the produced particles. The entropy of target evaporated fragments emitted in forward and backward hemispheres are the same within experimental errors, respectively.",
    "We investigate theoretically the temporal behavior of a quantum dot under off-resonant optical excitation targeted at fast acoustic phonon-assisted state preparation. We demonstrate that in a preparation process driven by short laser pulses three processes can be identified: a dressing of the states during the switch on of the laser pulse, a subsequent phonon-induced relaxation and an undressing at the end of the pulse. By analyzing excitation scenarios with different pulse shapes we highlight the decisive impact of an adiabatic undressing on the final state in short pulse protocols. Furthermore, we show that in exciton-biexciton systems the laser characteristics such as the pulse detuning and the pulse length as well as the biexciton binding energy can be used to select the targeted quantum dot state.",
    "In the quantum mechanical Hilbert space formalism, the probabilistic interpretation is a later ad-hoc add-on, more or less enforced by the experimental evidence, but not motivated by the mathematical model itself. A model involving a clear probabilistic interpretation from the very beginning is provided by the quantum logics with unique conditional probabilities. It includes the projection lattices in von Neumann algebras and here probability conditionalization becomes identical with the state transition of the Lueders - von Neumann measurement process. This motivates the definition of a hierarchy of five compatibility and comeasurability levels in the abstract setting of the quantum logics with unique conditional probabilities. Their meanings are: the absence of quantum interference or influence, the existence of a joint distribution, simultaneous measurability, and the independence of the final state after two successive measurements from the sequential order of these two measurements. A further level means that two elements of the quantum logic (events) belong to the same Boolean subalgebra. In the general case, the five compatibility and comeasurability levels appear to differ, but they all coincide in the common Hilbert space formalism of quantum mechanics, in von Neumann algebras, and in some other cases.",
    "An analysis is presented of wave-vector dispersion in elliptically birefringent stratified magneto-optic media having one-dimensional periodicity. It is found that local normal-mode polarization-state differences between adjacent layers lead to mode coupling and impact the wave-vector dispersion and the character of the Bloch states of the system. This coupling produces extra terms in the dispersion relation not present in uniform circularly birefringent magneto-optic stratified media. Normal mode coupling lifts the degeneracy at frequency band cross-over points under certain conditions and induces a magnetization-dependent optical band gap. This study examines the conditions for band gap formation in the system. It shows that such a frequency-split can be characterized by a simple coupling parameter that depends on the relation between polarization states of local normal modes in adjacent layers. The character of the Bloch states and conditions for maximizing the strength of the band splitting in these systems are analyzed.",
    "We study a natural extension of classical empirical risk minimization, where the hypothesis space is a random subspace of a given space. In particular, we consider possibly data dependent subspaces spanned by a random subset of the data, recovering as a special case Nystr\\\"om approaches for kernel methods. Considering random subspaces naturally leads to computational savings, but the question is whether the corresponding learning accuracy is degraded. These statistical-computational tradeoffs have been recently explored for the least squares loss and self-concordant loss functions, such as the logistic loss. Here, we work to extend these results to convex Lipschitz loss functions, that might not be smooth, such as the hinge loss used in support vector machines. This extension requires developing new proofs, that use different technical tools. Our main results show the existence of different settings, depending on how hard the learning problem is, for which computational efficiency can be improved with no loss in performance. Theoretical results are illustrated with simple numerical experiments.",
    "The notion of patient's consent plays a major role in granting access to medical data. In typical healthcare systems, consent is captured by a form that the patient has to fill in and sign. In e-Health systems, the paper-form consent is being replaced by the integration of the notion of consent in the mechanisms that regulate the access to the medical data. This helps in empowering the patient with the capability of granting and revoking consent in a more effective manner. However, the process of granting and revoking consent greatly varies according to the situation in which the patient is. Our main argument is that such a level of detail is very difficult and error-prone to capture as a set of authorisation policies. In this paper, we present ACTORS, a goal-driven approach to manage consent. The main idea behind ACTORS is to leverage the goal-driven approach of Teleo-Reactive (TR) programming for managing consent that takes into account changes regarding the domains and contexts in which the patient is providing her consent.",
    "This paper is concerned with the mathematical analysis of the inverse random source problem for the time fractional diffusion equation, where the source is assumed to be driven by a fractional Brownian motion. Given the random source, the direct problem is to study the stochastic time fractional diffusion equation. The inverse problem is to determine the statistical properties of the source from the expectation and variance of the final time data. For the direct problem, we show that it is well-posed and has a unique mild solution under a certain condition. For the inverse problem, the uniqueness is proved and the instability is characterized. The major ingredients of the analysis are based on the properties of the Mittag--Leffler function and the stochastic integrals associated with the fractional Brownian motion.",
    "Manifold learning methods play a prominent role in nonlinear dimensionality reduction and other tasks involving high-dimensional data sets with low intrinsic dimensionality. Many of these methods are graph-based: they associate a vertex with each data point and a weighted edge with each pair. Existing theory shows that the Laplacian matrix of the graph converges to the Laplace-Beltrami operator of the data manifold, under the assumption that the pairwise affinities are based on the Euclidean norm. In this paper, we determine the limiting differential operator for graph Laplacians constructed using $\\textit{any}$ norm. Our proof involves an interplay between the second fundamental form of the manifold and the convex geometry of the given norm's unit ball. To demonstrate the potential benefits of non-Euclidean norms in manifold learning, we consider the task of mapping the motion of large molecules with continuous variability. In a numerical simulation we show that a modified Laplacian eigenmaps algorithm, based on the Earthmover's distance, outperforms the classic Euclidean Laplacian eigenmaps, both in terms of computational cost and the sample size needed to recover the intrinsic geometry.",
    "We present an efficient integral equation approach to solve the heat equation, $u_t (\\x) - \\Delta u(\\x) = F(\\x,t)$, in a two-dimensional, multiply connected domain, and with Dirichlet boundary conditions. Instead of using integral equations based on the heat kernel, we take the approach of discretizing in time, first. This leads to a non-homogeneous modified Helmholtz equation that is solved at each time step. The solution to this equation is formulated as a volume potential plus a double layer potential.The volume potential is evaluated using a fast multipole-accelerated solver. The boundary conditions are then satisfied by solving an integral equation for the homogeneous modified Helmholtz equation. The integral equation solver is also accelerated by the fast multipole method (FMM). For a total of $N$ points in the discretization of the boundary and the domain, the total computational cost per time step is $O(N)$ or $O(N\\log N)$.",
    "We discuss a scheme in which sequential state-discrimination measurements are performed on qudits to determine the quantum state in which they were initially prepared. The qudits belong to a set of nonorthogonal quantum states and hence cannot be distinguished with certainty. Unambiguous state discrimination allows error-free measurements at the expense of occasionally failing to give a conclusive answer about the state of the qudit. Qudits have the potential to carry more information per transmission than qubits. We considered the situation in which Alice sends one of N qudits, where the dimension of the qudits is also N. We look at two cases, one in which the states all have the same overlap and one in which the qudits are divided into two sets, with qudits in different sets having different overlaps. We also study the robustness of our scheme against a simple eavesdropping attack and found that by using qudits rather than qubits, there is a greater probability that an eavesdropper will introduce errors and be detected.",
    "This work aims to provide a more secure access control in Hyperledger Fabric blockchain by combining multiple ID's, attributes, and policies with the components that regulate access control. The access control system currently used by Hyperledger Fabric is first completely analyzed. Next, a new implementation is proposed that builds upon the existing solution but provides users and developers with easier ways to make access control decisions based on combinations of multiple ID's, attributes, and policies. Our proposed implementation encapsulates the Fabric CA client to facilitate attribute addition and simplify the process of registering and enrolling a newly created certificate (corresponding to a new user). This research, concludes that it is possible to combine multiple ID's, attributes, and policies with the help of Hyperledger Fabric's smart contract technology. Furthermore, it could be seen that the performance impact for real-world applications is negligible compared to the insecure case of always providing access to a resource without performing access control.",
    "This work introduces pyramidal convolution (PyConv), which is capable of processing the input at multiple filter scales. PyConv contains a pyramid of kernels, where each level involves different types of filters with varying size and depth, which are able to capture different levels of details in the scene. On top of these improved recognition capabilities, PyConv is also efficient and, with our formulation, it does not increase the computational cost and parameters compared to standard convolution. Moreover, it is very flexible and extensible, providing a large space of potential network architectures for different applications. PyConv has the potential to impact nearly every computer vision task and, in this work, we present different architectures based on PyConv for four main tasks on visual recognition: image classification, video action classification/recognition, object detection and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baselines. For instance, on image recognition, our 50-layers network outperforms in terms of recognition performance on ImageNet dataset its counterpart baseline ResNet with 152 layers, while having 2.39 times less parameters, 2.52 times lower computational complexity and more than 3 times less layers. On image segmentation, our novel framework sets a new state-of-the-art on the challenging ADE20K benchmark for scene parsing. Code is available at: https://github.com/iduta/pyconv",
    "The status of the solar axion search with the CERN Axion Solar Telescope (CAST) will be discussed. Results from the first part of CAST phase II where the magnet bores were filled with 4He gas at variable pressure in order to scan axion masses up to 0.4 eV will be presented. From the absence of excess X-rays when the magnet was pointing to the Sun, we set a typical upper limit on the axion-photon coupling of g < 2.17 x 10^10 GeV$-1 at 95% CL for axion masses lower than 0.4 eV, the exact result depending on the pressure setting. Our search for axions with masses up to about 1.2 eV using 3He as a buffer gas is in progress in the second part of CAST phase II. Expectations for sensibilities will be given. Near future perspectives as well as more long term options for a new helioscope experiment will be evoked.",
    "Observations indicate that the Arctic sea ice cover is rapidly retreating while the Antarctic sea ice cover is steadily expanding. State-of-the-art climate models, by contrast, typically simulate a moderate decrease in both the Arctic and Antarctic sea ice covers. However, in each hemisphere there is a small subset of model simulations that have sea ice trends similar to the observations. Based on this, a number of recent studies have suggested that the models are consistent with the observations in each hemisphere when simulated internal climate variability is taken into account. Here we examine sea ice changes during 1979-2013 in simulations from the most recent Coupled Model Intercomparison Project (CMIP5) as well as the Community Earth System Model Large Ensemble (CESM-LE), drawing on previous work that found a close relationship in climate models between global-mean surface temperature and sea ice extent. We find that all of the simulations with 1979-2013 Arctic sea ice retreat as fast as observed have considerably more global warming than observations during this time period. Using two separate methods to estimate the sea ice retreat that would occur under the observed level of global warming in each simulation in both ensembles, we find that simulated Arctic sea ice retreat as fast as observed would occur less than 1% of the time. This implies that the models are not consistent with the observations. In the Antarctic, we find that simulated sea ice expansion as fast as observed typically corresponds with too little global warming, although these results are more equivocal. We show that because of this, the simulations do not capture the observed asymmetry between Arctic and Antarctic sea ice trends. This suggests that the models may be getting the right sea ice trends for the wrong reasons in both polar regions.",
    "Bio-features are fast becoming a key tool to authenticate the IoT devices; in this sense, the purpose of this investigation is to summaries the factors that hinder biometrics models' development and deployment on a large scale, including human physiological (e.g., face, eyes, fingerprints-palm, or electrocardiogram) and behavioral features (e.g., signature, voice, gait, or keystroke). The different machine learning and data mining methods used by authentication and authorization schemes for mobile IoT devices are provided. Threat models and countermeasures used by biometrics-based authentication schemes for mobile IoT devices are also presented. More specifically, We analyze the state of the art of the existing biometric-based authentication schemes for IoT devices. Based on the current taxonomy, We conclude our paper with different types of challenges for future research efforts in biometrics-based authentication schemes for IoT devices.",
    "Device fingerprinting over the web has received much attention both by the research community and the commercial market a like. Almost all the fingerprinting features proposed to date depend on software run on the device. All of these features can be changed by the user, thereby thwarting the device's fingerprint. In this position paper we argue that the recent emergence of the HTML5 standard gives rise to a new class of fingerprinting features that are based on the \\emph{hardware} of the device. Such features are much harder to mask or change thus provide a higher degree of confidence in the fingerprint. We propose several possible fingerprint methods that allow a HTML5 web application to identify a device's hardware. We also present an initial experiment to fingerprint a device's GPU.",
    "We present the partition function of Chern-Simons theory with the exceptional gauge group on three-sphere in the form of a partition function of the refined closed topological string with relation $2\\tau=g_s(1-b) $ between single K\\\"ahler parameter $\\tau$, string coupling constant $g_s$ and refinement parameter $b$, where $b=\\frac{5}{3},\\frac{5}{2},3,4,6$ for $G_2, F_4, E_6, E_7, E_8$, respectively. The non-zero BPS invariants $N^d_{J_L,J_R}$ ($d$ - degree) are $N^2_{0,\\frac{1}{2}}=1, N^{11}_{0,1}=1$. Besides these terms, partition function of Chern-Simons theory contains term corresponding to the refined constant maps of string theory. Derivation is based on the universal (in Vogel's sense) form of a Chern-Simons partition function on three-sphere, restricted to exceptional line $Exc$ with Vogel's parameters satisfying $\\gamma=2(\\alpha+\\beta)$. This line contains points, corresponding to the all exceptional groups. The same results are obtained for $F$ line $\\gamma=\\alpha+\\beta$ (containing $SU(4), SO(10)$ and $E_6$ groups), with the non-zero $N^2_{0,\\frac{1}{2}}=1, N^{7}_{0,1}=1$. In both cases refinement parameter $b$ ($=-\\epsilon_2/\\epsilon_1$ in terms of Nekrasov's parameters) is given in terms of universal parameters, restricted to the line, by $b=-\\beta/\\alpha$.",
    "The centerpoint theorem is a well-known and widely used result in discrete geometry. It states that for any point set $P$ of $n$ points in $\\mathbb{R}^d$, there is a point $c$, not necessarily from $P$, such that each halfspace containing $c$ contains at least $\\frac{n}{d+1}$ points of $P$. Such a point $c$ is called a centerpoint, and it can be viewed as a generalization of a median to higher dimensions. In other words, a centerpoint can be interpreted as a good representative for the point set $P$. But what if we allow more than one representative? For example in one-dimensional data sets, often certain quantiles are chosen as representatives instead of the median. We present a possible extension of the concept of quantiles to higher dimensions. The idea is to find a set $Q$ of (few) points such that every halfspace that contains one point of $Q$ contains a large fraction of the points of $P$ and every halfspace that contains more of $Q$ contains an even larger fraction of $P$. This setting is comparable to the well-studied concepts of weak $\\varepsilon$-nets and weak $\\varepsilon$-approximations, where it is stronger than the former but weaker than the latter.",
    "We have produced a new software package for the simulation of pulsar populations, \\textsc{PsrPopPy}, based on the \\textsc{Psrpop} package. The codebase has been re-written in Python (save for some external libraries, which remain in their native Fortran), utilising the object-oriented features of the language, and improving the modularity of the code. Pre-written scripts are provided for running the simulations in `standard' modes of operation, but the code is flexible enough to support the writing of personalised scripts. The modular structure also makes the addition of experimental features (such as new models for period or luminosity distributions) more straightforward than with the previous code. We also discuss potential additions to the modelling capabilities of the software. Finally, we demonstrate some potential applications of the code; first, using results of surveys at different observing frequencies, we find pulsar spectral indices are best fit by a normal distribution with mean $-1.4$ and standard deviation $1.0$. Second, we model pulsar spin evolution to calculate the best-fit for a relationship between a pulsar's luminosity and spin parameters. We used the code to replicate the analysis of Faucher-Gigu\\`ere & Kaspi, and have subsequently optimized their power-law dependence of radio luminosity, $L$, with period, $P$, and period derivative, $\\dot{P}$. We find that the underlying population is best described by $L \\propto P^{-1.39 \\pm 0.09} \\dot{P}^{0.48 \\pm 0.04}$ and is very similar to that found for $\\gamma$-ray pulsars by Perera et al. Using this relationship, we generate a model population and examine the age-luminosity relation for the entire pulsar population, which may be measurable after future large-scale surveys with the Square Kilometer Array.",
    "We study the dynamics of a spin ensemble strongly coupled to a single-mode resonator driven by external pulses. When the mean frequency of the spin ensemble is in resonance with the cavity mode, damped Rabi oscillations are found between the spin ensemble and the cavity mode which we describe very accurately, including the dephasing effect of the inhomogeneous spin broadening. We demonstrate that a precise knowledge of this broadening is crucial both for a qualitative and a quantitative understanding of the temporal spin-cavity dynamics. On this basis we show that coherent oscillations between the spin ensemble and the cavity can be enhanced by a few orders of magnitude, when driving the system with pulses that match special resonance conditions. Our theoretical approach is tested successfully with an experiment based on an ensemble of negatively charged nitrogen-vacancy (NV) centers in diamond strongly coupled to a superconducting coplanar single-mode waveguide resonator.",
    "We investigate the ground-state Riemannian metric and the cyclic quantum distance of an inhomogeneous quantum Ising spin-1/2 chain in a transverse field. This model can be diagonalized by using a general canonical transformation to the fermionic Hamiltonian mapped from the spin system. The ground-state Riemannian metric is derived exactly on a parameter manifold ring $S^1$, which is introduced by performing a gauge transformation to the spin Hamiltonian through a twist operator. The ground-state cyclic quantum distance and the second derivative of the ground-state energy are studied in different inhomogeneous exchange coupling parameter region. Particularly, we show that the quantum ferromagnetic phase in the uniform Ising chain can be characterized by an invariant cyclic quantum distance with a constant ground-state Riemannian metric, and this metric will rapidly decay to zero in the paramagnetic phase.",
    "Rotation measure synthesis allows the estimation of Faraday dispersion via a Fourier transform and is the primary tool to probe cosmic magnetic fields. We show this can be considered mathematically equivalent to the one dimensional interferometric intensity measurement equation, albeit in a different Fourier space. As a result, familiar concepts in two dimensional intensity interferometry designed to correctly account for a range of instrumental conditions can be translated to the analysis of Faraday dispersion. In particular, we show how to model the effect of channel averaging during Faraday reconstruction, which has to date limited the progress of polarimetic science using wide-band measurements. Further, we simulate 1d sparse reconstruction with channel averaging for realistic frequency coverages, and show that it is possible to recover signals with large rotation measure values that were previously excluded from possible detection. This is especially important for low-frequency and wide-band polarimetry. We extended these ideas to introduce mosaicking in Faraday depth into the channel averaging process. This work, thus provides the first framework for correctly undertaking wide-band rotation measure synthesis, including the provision to add data from multiple telescopes, a prospect that should vastly improve the quality and quantity of polarimetric science. This is of particular importance for extreme environments which generate high magnetic fields such as those associated with pulsars and Fast Radio Bursts (FRBs), and will allow such sources to be accurately used as probes of cosmological fields.",
    "Study of the characteristic properties of charged particle production in hadron-nucleus collisions at high energies, by utilising the approaches from different statistical models is performed.~Predictions from different approaches using the Negative Binomial distribution, shifted Gompertz distribution, Weibull distribution and the Krasznovszky-Wagner distribution are utilised for a comparative study of the relative successes of these models.~These distributions derived from a variety of functional forms are based on either phenomenological parameterizations or some model of the underlying dynamics.~Some of these have have also been used to study the data at the LHC for both proton-proton and nucleus-nucleus collisions.~Various physical and derived observables have been used for the analysis.",
    "In 1975 John Tukey proposed a multivariate median which is the 'deepest' point in a given data cloud in R^d. Later, in measuring the depth of an arbitrary point z with respect to the data, David Donoho and Miriam Gasko considered hyperplanes through z and determined its 'depth' by the smallest portion of data that are separated by such a hyperplane. Since then, these ideas has proved extremely fruitful. A rich statistical methodology has developed that is based on data depth and, more general, nonparametric depth statistics. General notions of data depth have been introduced as well as many special ones. These notions vary regarding their computability and robustness and their sensitivity to reflect asymmetric shapes of the data. According to their different properties they fit to particular applications. The upper level sets of a depth statistic provide a family of set-valued statistics, named depth-trimmed or central regions. They describe the distribution regarding its location, scale and shape. The most central region serves as a median. The notion of depth has been extended from data clouds, that is empirical distributions, to general probability distributions on R^d, thus allowing for laws of large numbers and consistency results. It has also been extended from d-variate data to data in functional spaces.",
    "Strain-engineering in SiGe nanostructures is fundamental for the design of optoelectronic devices at the nanoscale. Here we explore a new strategy, where SiGe structures are laterally confined by the Si substrate, to obtain high tensile strain avoiding the use of external stressors, and thus improving the scalability. Spectro-microscopy techniques, finite element method simulations and ab initio calculations are used to investigate the strain state of laterally confined Ge-rich SiGe nano-stripes. Strain information is obtained by tip enhanced Raman spectroscopy with an unprecedented lateral resolution of ~ 30 nm. The nano-stripes exhibit a large tensile hydrostatic strain component, which is maximum at the center of the top free surface, and becomes very small at the edges. The maximum lattice deformation is larger than the typical values of thermally relaxed Ge/Si(001) layers. This strain enhancement originates from a frustrated relaxation in the out-of-plane direction, resulting from the combination of the lateral confinement induced by the substrate side walls and the plastic relaxation of the misfit strain in the (001) plane at the SiGe/Si interface. The effect of this tensile lattice deformation at the stripe surface is probed by work function mapping, performed with a spatial resolution better than 100 nm using X-ray photoelectron emission microscopy. The nano-stripes exhibit a positive work function shift with respect to a bulk SiGe alloy, quantitatively confirmed by electronic structure calculations of tensile strained configurations. The present results have a potential impact on the design of optoelectronic devices at a nanometer length scale.",
    "During an infectious disease pandemic, it is critical to share electronic medical records or models (learned from these records) across regions. Applying one region's data/model to another region often have distribution shift issues that violate the assumptions of traditional machine learning techniques. Transfer learning can be a solution. To explore the potential of deep transfer learning algorithms, we applied two data-based algorithms (domain adversarial neural networks and maximum classifier discrepancy) and model-based transfer learning algorithms to infectious disease detection tasks. We further studied well-defined synthetic scenarios where the data distribution differences between two regions are known. Our experiments show that, in the context of infectious disease classification, transfer learning may be useful when (1) the source and target are similar and the target training data is insufficient and (2) the target training data does not have labels. Model-based transfer learning works well in the first situation, in which case the performance closely matched that of the data-based transfer learning models. Still, further investigation of the domain shift in real world research data to account for the drop in performance is needed.",
    "Bound states in the continuum (BIC) have been at the forefront of research in optics and photonics over the past decade. It is of great interest to study the effects associated with quasi-BICs in the simplest structures, where quasi-BICs are very pronounced. An example is a dielectric cylinder, and in a number of works, quasi-BICs have been studied both in single cylinders and in structures composed of cylinders. In this work, we studied the properties of quasi-BICs during the transition from a homogeneous dielectric cylinder in an air environment to a ring with narrow walls while increasing the diameter of the inner air cylinder gradually. The results demonstrate the quasi-BIC crossover from the strong-coupling to the weak-coupling regime, which manifests itself in the transition from avoided crossing of branches to their intersection with the quasi-BIC being preserved on only one straight branch. In the regime of strong-coupling and quasi-BIC, three waves interfere in the far-field zone: two waves corresponding to the resonant modes of the structure and the wave scattered by the structure as a whole. The validity of the Fano resonance concept is discussed, since it describes the interference of only two waves under weak coupling conditions.",
    "Turbulent thermal diffusion is a combined effect of the temperature stratified turbulence and inertia of small particles. It causes the appearance of a non-diffusive turbulent flux of particles in the direction of the turbulent heat flux. This non-diffusive turbulent flux of particles is proportional to the product of the mean particle number density and the effective velocity of inertial particles. The theory of this effect has been previously developed only for small temperature gradients and small Stokes numbers (Phys. Rev. Lett. {\\bf 76}, 224, 1996). In this study a generalized theory of turbulent thermal diffusion for arbitrary temperature gradients and Stokes numbers has been developed. The laboratory experiments in the oscillating grid turbulence and in the multi-fan produced turbulence have been performed to validate the theory of turbulent thermal diffusion in strongly stratified turbulent flows. It has been shown that the ratio of the effective velocity of inertial particles to the characteristic vertical turbulent velocity for large Reynolds numbers is less than 1. The effective velocity of inertial particles as well as the effective coefficient of turbulent thermal diffusion increase with Stokes numbers reaching the maximum at small Stokes numbers and decreases for larger Stokes numbers. The effective coefficient of turbulent thermal diffusion also decreases with the mean temperature gradient. It has been demonstrated that the developed theory is in a good agreement with the results of the laboratory experiments.",
    "A standard model for the visibility of pulsar radio emission is based on the assumption that the emission is confined to a narrow cone about the tangent to a dipolar field line. The widely accepted rotating vector model (RVM) is an approximation in which the line of sight is fixed and the field line is not strictly tangent to it. We refer to an exact treatment (Gangadhara 2004) as the tangent model. In the tangent model (but not in the RVM) the visible point changes as a function of pulsar rotational phase, $\\psi$, defining a trajectory on a sphere of radius $r$. We solve for the trajectory and for the angular velocity of the visible point around it. We note the recent claim that this motion is observable using interstellar holography (Pen et al. 2014). We estimate the error introduced by use of the RVM and find that it is significant for pulsars with emission over a wide range of $\\psi$. The RVM tends to underestimate the range of $\\psi$ over which emission is visible. We suggest that the geometry alone strongly favors the visible pulsar radio being emitted at a heights more than ten percent of the light-cylinder distance, where our neglect of retardation effects becomes significant.",
    "In image recognition, there are many cases where training samples cannot cover all target classes. Zero-shot learning (ZSL) utilizes the class semantic information to classify samples of the unseen categories that have no corresponding samples contained in the training set. In this paper, we propose an end-to-end framework, called Global Semantic Consistency Network (GSC-Net for short), which makes complete use of the semantic information of both seen and unseen classes, to support effective zero-shot learning. We also adopt a soft label embedding loss to further exploit the semantic relationships among classes. To adapt GSC-Net to a more practical setting, Generalized Zero-shot Learning (GZSL), we introduce a parametric novelty detection mechanism. Our approach achieves the state-of-the-art performance on both ZSL and GZSL tasks over three visual attribute datasets, which validates the effectiveness and advantage of the proposed framework.",
    "The popular view according to which Category theory provides a support for Mathematical Structuralism is erroneous. Category-theoretic foundations of mathematics require a different philosophy of mathematics. While structural mathematics studies invariant forms (Awodey) categorical mathematics studies covariant transformations which, generally, don t have any invariants. In this paper I develop a non-structuralist interpretation of categorical mathematics and show its consequences for history of mathematics and mathematics education.",
    "We show that in a non-equilibrium system of an exciton-polariton condensate, where polaritons are generated from incoherent pumping, a ring-shaped pump allows for stationary vortex memory elements of topological charge $m = 1$ or $m = -1$. Using simple potential guides we can choose whether to copy the same charge or invert it onto another spatially separate ring pump. Such manipulation of binary information opens the possibility of a new type processing using vortices as topologically protected memory components.",
    "During the three years long assessment phase of the LOFT mission, candidate to the M3 launch opportunity of the ESA Cosmic Vision programme, we estimated and measured the radiation damage of the silicon drift detectors (SDDs) of the satellite instrumentation. In particular, we irradiated the detectors with protons (of 0.8 and 11 MeV energy) to study the increment of leakage current and the variation of the charge collection efficiency produced by the displacement damage, and we \"bombarded\" the detectors with hypervelocity dust grains to measure the effect of the debris impacts. In this paper we describe the measurements and discuss the results in the context of the LOFT mission.",
    "In this paper we examine the ability of low-level multimodal features to extract movie similarity, in the context of a content-based movie recommendation approach. In particular, we demonstrate the extraction of multimodal representation models of movies, based on textual information from subtitles, as well as cues from the audio and visual channels. With regards to the textual domain, we emphasize our research in topic modeling of movies based on their subtitles, in order to extract topics that discriminate between movies. Regarding the visual domain, we focus on the extraction of semantically useful features that model camera movements, colors and faces, while for the audio domain we adopt simple classification aggregates based on pretrained models. The three domains are combined with static metadata (e.g. directors, actors) to prove that the content-based movie similarity procedure can be enhanced with low-level multimodal information. In order to demonstrate the proposed content representation approach, we have built a small dataset of 160 widely known movies. We assert movie similarities, as propagated by the individual modalities and fusion models, in the form of recommendation rankings. Extensive experimentation proves that all three low-level modalities (text, audio and visual) boost the performance of a content-based recommendation system, compared to the typical metadata-based content representation, by more than $50\\%$ relative increase. To our knowledge, this is the first approach that utilizes a wide range of features from all involved modalities, in order to enhance the performance of the content similarity estimation, compared to the metadata-based approaches.",
    "We study black hole radiation of a Reissner-Nordstrom black hole with an electric charge in the framework of quantum gravity. Based on a canonical quantization for a spherically symmetric geometry, under physically plausible assumptions, we solve the Wheeler-De Witt equation in the regions not only between the outer apparent horizon and the spatial infinity but also between the spacetime singularity and the inner apparent horizon, and then show that the mass loss rate of an evaporating black hole due to thermal radiation agrees with the semiclassical result when we choose an integration constant properly by physical reasoning. Furthermore, we also solve the Wheeler-De Witt equation in the region between the inner Cauchy horizon and the outer apparent horizon, and show that the mass loss rate of an evaporating black hole has the same expression. The present study is the natural generalization of the case of a Schwarzschild black hole to that of a charged Reissner-Nordstrom black hole.",
    "We present multi-agent A* (MAA*), the first complete and optimal heuristic search algorithm for solving decentralized partially-observable Markov decision problems (DEC-POMDPs) with finite horizon. The algorithm is suitable for computing optimal plans for a cooperative group of agents that operate in a stochastic environment such as multirobot coordination, network traffic control, `or distributed resource allocation. Solving such problems efiectively is a major challenge in the area of planning under uncertainty. Our solution is based on a synthesis of classical heuristic search and decentralized control theory. Experimental results show that MAA* has significant advantages. We introduce an anytime variant of MAA* and conclude with a discussion of promising extensions such as an approach to solving infinite horizon problems.",
    "We present morphological classifications obtained using machine learning for objects in SDSS DR6 that have been classified by Galaxy Zoo into three classes, namely early types, spirals and point sources/artifacts. An artificial neural network is trained on a subset of objects classified by the human eye and we test whether the machine learning algorithm can reproduce the human classifications for the rest of the sample. We find that the success of the neural network in matching the human classifications depends crucially on the set of input parameters chosen for the machine-learning algorithm. The colours and parameters associated with profile-fitting are reasonable in separating the objects into three classes. However, these results are considerably improved when adding adaptive shape parameters as well as concentration and texture. The adaptive moments, concentration and texture parameters alone cannot distinguish between early type galaxies and the point sources/artifacts. Using a set of twelve parameters, the neural network is able to reproduce the human classifications to better than 90% for all three morphological classes. We find that using a training set that is incomplete in magnitude does not degrade our results given our particular choice of the input parameters to the network. We conclude that it is promising to use machine- learning algorithms to perform morphological classification for the next generation of wide-field imaging surveys and that the Galaxy Zoo catalogue provides an invaluable training set for such purposes.",
    "The Lambek calculus is a well-known logical formalism for modelling natural language syntax. The original calculus covered a substantial number of intricate natural language phenomena, but only those restricted to the context-free setting. In order to address more subtle linguistic issues, the Lambek calculus has been extended in various ways. In particular, Morrill and Valentin (2015) introduce an extension with so-called exponential and bracket modalities. Their extension is based on a non-standard contraction rule for the exponential that interacts with the bracket structure in an intricate way. The standard contraction rule is not admissible in this calculus. In this paper we prove undecidability of the derivability problem in their calculus. We also investigate restricted decidable fragments considered by Morrill and Valentin and we show that these fragments belong to the NP class.",
    "The transition between the two phases of 4D Euclidean Dynamical Triangulation [1] was long believed to be of second order until in 1996 first order behavior was found for sufficiently large systems [5,9]. However, one may wonder if this finding was affected by the numerical methods used: to control volume fluctuations, in both studies [5,9] an artificial harmonic potential was added to the action; in [9] measurements were taken after a fixed number of accepted instead of attempted moves which introduces an additional error. Finally the simulations suffer from strong critical slowing down which may have been underestimated. In the present work, we address the above weaknesses: we allow the volume to fluctuate freely within a fixed interval; we take measurements after a fixed number of attempted moves; and we overcome critical slowing down by using an optimized parallel tempering algorithm [12]. With these improved methods, on systems of size up to 64k 4-simplices, we confirm that the phase transition is first order. In addition, we discuss a local criterion to decide whether parts of a triangulation are in the elongated or crumpled state and describe a new correspondence between EDT and the balls in boxes model. The latter gives rise to a modified partition function with an additional, third coupling. Finally, we propose and motivate a class of modified path-integral measures that might remove the metastability of the Markov chain and turn the phase transition into second order.",
    "We characterize the virtually nilpotent finitely generated groups (or, equivalently by Gromov's theorem, groups of polynomial growth) for which the Domino Problem is decidable: These are the virtually free groups, i.e. finite groups, and those having $\\Z$ as a subgroup of finite index.",
    "Gamma rays from the annihilation of dark matter particles in the Galactic halo provide a particularly promising means of indirectly detecting dark matter. Here, we demonstrate that pronounced spectral features at energies near the dark matter particles' mass, which are a generic prediction for most models, can significantly improve the sensitivity of gamma-ray telescopes to dark matter signals. We derive projected limits on such features (including the traditionally looked-for line signals) and show that they can be much more efficient in constraining the nature of dark matter than the model-independent broad spectral features expected at lower energies.",
    "The attainment of carbon neutrality requires a research agenda that addresses the technical and economic challenges that will be encountered as we progress toward 100% renewable electricity generation. Increasing proportions of variable renewable energy (VRE) sources (such as wind turbines and photovoltaic systems) render the supply-and-demand balance of VRE-dominated power grids difficult. The operational characteristics and effects of VRE inverters also require attention. Here, we examine the implications of the paradigm shift to carbon neutrality and summarize the associated research challenges in terms of system planning, operation, and sta-bility, and the need for energy storage integration, demand-side participation, distributed con-trol and estimation, and energy sector coupling. We also highlight the existing literature gaps, and our recent studies that can fill in the gaps, thereby facilitating the improvement of grid op-eration and estimation. The numerical results of comparative case studies are also provided on the operational stability and economics of power grids with a high level of VRE sources, assist-ing stakeholders in establishing specific roadmaps and making relevant decisions.",
    "Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to \"absorb\" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers. We present a novel network architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to drastically better compressed performance than several relevant baselines.",
    "We conducted a Project-Based Learning (PBL)-type exercise incorporating Japanese cartoon (manga) techniques into Requirements Development (RD) processes. Manga has established techniques, such as those for character setting and story development, that we thought are also valid for RD processes. Using this manga-driven method, students were able to clarify high-level project goals early in the development life-cycle, and succeeded in defining high quality and unique system ideas.",
    "The standard Hawking formula predicts the complete evaporation of black holes. Taking into account effects of quantum gravity, we investigate fermions' tunnelling from a 5-dimensional rotating black string. The temperature is determined not only by the string, but also affected by the quantum number of the emitted fermion and the effect of the extra spatial dimension. The quantum correction slows down the increase of the temperature, which naturally leads to the remnant in the evaporation.",
    "We introduce second-order vector representations of words, induced from nearest neighborhood topological features in pre-trained contextual word embeddings. We then analyze the effects of using second-order embeddings as input features in two deep natural language processing models, for named entity recognition and recognizing textual entailment, as well as a linear model for paraphrase recognition. Surprisingly, we find that nearest neighbor information alone is sufficient to capture most of the performance benefits derived from using pre-trained word embeddings. Furthermore, second-order embeddings are able to handle highly heterogeneous data better than first-order representations, though at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information further improves model performance in some cases. Due to variance in the random initializations of word embeddings, utilizing nearest neighbor features from multiple first-order embedding samples can also contribute to downstream performance gains. Finally, we identify intriguing characteristics of second-order embedding spaces for further research, including much higher density and different semantic interpretations of cosine similarity.",
    "RIS-aided millimeter wave wireless systems benefit from robustness to blockage and enhanced coverage. In this paper, we study the ability of RIS to also provide enhanced localization capabilities as a by-product of communication. We consider sparse reconstruction algorithms to obtain high resolution channel estimates that are mapped to position information. In RIS-aided mmWave systems, the complexity of sparse recovery becomes a bottleneck, given the large number of elements of the RIS and the large communication arrays. We propose to exploit a multidimensional orthogonal matching pursuit strategy for compressive channel estimation in a RIS-aided millimeter wave system. We show how this algorithm, based on computing the projections on a set of independent dictionaries instead of a single large dictionary, enables high accuracy channel estimation at reduced complexity. We also combine this strategy with a localization approach which does not rely on the absolute time of arrival of the LoS path. Localization results in a realistic 3D indoor scenario show that RIS-aided wireless system can also benefit from a significant improvement in localization accuracy.",
    "Detection and quantification of information leaks through timing side channels are important to guarantee confidentiality. Although static analysis remains the prevalent approach for detecting timing side channels, it is computationally challenging for real-world applications. In addition, the detection techniques are usually restricted to 'yes' or 'no' answers. In practice, real-world applications may need to leak information about the secret. Therefore, quantification techniques are necessary to evaluate the resulting threats of information leaks. Since both problems are very difficult or impossible for static analysis techniques, we propose a dynamic analysis method. Our novel approach is to split the problem into two tasks. First, we learn a timing model of the program as a neural network. Second, we analyze the neural network to quantify information leaks. As demonstrated in our experiments, both of these tasks are feasible in practice --- making the approach a significant improvement over the state-of-the-art side channel detectors and quantifiers. Our key technical contributions are (a) a neural network architecture that enables side channel discovery and (b) an MILP-based algorithm to estimate the side-channel strength. On a set of micro-benchmarks and real-world applications, we show that neural network models learn timing behaviors of programs with thousands of methods. We also show that neural networks with thousands of neurons can be efficiently analyzed to detect and quantify information leaks through timing side channels.",
    "The inner asteroid belt between 2.1 and 2.5 au is of particular dynamical significance because it is the dominant source of both chondritic meteorites and near-Earth asteroids. This inner belt is bounded by an eccentricity-type secular resonance and by the 1:3 mean motion resonance with Jupiter. Unless asteroid perihelia are low enough to allow scattering by Mars, escape requires transport to one of the bounding resonances. In addition Yarkovsky forces are generally ineffective in changing either the eccentricity and/or inclination for asteroids with diameter $\\gtrsim$30 km. Thus, large asteroids with pericentres far from Mars may only escape from the inner belt through large changes in their eccentricities. In this paper we study chaotic diffusion of orbits near the 1:2 mean motion resonance with Mars in a systematic way. We show that, while chaotic orbital evolution in both resonant and non-resonant orbits increase the dispersion of the inclinations and eccentricities, it does not significantly change their mean values. We show further that, while the dispersive growth is greatest for resonant orbits, at high $e$ the resonance acts to mitigate asteroid scattering by Mars - making the asteroid lifetime in the belt longer than it would have been for a non-resonant orbit. For asteroids of all sizes in both resonant and non-resonant orbits, the changes in eccentricity needed to account for the observations cannot be achieved by gravitational forces alone. The role of resonant trapping in protecting asteroids from encounters with Mars is also analysed.",
    "The presence of nonstandard neutrino interactions (NSI) has a large effect on the precision measurements at next generation neutrino oscillation experiments. Other type of experiments are needed to constrain the NSI parameter space. We study the constraints on NSI with electrons from current and future $e^+e^-$ collider experiments including Belle II, STCF and CEPC. We find that Belle II and STCF will provide competitive and complementary bounds on electron-type NSI parameters compared to the current global analysis, and strong improvements for the constraints on tau-type NSI. In addition, CEPC alone will impose stringent constraints on the parameter space of NSI with electrons. We find that the degeneracy between the left-handed (vector) and right-handed (axial-vector) NSI parameters can be lifted by combining the data from three different running modes, and the allowed ranges for $|\\epsilon_{ee}^{eL}|$ ($|\\epsilon_{ee}^{eV}|$) and $|\\epsilon_{ee}^{eR}|$ ($|\\epsilon_{ee}^{eA}|$) can be constrained to be smaller than 0.002 at CEPC even if both of them are present.",
    "The Deep Underground Neutrino Experiment (DUNE) is a leading-edge experiment designed to perform neutrino science and proton decay searches. In particular, the far detector will consist of four 10-kton Liquid Argon (LAr) Time Projection Chambers using both single and dual-phase technologies. The latter provides charge amplification in the gaseous phase. In order to optimize these designs, two large prototypes are taking data at CERN since 2018. Previously, a dual-phase 4-tonne demonstrator was constructed and exposed to cosmic muons in 2017 and exhibited good performance in terms of charge and light collection. The light detection system is important to provide a trigger to the charge acquisition system and to obtain additional information from the scintillation light produced in the particle interaction. In the demonstrator, five cryogenic photo-multipliers were installed with different base polarity configurations and wavelength shifting methods. During the detector operation, scintillation light data were collected in different drift and amplification field conditions. An overview of the light detection system performance and results on the light production and propagation are presented. Our studies allowed to improve the understanding of some LAr properties.",
    "Major chip manufacturers have all introduced Multithreaded processors. These processors are used for running a variety of workloads. Efficient resource utilization is an important design aspect in such processors. Particularly, it is important to take advantage of available memory-level parallelism(MLP). In this paper I propose a MLP aware operating system (OS) scheduling algorithm for Multithreaded Multi-core processors. By observing the MLP available in each thread and by balancing it with available MLP resources in the system the OS will come up with a new schedule of threads for the next quantum that could potentially improve overall performance. We do a qualitative comparison of our solution with other hardware and software techniques. This work can be extended by doing a quantitative evaluation and by further refining the scheduling optimization.",
    "We consider the problem of calibrating a compressed sensing measurement system under the assumption that the decalibration consists in unknown gains on each measure. We focus on {\\em blind} calibration, using measures performed on a few unknown (but sparse) signals. A naive formulation of this blind calibration problem, using $\\ell_{1}$ minimization, is reminiscent of blind source separation and dictionary learning, which are known to be highly non-convex and riddled with local minima. In the considered context, we show that in fact this formulation can be exactly expressed as a convex optimization problem, and can be solved using off-the-shelf algorithms. Numerical simulations demonstrate the effectiveness of the approach even for highly uncalibrated measures, when a sufficient number of (unknown, but sparse) calibrating signals is provided. We observe that the success/failure of the approach seems to obey sharp phase transitions.",
    "We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder- decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.",
    "An increasing number of use cases require a timely extraction of non-trivial knowledge from semantically annotated data streams, especially on the Web and for the Internet of Things (IoT). Often, this extraction requires expressive reasoning, which is challenging to compute on large streams. We propose Laser, a new reasoner that supports a pragmatic, non-trivial fragment of the logic LARS which extends Answer Set Programming (ASP) for streams. At its core, Laser implements a novel evaluation procedure which annotates formulae to avoid the re-computation of duplicates at multiple time points. This procedure, combined with a judicious implementation of the LARS operators, is responsible for significantly better runtimes than the ones of other state-of-the-art systems like C-SPARQL and CQELS, or an implementation of LARS which runs on the ASP solver Clingo. This enables the application of expressive logic-based reasoning to large streams and opens the door to a wider range of stream reasoning use cases.",
    "The second law of thermodynamics dictates the fundamental limits to the amount of energy and information that can be exchanged between physical systems. In this work, we extend a thermodynamic formalism describing this flow of energy and information developed for a pair of bipartite systems to many multipartite systems. We identify a natural thermodynamic quantity that describes the information exchanged among these systems. We then introduce and discuss a refined version. Our results are illustrated with a model of two, competing Maxwell demons.",
    "The negligible intrinsic spin-orbit coupling (SOC) in graphene can be enhanced by proximity effects in stacked heterostructures of graphene and transition metal dichalcogenides (TMDCs). The composition of the TMDC layer plays a key role in determining the nature and strength of the resultant SOC induced in the graphene layer. Here, we study the evolution of the proximity-induced SOC as the TMDC layer is deliberately defected. Alloyed ${\\rm G/W_{\\chi}Mo_{1-\\chi}Se_2}$ heterostructures with diverse compositions ($\\chi$) and defect distributions are simulated using density functional theory. Comparison with continuum and tight-binding models allows both local and global signatures of the metal-atom alloying to be clarified. Our findings show that, despite some dramatic perturbation of local parameters for individual defects, the low-energy spin and electronic behaviour follow a simple effective medium model which depends only on the composition ratio of the metallic species in the TMDC layer. Furthermore, we demonstrate that the topological state of such alloyed systems can be feasibly tuned by controlling this ratio.",
    "Atomic masses play a crucial role in many nuclear astrophysics calculations. The lack of experimental values for relevant exotic nuclides triggered a rapid development of new mass measurement devices around the world. The Time-of-Flight (TOF) mass measurements offer a complementary technique to the most precise one, Penning trap measurements, the latter being limited by the rate and half-lives of the ions of interest. The NSCL facility provides a well-suited infrastructure for TOF mass measurements of very exotic nuclei. At this facility, we have recently implemented a TOF-Brho technique and performed mass measurements of neutron-rich nuclides in the Fe region, important for r-process calculations and for calculations of processes occurring in the crust of accreting neutron stars.",
    "Although the super-massive (AGN) and stellar mass (XRBs) black holes have many properties in common, the broad emission lines (BELs) are exclusively signatures of the AGN. Based on the detection of these lines from SDSS data bases, there seems to be no AGN with mass M_BH \\lesssim 10^5 M_sun. In this paper we investigate if such low mass black holes are really non-existent or they are undetected because the BELs in them are not produced efficiently. Using the ionizing spectral energy distribution for a wide range of black hole mass, 10 - 10^9 M_sun, spanning XRBs to AGN, we calculate the equivalent widths (EWs) of ultraviolet and optical lines Ly\\alpha 1216 \\AA, H\\beta 4861 \\AA, CIV 1549 \\AA and MgII 2798 \\AA. The LOC (locally optimally emitting cloud) model has been used to describe the broad emission line region (BELR) for the calculations. We find that the hardening of the SED shape with decreasing mass do not decrease the BEL EWs. However, finite size of the BELR, as measured by the line widths, which is controlled by the mass of the black hole, regulates the production of these emission lines. There seems to be a peak in the EWs of the emission lines for typical AGN black holes of ~ 10^8 M_sun, below which the lines become intrinsically fainter with a sharp fall-off below ~ 10^6 M_sun. This may be the cause of the absence of low mass AGN in SDSS.",
    "The precision of synchronization algorithms based on the theory of pulse-coupled oscillators is evaluated on FPGA-based radios for the first time. Measurements show that such algorithms can reach precision in the low microsecond range when being implemented in the physical layer. Furthermore, we propose an algorithm extension accounting for phase rate deviations of the hardware and show that an improved precision below one microsecond is possible with this extension in the given setup. The resulting algorithm can thus be applied in ad hoc wireless systems for fully distributed synchronization of transmission slots or sleep cycles, in particular, if centralized synchronization is impossible.",
    "Human Trajectory Prediction (HTP) has gained much momentum in the last years and many solutions have been proposed to solve it. Proper benchmarking being a key issue for comparing methods, this paper addresses the question of evaluating how complex is a given dataset with respect to the prediction problem. For assessing a dataset complexity, we define a series of indicators around three concepts: Trajectory predictability; Trajectory regularity; Context complexity. We compare the most common datasets used in HTP in the light of these indicators and discuss what this may imply on benchmarking of HTP algorithms. Our source code is released on Github.",
    "The purpose of this paper is to show how a class of classical linear stochastic systems can be physically implemented using quantum optical components. Quantum optical systems typically have much higher bandwidth than electronic devices, meaning faster response and processing times, and hence has the potential for providing better performance than classical systems. A procedure is provided for constructing the quantum optical realization. The paper also describes the use of the quantum optical realization in a measurement feedback loop. Some examples are given to illustrate the application of the main results.",
    "Systems biology uses large networks of biochemical reactions to model the functioning of biological cells from the molecular to the cellular scale. The dynamics of dissipative reaction networks with many well separated time scales can be described as a sequence of successive equilibrations of different subsets of variables of the system. Polynomial systems with separation are equilibrated when at least two monomials, of opposite signs, have the same order of magnitude and dominate the others. These equilibrations and the corresponding truncated dynamics, obtained by eliminating the dominated terms, find a natural formulation in tropical analysis and can be used for model reduction.",
    "We performed spectral analysis of Suzaku data of the galactic disk and outflow regions of the starburst galaxy M82. Thermal modeling of the central disk regions requires at least three temperature components. The Ly$\\beta$ line fluxes of O VIII and Ne X exceed those expected from a plasma in collisional ionization equilibrium. The ratios of Ly$\\beta$/Ly$\\alpha$ lines for O VIII and Ne X are higher than those of collisional ionization equilibrium, which may be caused by the process of charge exchange. In the outflow wind region, the spectra are well reproduced with two-temperature thermal models, and we have derived the metal abundances of O, Ne, Mg, and Fe in the outflow. The ratios of O/Fe, Ne/Fe, and Mg/Fe are about 2, 3, and 2, respectively, relative to the solar value determined by Lodders (2003). Since there is no evidence of charge exchange in outflow region, the metal abundances should be more reliable than those in the central region. This abundance pattern indicates that starburst activity enriches the outflow through SN II metal ejection into intergalactic space.",
    "Dust grains are classically thought to form in the winds of asymptotic giant branch (AGB) stars. However, there is increasing evidence today for dust formation in supernovae (SNe). To establish the relative importance of these two classes of stellar sources of dust, it is important to know the fraction of freshly formed dust in SN ejecta that is able to survive the passage of the reverse shock and be injected in the interstellar medium. We have developed a new code (GRASH\\_Rev) which follows the newly-formed dust evolution throughout the supernova explosion until the merging of the forward shock with the circumstellar ISM. We have considered four well studied SNe in the Milky Way and Large Magellanic Cloud: SN1987A, CasA, the Crab Nebula, and N49. For all the simulated models, we find good agreement with observations and estimate that between 1 and 8$\\%$ of the observed mass will survive, leading to a SN dust production rate of $(3.9 \\pm 3.7) \\times 10^{-4}$ M$_{\\odot}$yr$^{-1}$ in the Milky Way. This value is one order of magnitude larger than the dust production rate by AGB stars but insufficient to counterbalance the dust destruction by SNe, therefore requiring dust accretion in the gas phase.",
    "This paper investigates in hatching process strategies for additive manufacturing using an electron beam by numerical simulations. The underlying physical model and the corresponding three dimensional thermal free surface lattice Boltzmann method of the simulation software are briefly presented. The simulation software has already been validated on the basis of experiments up to 1.2 kW beam power by hatching a cuboid with a basic process strategy, whereby the results are classified into `porous', `good' and `uneven', depending on their relative density and top surface smoothness. In this paper we study the limitations of this basic process strategy in terms of higher beam powers and scan velocities to exploit the future potential of high power electron beam guns up to 10 kW. Subsequently, we introduce modified process strategies, which circumvent these restrictions, to build the part as fast as possible under the restriction of a fully dense part with a smooth top surface. These process strategies are suitable to reduce the build time and costs, maximize the beam power usage and therefore use the potential of high power electron beam guns.",
    "Bayesian optimization (BO) is a class of global optimization algorithms, suitable for minimizing an expensive objective function in as few function evaluations as possible. While BO budgets are typically given in iterations, this implicitly measures convergence in terms of iteration count and assumes each evaluation has identical cost. In practice, evaluation costs may vary in different regions of the search space. For example, the cost of neural network training increases quadratically with layer size, which is a typical hyperparameter. Cost-aware BO measures convergence with alternative cost metrics such as time, energy, or money, for which vanilla BO methods are unsuited. We introduce Cost Apportioned BO (CArBO), which attempts to minimize an objective function in as little cost as possible. CArBO combines a cost-effective initial design with a cost-cooled optimization phase which depreciates a learned cost model as iterations proceed. On a set of 20 black-box function optimization problems we show that, given the same cost budget, CArBO finds significantly better hyperparameter configurations than competing methods.",
    "This work contributes a marsupial robotic system-of-systems involving a legged and an aerial robot capable of collaborative mapping and exploration path planning that exploits the heterogeneous properties of the two systems and the ability to selectively deploy the aerial system from the ground robot. Exploiting the dexterous locomotion capabilities and long endurance of quadruped robots, the marsupial combination can explore within large-scale and confined environments involving rough terrain. However, as certain types of terrain or vertical geometries can render any ground system unable to continue its exploration, the marsupial system can - when needed - deploy the flying robot which, by exploiting its 3D navigation capabilities, can undertake a focused exploration task within its endurance limitations. Focusing on autonomy, the two systems can co-localize and map together by sharing LiDAR-based maps and plan exploration paths individually, while a tailored graph search onboard the legged robot allows it to identify where and when the ferried aerial platform should be deployed. The system is verified within multiple experimental studies demonstrating the expanded exploration capabilities of the marsupial system-of-systems and facilitating the exploration of otherwise individually unreachable areas.",
    "We propose a method for polarising antiprotons in a storage ring by means of a polarised positron beam moving parallel to the antiprotons. If the relative velocity is adjusted to $v/c \\approx 0.002$ the cross section for spin-flip is as large as about $2 \\cdot 10^{13}$ barn as shown by new QED-calculations of the triple spin-cross sections. Two possibilities for providing a positron source with sufficient flux density are presented. A polarised positron beam with a polarisation of 0.70 and a flux density of approximately $1.5 \\cdot 10^{10}$/(mm$^2$ s) appears to be feasible by means of a radioactive $^{11}$C dc-source. A more involved proposal is the production of polarised positrons by pair production with circularly polarised photons. It yields a polarisation of 0.76 and requires the injection into a small storage ring. Such polariser sources can be used at low (100 MeV) as well as at high (1 GeV) energy storage rings providing a time of about one hour for polarisation build-up of about $10^{10}$ antiprotons to a polarisation of about 0.18. A comparison with other proposals show a gain in the figure-of-merit by a factor of about ten.",
    "Loops are essential secondary structure elements in folded DNA and RNA molecules and proliferate close to the melting transition. Using a theory for nucleic acid secondary structures that accounts for the logarithmic entropy c ln m for a loop of length m, we study homopolymeric single-stranded nucleic acid chains under external force and varying temperature. In the thermodynamic limit of a long strand, the chain displays a phase transition between a low temperature / low force compact (folded) structure and a high temperature / high force molten (unfolded) structure. The influence of c on phase diagrams, critical exponents, melting, and force extension curves is derived analytically. For vanishing pulling force, only for the limited range of loop exponents 2 < c < 2.479 a melting transition is possible; for c <= 2 the chain is always in the folded phase and for 2.479 < c always in the unfolded phase. A force induced melting transition with singular behavior is possible for all loop exponents c < 2.479 and can be observed experimentally by single molecule force spectroscopy. These findings have implications for the hybridization or denaturation of double stranded nucleic acids. The Poland-Scheraga model for nucleic acid duplex melting does not allow base pairing between nucleotides on the same strand in denatured regions of the double strand. If the sequence allows these intra-strand base pairs, we show that for a realistic loop exponent c ~ 2.1 pronounced secondary structures appear inside the single strands. This leads to a lower melting temperature of the duplex than predicted by the Poland-Scheraga model. Further, these secondary structures renormalize the effective loop exponent c^, which characterizes the weight of a denatured region of the double strand, and thus affect universal aspects of the duplex melting transition.",
    "We study the Zeeman spin-splitting in hole quantum wires oriented along the $[011]$ and $[01\\bar{1}]$ crystallographic axes of a high mobility undoped (100)-oriented AlGaAs/GaAs heterostructure. Our data shows that the spin-splitting can be switched `on' (finite $g^{*}$) or `off' (zero $g^{*}$) by rotating the field from a parallel to a perpendicular orientation with respect to the wire, and the properties of the wire are identical for the two orientations with respect to the crystallographic axes. We also find that the $g$-factor in the parallel orientation decreases as the wire is narrowed. This is in contrast to electron quantum wires, where the $g$-factor is enhanced by exchange effects as the wire is narrowed. This is evidence for a $k$-dependent Zeeman splitting that arises from the spin-3/2 nature of holes.",
    "An analogy with real Clifford algebras on even-dimensional vector spaces suggests to assign a couple of space and time dimensions modulo 8 to any algebra (represented over a complex Hilbert space) containing two self-adjoint involutions and an anti-unitary operator with specific commutation relations. It is shown that this assignment is compatible with the tensor product: the space and time dimensions of the tensor product are the sums of the space and time dimensions of its factors. This could provide an interpretation of the presence of such algebras in PT-symmetric Hamiltonians or the description of topological matter. This construction is used to build an indefinite (i.e. pseudo-Riemannian) version of the spectral triples of noncommutative geometry, defined over Krein spaces instead of Hilbert spaces. Within this framework, we can express the Lagrangian (both bosonic and fermionic) of a Lorentzian almost-commutative spectral triple. We exhibit a space of physical states that solves the fermion-doubling problem. The example of quantum electrodynamics is described.",
    "We study the space-time symmetries of the actions obtained by expanding the action for a massive free relativistic particle around the Galilean action. We obtain all the point space-time symmetries of the post-Galilean actions by working in canonical space. We also construct an infinite collection of generalized Schr\\\"odinger algebras parameterized by an integer $M$, with $M=0$ corresponding to the standard Schr\\\"odinger algebra. We discuss the Schr\\\"odinger equations associated to these algebras, their solutions and projective phases.",
    "Accretion disc theory is less developed than stellar evolution theory although a similarly mature phenomenological picture is ultimately desired. While the interplay of theory and numerical simulations has amplified community awareness of the role of magnetic fields in angular momentum transport, there remains a long term challenge to incorporate insight gained from simulations back into improving practical models for comparison with observations. Here we emphasize the need to incorporate the role of non-local transport more precisely. To show where large scale transport would fit into the theoretical framework and how it is currently missing, we review why the wonderfully practical approach of Shakura-Sunyaev (1973,SS73) is necessarily a mean field theory, and one which does not include large scale transport. Observations of coronae and jets combined with the interpretation of results even from shearing box simulations of the magnetorotational instability (MRI) suggest that a significant fraction of disc transport is indeed non-local. We show that the Maxwell stresses in saturation are dominated by large scale contributions and the physics of MRI transport is not fully captured by a viscosity. We also clarify the standard physical interpretation of the MRi as it applies to shearing boxes. Computational limitations have so far focused most attention toward local simulations but the next generation of global simulations should help to inform improved mean field theories. Mean field accretion theory and mean field dynamo theory should in fact be unified into a single theory that predicts the time evolution of spectra and luminosity from separate disc, corona, and outflow contributions. Finally, we note that any mean field theory has a finite predictive precision that needs to be quantified when comparing the predictions to observations.",
    "This paper is devoted to the global well-posedness of two Diffuse Interface systems modeling the motion of an incompressible two-phase fluid mixture in presence of capillarity effects in a bounded smooth domain $\\Omega\\subset \\mathbb{R}^d$, $d=2,3$. We focus on dissipative mixing effects originating from the mass-conserving Allen-Cahn dynamics with the physically relevant Flory-Huggins potential. More precisely, we study the mass-conserving Navier-Stokes-Allen-Cahn system for nonhomogeneous fluids and the mass-conserving Euler-Allen-Cahn system for homogeneous fluids. We prove existence and uniqueness of global weak and strong solutions as well as their property of separation from the pure states. In our analysis, we combine the energy and entropy estimates, a novel end-point estimate of the product of two functions, a new estimate for the Stokes problem with non-constant viscosity, and logarithmic type Gronwall arguments.",
    "We present explicit expressions for Fock-space projection operators that correspond to realistic final states in scattering experiments. Our operators automatically sum over unobserved quanta and account for non-emission into sub-regions of momentum space.",
    "In this talk we discuss mathematical structures associated to Feynman graphs. Feynman graphs are the backbone of calculations in perturbative quantum field theory. The mathematical structures -- apart from being of interest in their own right -- allow to derive algorithms for the computation of these graphs. Topics covered are the relations of Feynman integrals to periods, shuffle algebras and multiple polylogarithms.",
    "We report on a calculation of the generalized parton distributions of the photon when there is non-zero momentum transfer both in the transverse and longitudinal directions. By taking Fourier transforms of the GPDs with respect transverse and longitudinal momentum transfer, we obtain the parton distributions of the photon in position space.",
    "Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.",
    "We compute the leading logarithmic behaviour of the cross-section for the production of a pseudoscalar Higgs boson in gluon-gluon fusion to all-orders in perturbation theory, in the limit of large partonic centre of mass energy. We also calculate the Higgs rapidity distribution to the same accuracy. We include the contributions of top and bottom quarks, together with their interference. Our results are given in terms of single and double integrals, evaluated explicitly up to next-to next-to leading order (NNLO). We use our results to improve the known NNLO inclusive cross-section computed in the effective theory where the fermions in the loop are integrated out. The size of finite fermion mass effects on the inclusive cross-section is found to be small, reaching a few percent only for large values of the pseudoscalar mass.",
    "Finding outlying elements in probability distributions can be a hard problem. Taking a real example from Voting Rights Act enforcement, we consider the problem of maximizing the number of simultaneous majority-minority districts in a political districting plan. An unbiased random walk on districting plans is unlikely to find plans that approach this maximum. A common search approach is to use a biased random walk: preferentially select districting plans with more majority-minority districts. Here, we present a third option, called short bursts, in which an unbiased random walk is performed for a small number of steps (called the burst length), then re-started from the most extreme plan that was encountered in the last burst. We give empirical evidence that short-burst runs outperform biased random walks for the problem of maximizing the number of majority-minority districts, and that there are many values of burst length for which we see this improvement. Abstracting from our use case, we also consider short bursts where the underlying state space is a line with various probability distributions, and then explore some features of more complicated state spaces and how these impact the effectiveness of short bursts.",
    "We report on a molecular dynamics investigation of the wetting properties of graphitic surfaces by various solutions at concentrations 1-8 wt% of commercially available non-ionic surfactants with long hydrophilic chains, linear or T-shaped. These are surfactants of length up to 160 [\\AA]. It turns out that molecular dynamics simulations of such systems ask for a number of solvent particles that can be reached without seriously compromising computational efficiency only by employing a coarse-grained model. The MARTINI force field with polarizable water offers a framework particularly suited for the parameterization of our systems. In general, its advantages over other coarse-grained models are the possibility to explore faster long time scales and the wider range of applicability. Although the accuracy is sometimes put under question, the results for the wetting properties by pure water are in good agreement with those for the corresponding atomistic systems and theoretical predictions. On the other hand, the bulk properties of various aqueous surfactant solutions indicate that the micellar formation process is too strong. For this reason, a typical experimental configuration is better approached by preparing the droplets with the surfactants arranged in the initial state in the vicinity of contact line. Cross-comparisons are possible and illuminating, but equilibrium contanct angles as obtained from simulations overestimate the experimental results. Nevertheless, our findings can provide guidelines for the preliminary assessment and screening of surfactants. [See pdf file for full abstract]",
    "We review recent experiments in which superfluid $^3$He has been studied under highly controlled confinement in nanofluidic sample chambers. We discuss the experimental challenges and their resolution. These methods open the way to a systematic investigation of the superfluidity of $^3$He films, and the surface and edge excitations of topological superfluids.",
    "Code-mixed machine translation has become an important task in multilingual communities and extending the task of machine translation to code mixed data has become a common task for these languages. In the shared tasks of WMT 2022, we try to tackle the same for both English + Hindi to Hinglish and Hinglish to English. The first task dealt with both Roman and Devanagari script as we had monolingual data in both English and Hindi whereas the second task only had data in Roman script. To our knowledge, we achieved one of the top ROUGE-L and WER scores for the first task of Monolingual to Code-Mixed machine translation. In this paper, we discuss the use of mBART with some special pre-processing and post-processing (transliteration from Devanagari to Roman) for the first task in detail and the experiments that we performed for the second task of translating code-mixed Hinglish to monolingual English.",
    "Contrastive learning has shown promising potential in self-supervised spatio-temporal representation learning. Most works naively sample different clips to construct positive and negative pairs. However, we observe that this formulation inclines the model towards the background scene bias. The underlying reasons are twofold. First, the scene difference is usually more noticeable and easier to discriminate than the motion difference. Second, the clips sampled from the same video often share similar backgrounds but have distinct motions. Simply regarding them as positive pairs will draw the model to the static background rather than the motion pattern. To tackle this challenge, this paper presents a novel dual contrastive formulation. Concretely, we decouple the input RGB video sequence into two complementary modes, static scene and dynamic motion. Then, the original RGB features are pulled closer to the static features and the aligned dynamic features, respectively. In this way, the static scene and the dynamic motion are simultaneously encoded into the compact RGB representation. We further conduct the feature space decoupling via activation maps to distill static- and dynamic-related features. We term our method as \\textbf{D}ual \\textbf{C}ontrastive \\textbf{L}earning for spatio-temporal \\textbf{R}epresentation (DCLR). Extensive experiments demonstrate that DCLR learns effective spatio-temporal representations and obtains state-of-the-art or comparable performance on UCF-101, HMDB-51, and Diving-48 datasets.",
    "The electronic bandstructure and the Fermi surfaces of ferromagnetic CeRh3B2 are calculated by using FLAPW and LSDA+U method. As assuming several kinds of the ground state to describe the 4f electronic state, we propose a fully orbital- and spin-polarized state |lz=0, sx=1/2> as the ground state, instead of the conventional LS-coupled CEF ground state, generally expected in typical 4f compounds. This is supported by the fact that both the observed magnetic moment and the observed dHvA frequencies are well explained by the calculated electronic structure and the Fermi surfaces. The unconventional ground state is stabilized by the strong 4f-4f direct mixing between the neighbored Ce atoms along the extremely small distance along the c-axis in the hexagonal crystal cell.",
    "Matrix acidization simulation is a challenging task in the study of flows in porous media, due to the changing porosity in the procedure. The improved DBF framework is one model to do this simulation, and its numerical scheme discretises the mass and momentum conservation equations together to form a pressure-velocity linear system. However, this linear system can only be solved by direct solvers to solve for pressure and velocity simultaneously, since zeros appear in the diagonal of the coefficient matrix. Considering the large-scale attribute of matrix acidization simulation, the solving time of direct solvers is not intolerant. Thus, a decoupled scheme is proposed in this work to decouple the coupled pressure-velocity linear system into two independent linear systems: one is to solve for pressure, and the other one is to solve for velocity. Both of the new linear systems can be solved by parallel and iterative solvers, which guarantees the large-scale simulation can be finished in a reasonable time period. A numerical experiment is carried out to demonstrate the correctness of the decoupled scheme and its higher computing efficiency.",
    "Sensemaking and narrative are two inherently interconnected concepts about how people understand the world around them. Sensemaking is the process by which people structure and interconnect the information they encounter in the world with the knowledge and inferences they have made in the past. Narratives are important constructs that people use sensemaking to create; ones that reflect provide a more holistic account of the world than the information within any given narrative is able to alone. Both are important to how human beings parse the world, and both would be valuable for a computational system attempting to do the same. In this paper, we discuss theories of sensemaking and narrative with respect to how people build an understanding of the world based on the information they encounter, as well as the links between the fields of sensemaking and narrative research. We highlight a specific computational task, visual storytelling, whose solutions we believe can be enhanced by employing a sensemaking and narrative component. We then describe our system for visual storytelling using sensemaking and narrative and discuss examples from its current implementation.",
    "Evaluation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. However, currently, there exists no way to quantify how metrics respond to change in the dialect of a generated utterance. We thus formalize dialect robustness and dialect awareness as goals for NLG evaluation metrics. We introduce a suite of methods and corresponding statistical tests one can use to assess metrics in light of the two goals. Applying the suite to current state-of-the-art metrics, we demonstrate that they are not dialect-robust and that semantic perturbations frequently lead to smaller decreases in a metric than the introduction of dialect features. As a first step to overcome this limitation, we propose a training schema, NANO, which introduces regional and language information to the pretraining process of a metric. We demonstrate that NANO provides a size-efficient way for models to improve the dialect robustness while simultaneously improving their performance on the standard metric benchmark.",
    "Geographic routing consists in using the position information of nodes to assist in the routing process, and has been a widely studied subject in sensor networks. One of the outstanding challenges facing geographic routing has been its applicability. Authors either make some broad assumptions on an idealized version of wireless networks which are often unverifiable, or they use costly methods to planarize the communication graph. The overarching questions that drive us are the following. When, and how should we use geographic routing? Is there a criterion to tell whether a communication network is fit for geographic routing? When exactly does geographic routing make sense? In this paper we formulate the four principles that define geographic routing and explore their topological consequences. Given a localized communication network, we then define and compute its geographic eccentricity, which measures its fitness for geographic routing. Finally we propose a distributed algorithm that either enables geographic routing on the network or proves that its geographic eccentricity is too high.",
    "We show that spatial variation and correlation of superconductivity fluctuations in a two-band model are scaled by two characteristic lengths. This results in substantially more complicated picture compared to one-band systems. In particular, short-range correlations are always present in a two-band scenario, even near the phase transition point.",
    "We present online prediction methods for time series that let us explicitly handle nonstationary artifacts (e.g. trend and seasonality) present in most real time series. Specifically, we show that applying appropriate transformations to such time series before prediction can lead to improved theoretical and empirical prediction performance. Moreover, since these transformations are usually unknown, we employ the learning with experts setting to develop a fully online method (NonSTOP-NonSTationary Online Prediction) for predicting nonstationary time series. This framework allows for seasonality and/or other trends in univariate time series and cointegration in multivariate time series. Our algorithms and regret analysis subsume recent related work while significantly expanding the applicability of such methods. For all the methods, we provide sub-linear regret bounds using relaxed assumptions. The theoretical guarantees do not fully capture the benefits of the transformations, thus we provide a data-dependent analysis of the follow-the-leader algorithm that provides insight into the success of using such transformations. We support all of our results with experiments on simulated and real data.",
    "We present a heuristic framework for attacking the undecidable termination problem of logic programs, as an alternative to current termination/non-termination proof approaches. We introduce an idea of termination prediction, which predicts termination of a logic program in case that neither a termination nor a non-termination proof is applicable. We establish a necessary and sufficient characterization of infinite (generalized) SLDNF-derivations with arbitrary (concrete or moded) queries, and develop an algorithm that predicts termination of general logic programs with arbitrary non-floundering queries. We have implemented a termination prediction tool and obtained quite satisfactory experimental results. Except for five programs which break the experiment time limit, our prediction is 100% correct for all 296 benchmark programs of the Termination Competition 2007, of which eighteen programs cannot be proved by any of the existing state-of-the-art analyzers like AProVE07, NTI, Polytool and TALP.",
    "Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoretically tractable variant of random regression forests and prove that our algorithm is consistent. We also provide an empirical evaluation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in practice. Our experiments provide insight into the relative importance of different simplifications that theoreticians have made to obtain tractable models for analysis.",
    "Factorial Hidden Markov Models (FHMMs) are powerful models for sequential data but they do not scale well with long sequences. We propose a scalable inference and learning algorithm for FHMMs that draws on ideas from the stochastic variational inference, neural network and copula literatures. Unlike existing approaches, the proposed algorithm requires no message passing procedure among latent variables and can be distributed to a network of computers to speed up learning. Our experiments corroborate that the proposed algorithm does not introduce further approximation bias compared to the proven structured mean-field algorithm, and achieves better performance with long sequences and large FHMMs.",
    "Molecular dynamics simulations have been performed on pure liquid water, aqueous solutions of sodium chloride, and polymer solutions exposed to a strong external electric field with the goal to gain molecular insight into the structural response to the field. Several simulation methodologies have been used to elucidate the molecular mechanisms of the processes leading to the formation of liquid bridges and jets (in the production of nanofibers). It is shown that in the established nanoscale structures, the molecules form a chain with their dipole moments oriented parallel to the applied field throughout the entire sample volume. The presence of ions may disturb this structure leading to its ultimate disintegration into droplets; the concentration dependence of the threshold field required to stabilize a liquid column has been determined. Conformational changes of the polymer in the jetting process have also been observed.",
    "Recommender systems are increasingly used to predict and serve content that aligns with user taste, yet the task of matching new users with relevant content remains a challenge. We consider podcasting to be an emerging medium with rapid growth in adoption, and discuss challenges that arise when applying traditional recommendation approaches to address the cold-start problem. Using music consumption behavior, we examine two main techniques in inferring Spotify users preferences over more than 200k podcasts. Our results show significant improvements in consumption of up to 50\\% for both offline and online experiments. We provide extensive analysis on model performance and examine the degree to which music data as an input source introduces bias in recommendations.",
    "We calculate the Casimir energy and entropy for two perfect metal spheres in the large and short separation limit. We obtain nonmonotonic behavior of the Helmholtz free energy with separation and temperature, leading to parameter ranges with negative entropy, and also nonmonotonic behavior of the entropy with temperature and with the separation between the spheres. The appearance of this anomalous behavior of the entropy is discussed as well as its thermodynamic consequences.",
    "Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.",
    "In this paper, we propose two mask-based beamforming methods using a deep neural network (DNN) trained by multichannel loss functions. Beamforming technique using time-frequency (TF)-masks estimated by a DNN have been applied to many applications where TF-masks are used for estimating spatial covariance matrices. To train a DNN for mask-based beamforming, loss functions designed for monaural speech enhancement/separation have been employed. Although such a training criterion is simple, it does not directly correspond to the performance of mask-based beamforming. To overcome this problem, we use multichannel loss functions which evaluate the estimated spatial covariance matrices based on the multichannel Itakura--Saito divergence. DNNs trained by the multichannel loss functions can be applied to construct several beamformers. Experimental results confirmed their effectiveness and robustness to microphone configurations.",
    "Nano-FTIR imaging is a powerful scanning-based technique at nanometer spatial resolution which combines Fourier transform infrared spectroscopy (FTIR) and scattering-type scanning near-field optical microscopy (s-SNOM). However, recording large spatial areas with nano-FTIR is limited by long measurement times due to its sequential data acquisition. Several mathematical approaches have been proposed to tackle this problem. All of them have in common that only a small fraction of randomly chosen measurements is required. However, choosing the fraction of measurements in a random fashion poses practical challenges for scanning procedures and does not lead to time savings as large as desired. We consider different, practically relevant sub-sampling schemes assuring a faster acquisition. It is demonstrated that results for almost all considered sub-sampling schemes, namely original Lissajous, triangle Lissajous, and random reflection sub-sampling, at a sub-sampling rate of 10%, are comparable to results when using a random sub-sampling of 10%. This implies that random sub-sampling is not required for efficient data acquisition.",
    "We determine from Polyakov loop correlators the screening masses in th e deconfined phase of the (3+1)d SU(3) pure gauge theory at finite temperature near transition, for two different channels of angular momentum and parity. Their ratio is compared with that of the massive excitations with the same quantum numbers in the 3d 3-state Potts model in the broken phase near the transition point at zero magnetic field. Moreover we study the inverse decay length of the correlation between the real parts and between the imaginary parts of the Polyakov loop and compare the results with expectations from perturbation theory and mean-field Polyakov loop models.",
    "The Mahalanobis distance-based confidence score, a recently proposed anomaly detection method for pre-trained neural classifiers, achieves state-of-the-art performance on both out-of-distribution (OoD) and adversarial examples detection. This work analyzes why this method exhibits such strong performance in practical settings while imposing an implausible assumption; namely, that class conditional distributions of pre-trained features have tied covariance. Although the Mahalanobis distance-based method is claimed to be motivated by classification prediction confidence, we find that its superior performance stems from information not useful for classification. This suggests that the reason the Mahalanobis confidence score works so well is mistaken, and makes use of different information from ODIN, another popular OoD detection method based on prediction confidence. This perspective motivates us to combine these two methods, and the combined detector exhibits improved performance and robustness. These findings provide insight into the behavior of neural classifiers in response to anomalous inputs.",
    "Algorithms like those for differentiating functional expressions manipulate the syntactic structure of mathematical expressions in a mathematically meaningful way. A formalization of such an algorithm should include a specification of its computational behavior, a specification of its mathematical meaning, and a mechanism for applying the algorithm to actual expressions. Achieving these goals requires the ability to integrate reasoning about the syntax of the expressions with reasoning about what the expressions mean. A syntax framework is a mathematical structure that is an abstract model for a syntax reasoning system. It contains a mapping of expressions to syntactic values that represent the syntactic structures of the expressions; a language for reasoning about syntactic values; a quotation mechanism to refer to the syntactic value of an expression; and an evaluation mechanism to refer to the value of the expression represented by a syntactic value. We present and compare two approaches, based on instances of a syntax framework, to formalize a syntax-based mathematical algorithm in a formal theory T. In the first approach the syntactic values for the expressions manipulated by the algorithm are members of an inductive type in T, but quotation and evaluation are functions defined in the metatheory of T. In the second approach every expression in T is represented by a syntactic value, and quotation and evaluation are operators in T itself.",
    "The paper concerns two interacting consumer-resource pairs based on chemostat-like equations under the assumption that the dynamics of the resource is considerably slower than that of the consumer. The presence of two different time scales enables to carry out a fairly complete analysis of the problem. This is done by treating consumers and resources in the coupled system as fast-scale and slow-scale variables respectively and subsequently considering developments in phase planes of these variables, fast and slow, as if they are independent. When uncoupled, each pair has unique asymptotically stable steady state and no self-sustained oscillatory behavior (although damped oscillations about the equilibrium are admitted). When the consumer-resource pairs are weakly coupled through direct reciprocal inhibition of consumers, the whole system exhibits self-sustained relaxation oscillations with a period that can be significantly longer than intrinsic relaxation time of either pair. It is shown that the model equations adequately describe locally linked consumer-resource systems of quite different nature: living populations under interspecific interference competition and lasers coupled via their cavity losses.",
    "Wireless local area networks (WLAN) still suffer from a severe performance discrepancy between different users in the uplink. This is because of the spatially varying channel conditions provided by the wireless medium. Cooperative medium access control (MAC) protocols as for example CoopMAC were proposed to mitigate this problem. In this work, it is shown that cooperation implies for cooperating nodes a tradeoff between throughput and bit-cost, which is the energy needed to transmit one bit. The tradeoff depends on the degree of cooperation. For carrier sense multiple access (CSMA) based networks, the throughput/bit-cost tradeoff curve is theoretically derived. A new distributed CSMA protocol called fairMAC is proposed and it is theoretically shown that fairMAC can asymptotically achieve any operating point on the tradeoff curve when the packet lengths go to infinity. The theoretical results are validated through Monte Carlo simulations.",
    "Social tagging has become an interesting approach to improve search and navigation over the actual Web, since it aggregates the tags added by different users to the same resource in a collaborative way. This way, it results in a list of weighted tags describing its resource. Combined to a classical taxonomic classification system such as that by Wikipedia, social tags can enhance document navigation and search. On the one hand, social tags suggest alternative navigation ways, including pivot-browsing, popularity-driven navigation, and filtering. On the other hand, it provides new metadata, sometimes uncovered by documents' content, that can substantially improve document search. In this work, the inclusion of an interface to add user-defined tags describing Wikipedia articles is proposed, as a way to improve article navigation and retrieval. As a result, a prototype on applying tags over Wikipedia is proposed in order to evaluate its effectiveness.",
    "Quantum Computing and especially Quantum Machine Learning, in a short period of time, has gained a lot of interest through research groups around the world. This can be seen in the increasing number of proposed models for pattern classification applying quantum principles to a certain degree. Despise the increasing volume of models, there is a void in testing these models on real datasets and not only on synthetic ones. The objective of this work is to classify patterns with binary attributes using a quantum classifier. Specially, we show results of a complete quantum classifier applied to image datasets. The experiments show favorable output while dealing with balanced classification problems as well as with imbalanced classes where the minority class is the most relevant. This is promising in medical areas, where usually the important class is also the minority class.",
    "We present near- and mid-infrared observations on the shock-cloud interaction region in the southern part of the supernova remnant HB 21, performed with the InfraRed Camera (IRC) aboard AKARI satellite and the Wide InfraRed Camera (WIRC) at the Palomar 5 m telescope. The IRC 4 um (N4), 7 um (S7), and 11 um (S11) band images and the WIRC H2 v=1->0 S(1) 2.12 um image show similar diffuse features, around a shocked CO cloud. We analyzed the emission through comparison with the H2 line emission of several shock models. The IRC colors are well explained by the thermal admixture model of H2 gas--whose infinitesimal H2 column density has a power-law relation with the temperature $T$, $dN\\sim T^{-b}dT$--with n(H2) $\\sim3.9\\times10^4$ cm^{-2}, $b\\sim4.2$, and N(H2;T>100K) $\\sim2.8\\times10^{21}$ cm^{-2}. We interpreted these parameters with several different pictures of the shock-cloud interactions--multiple planar C-shocks, bow shocks, and shocked clumps--and discuss their weaknesses and strengths. The observed H2 v=1->0 S(1) intensity is four times greater than the prediction from the power-law admixture model, the same tendency as found in the northern part of HB 21 (Paper I). We also explored the limitation of the thermal admixture model with respect to the derived model parameters.",
    "Recently, vision transformers started to show impressive results which outperform large convolution based models significantly. However, in the area of small models for mobile or resource constrained devices, ConvNet still has its own advantages in both performance and model complexity. We propose ParC-Net, a pure ConvNet based backbone model that further strengthens these advantages by fusing the merits of vision transformers into ConvNets. Specifically, we propose position aware circular convolution (ParC), a light-weight convolution op which boasts a global receptive field while producing location sensitive features as in local convolutions. We combine the ParCs and squeeze-exictation ops to form a meta-former like model block, which further has the attention mechanism like transformers. The aforementioned block can be used in plug-and-play manner to replace relevant blocks in ConvNets or transformers. Experiment results show that the proposed ParC-Net achieves better performance than popular light-weight ConvNets and vision transformer based models in common vision tasks and datasets, while having fewer parameters and faster inference speed. For classification on ImageNet-1k, ParC-Net achieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11% parameters and 13% computational cost but gaining 0.2% higher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288) compared with MobileViT, and uses only 0.5 times parameters but gaining 2.7% accuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC segmentation tasks, ParC-Net also shows better performance. Source code is available at https://github.com/hkzhang91/ParC-Net",
    "From the algebraic solution of $x^{n}-x+t=0$ for $n=2,3,4$ and the corresponding solution in terms of hypergeometric functions, we obtain a set of reduction formulas for hypergeometric functions. By differentiation and integration of these results, and applying other known reduction formulas of hypergeometric functions, we derive new reduction formulas of special functions as well as the calculation of some infinite integrals in terms of elementary functions.",
    "Air-gap covert channels are special types of covert communication channels that enable attackers to exfiltrate data from isolated, network-less computers. Various types of air-gap covert channels have been demonstrated over the years, including electromagnetic, magnetic, acoustic, optical, and thermal. In this paper, we introduce a new type of vibrational (seismic) covert channel. We observe that computers vibrate at a frequency correlated to the rotation speed of their internal fans. These inaudible vibrations affect the entire structure on which the computer is placed. Our method is based on malware's capability of controlling the vibrations generated by a computer, by regulating its internal fan speeds. We show that the malware-generated covert vibrations can be sensed by nearby smartphones via the integrated, sensitive \\textit{accelerometers}. Notably, the accelerometer sensors in smartphones can be accessed by any app without requiring the user permissions, which make this attack highly evasive. We implemented AiR-ViBeR, malware that encodes binary information, and modulate it over a low frequency vibrational carrier. The data is then decoded by malicious application on a smartphone placed on the same surface (e.g., on a desk). We discuss the attack model, provide technical background, and present the implementation details and evaluation results. Our results show that using AiR-ViBeR, data can be exfiltrated from air-gapped computer to a nearby smartphone on the same table, or even an adjacent table, via vibrations. Finally, we propose a set of countermeasures for this new type of attack.",
    "The total cost of a 25 W average load magnetic refrigerator using commercial grade Gd is calculated using a numerical model. The price of magnetocaloric material, magnet material and cost of operation are considered, and all influence the total cost. The lowest combined total cost with a device lifetime of 15 years is found to be in the range \\$150-\\$400 depending on the price of the magnetocaloric and magnet material. The cost of the magnet is largest, followed closely by the cost of operation, while the cost of the magnetocaloric material is almost negligible. For the lowest cost device, the optimal magnetic field is about 1.4 T, the particle size is 0.23 mm, the length of the regenerator is 40-50 mm and the utilization is about 0.2, for all device lifetimes and material and magnet prices, while the operating frequency vary as function of device lifetime. The considered performance characteristics are based on the performance of a conventional A$^{+++}$ refrigeration unit. In a rough life time cost comparison between the magnetic refrigeration device and such a unit we find similar costs, the former being slightly cheaper, assuming the cost of the magnet can be recuperated at end of life.",
    "We prove the existence of initial data sets which possess an asymptotically flat and an asymptotically cylindrical end. Such geometries are known as trumpets in the community of numerical relativists.",
    "Enzymes utilize protein architectures to create highly specialized structural motifs that can greatly enhance the rates of complex chemical transformations. Here we use experiments, combined with ab initio simulations that exactly include nuclear quantum effects, to show that a triad of strongly hydrogen bonded tyrosine residues within the active site of the enzyme ketosteroid isomerase (KSI) facilitates quantum proton delocalization. This delocalization dramatically stabilizes the deprotonation of an active site tyrosine residue, resulting in a very large isotope effect on its acidity. When an intermediate analog is docked, it is incorporated into the hydrogen bond network, giving rise to extended quantum proton delocalization in the active site. These results shed light on the role of nuclear quantum effects in the hydrogen bond network that stabilizes the reactive intermediate of KSI, and the behavior of protons in biological systems containing strong hydrogen bonds.",
    "In this work, we propose ENSEI, a secure inference (SI) framework based on the frequency-domain secure convolution (FDSC) protocol for the efficient execution of privacy-preserving visual recognition. Our observation is that, under the combination of homomorphic encryption and secret sharing, homomorphic convolution can be obliviously carried out in the frequency domain, significantly simplifying the related computations. We provide protocol designs and parameter derivations for number-theoretic transform (NTT) based FDSC. In the experiment, we thoroughly study the accuracy-efficiency trade-offs between time- and frequency-domain homomorphic convolution. With ENSEI, compared to the best known works, we achieve 5--11x online time reduction, up to 33x setup time reduction, and up to 10x reduction in the overall inference time. A further 33% of bandwidth reductions can be obtained on binary neural networks with only 1% of accuracy degradation on the CIFAR-10 dataset.",
    "Recommender systems benefit us in tackling the problem of information overload by predicting our potential choices among diverse niche objects. So far, a variety of personalized recommendation algorithms have been proposed and most of them are based on similarities, such as collaborative filtering and mass diffusion. Here, we propose a novel vertex similarity index named CosRA, which combines advantages of both the cosine index and the resource-allocation (RA) index. By applying the CosRA index to real recommender systems including MovieLens, Netflix and RYM, we show that the CosRA-based method has better performance in accuracy, diversity and novelty than some benchmark methods. Moreover, the CosRA index is free of parameters, which is a significant advantage in real applications. Further experiments show that the introduction of two turnable parameters cannot remarkably improve the overall performance of the CosRA index.",
    "Many real-world applications adopt multi-label data streams as the need for algorithms to deal with rapidly changing data increases. Changes in data distribution, also known as concept drift, cause the existing classification models to rapidly lose their effectiveness. To assist the classifiers, we propose a novel algorithm called Label Dependency Drift Detector (LD3), an implicit (unsupervised) concept drift detector using label dependencies within the data for multi-label data streams. Our study exploits the dynamic temporal dependencies between labels using a label influence ranking method, which leverages a data fusion algorithm and uses the produced ranking to detect concept drift. LD3 is the first unsupervised concept drift detection algorithm in the multi-label classification problem area. In this study, we perform an extensive evaluation of LD3 by comparing it with 14 prevalent supervised concept drift detection algorithms that we adapt to the problem area using 12 datasets and a baseline classifier. The results show that LD3 provides between 19.8\\% and 68.6\\% better predictive performance than comparable detectors on both real-world and synthetic data streams.",
    "The universality of the Cepheid Period-Luminosity relations has been under discussion since metallicity effects have been assumed to play a role in the value of the intercept and, more recently, of the slope of these relations. The goal of the present study is to calibrate the Galactic PL relations in various photometric bands (from B to K) and to compare the results to the well-established PL relations in the LMC. We use a set of 59 calibrating stars, the distances of which are measured using five different distance indicators: Hubble Space Telescope and revised Hipparcos parallaxes, infrared surface brightness and interferometric Baade-Wesselink parallaxes, and classical Zero-Age-Main-Sequence-fitting parallaxes for Cepheids belonging to open clusters or OB stars associations. A detailed discussion of absorption corrections and projection factor to be used is given. We find no significant difference in the slopes of the PL relations between LMC and our Galaxy. We conclude that the Cepheid PL relations have universal slopes in all photometric bands, not depending on the galaxy under study (at least for LMC and Milky Way). The possible zero-point variation with metal content is not discussed in the present work, but an upper limit of 18.50 for the LMC distance modulus can be deduced from our data.",
    "Ensembling methods are well known for improving prediction accuracy. However, they are limited in the sense that they cannot discriminate among component models effectively. In this paper, we propose stacking with auxiliary features that learns to fuse relevant information from multiple systems to improve performance. Auxiliary features enable the stacker to rely on systems that not just agree on an output but also the provenance of the output. We demonstrate our approach on three very different and difficult problems -- the Cold Start Slot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet object detection tasks. We obtain new state-of-the-art results on the first two tasks and substantial improvements on the detection task, thus verifying the power and generality of our approach.",
    "We present a simple and fast method to simulate spin-torque driven magnetisation dynamics in nano-pillar spin-valve structures. The approach is based on the coupling between a spin transport code based on random matrix theory and a micromagnetics finite-elements software. In this way the spatial dependence of both spin transport and magnetisation dynamics is properly taken into account. Our results are compared with experiments. The excitation of the spin-wave modes, in- cluding the threshold current for steady state magnetisation precession and the nonlinear frequency shift of the modes are reproduced correctly. The giant magneto resistance effect and the magnetisa- tion switching also agree with experiment. The similarities with recently described spin-caloritronics devices are also discussed.",
    "We present a simple framework to compute hyperbolic Voronoi diagrams of finite point sets as affine diagrams. We prove that bisectors in Klein's non-conformal disk model are hyperplanes that can be interpreted as power bisectors of Euclidean balls. Therefore our method simply consists in computing an equivalent clipped power diagram followed by a mapping transformation depending on the selected representation of the hyperbolic space (e.g., Poincar\\'e conformal disk or upper-plane representations). We discuss on extensions of this approach to weighted and $k$-order diagrams, and describe their dual triangulations. Finally, we consider two useful primitives on the hyperbolic Voronoi diagrams for designing tailored user interfaces of an image catalog browsing application in the hyperbolic disk: (1) finding nearest neighbors, and (2) computing smallest enclosing balls.",
    "The problem of diffusive bond-dissociation in a double well potential under application of an external force is scrutinized. We compute the probability distribution of rupture forces and present a detailed discussion of the influence of finite rebinding probabilities on the dynamic force spectrum. In particular, we focus on barrier crossing upon extension, i.e. under linearly increased load, and upon relaxation starting from completely separated bonds. For large loading rates the rupture force and the rejoining force depend on the loading rate in the expected manner determined by the shape of the potential. For small loading rates the mean forces obtained from pull and relax modes approach each other as the system reaches equilibrium. We investigate the dependence of the rupture force distributions and mean rupture forces on external parameters like cantilever stiffness and influence of a soft linker. We find that depending on the implementation of a soft linker the equilibrium rupture force is either unaffected by the presence of the linker or changes in a predictable way with the linker-compliance. Additionally, we show that it is possible to extract the equilibrium constant of the on- and off-rates from the determination of the equilibrium rupture forces.",
    "We propose an invariant feature space for the detection of viscous dominated and turbulent regions (i.e., boundary layers and wakes). The developed methodology uses the principal invariants of the strain and rotational rate tensors as input to an unsupervised Machine Learning Gaussian mixture model. The selected feature space is independent of the coordinate frame used to generate the processed data, as it relies on the principal invariants of strain and rotational rate, which are Galilean invariants. This methodology allows us to identify two distinct flow regions: a viscous dominated, rotational region (boundary layer and wake region) and an inviscid, irrotational region (outer flow region). We test the methodology on a laminar and a turbulent (using Large Eddy Simulation) case for flows past a circular cylinder at $Re=40$ and $Re=3900$. The simulations have been conducted using a high-order nodal Discontinuous Galerkin Spectral Element Method (DGSEM). The results obtained are analysed to show that Gaussian mixture clustering provides an effective identification method of viscous dominated and rotational regions in the flow. We also include comparisons with traditional sensors to show that the proposed clustering does not depend on the selection of an arbitrary threshold, as required when using traditional sensors.",
    "Engineered quantum systems allow us to observe phenomena that are not easily accessible naturally. The LEGO-like nature of superconducting circuits makes them particularly suited for building and coupling artificial atoms. Here, we introduce an artificial molecule, composed of two strongly coupled fluxonium atoms, which possesses a tunable magnetic moment. Using an applied external flux, one can tune the molecule between two regimes: one in which the ground-excited state manifold has a magnetic dipole moment and one in which the ground-excited state manifold has only a magnetic quadrupole moment. By varying the applied external flux, we find the coherence of the molecule to be limited by local flux noise. The ability to engineer and control artificial molecules paves the way for building more complex circuits for protected qubits and quantum simulation.",
    "We describe a method for time-critical decision making involving sequential tasks and stochastic processes. The method employs several iterative refinement routines for solving different aspects of the decision making problem. This paper concentrates on the meta-level control problem of deliberation scheduling, allocating computational resources to these routines. We provide different models corresponding to optimization problems that capture the different circumstances and computational strategies for decision making under time constraints. We consider precursor models in which all decision making is performed prior to execution and recurrent models in which decision making is performed in parallel with execution, accounting for the states observed during execution and anticipating future states. We describe algorithms for precursor and recurrent models and provide the results of our empirical investigations to date.",
    "The role model strategy is introduced as a method for designing an estimator by approaching the output of a superior estimator that has better input observations. This strategy is shown to yield the optimal Bayesian estimator when a Markov condition is fulfilled. Two examples involving simple channels are given to illustrate its use. The strategy is combined with time averaging to construct a statistical model by numerically solving a convex program. The role model strategy was developed in the context of low complexity decoder design for iterative decoding. Potential applications outside the field of communications are discussed.",
    "Computer vision systems currently lack the ability to reliably recognize artistically rendered objects, especially when such data is limited. In this paper, we propose a method for recognizing objects in artistic modalities (such as paintings, cartoons, or sketches), without requiring any labeled data from those modalities. Our method explicitly accounts for stylistic domain shifts between and within domains. To do so, we introduce a complementary training modality constructed to be similar in artistic style to the target domain, and enforce that the network learns features that are invariant between the two training modalities. We show how such artificial labeled source domains can be generated automatically through the use of style transfer techniques, using diverse target images to represent the style in the target domain. Unlike existing methods which require a large amount of unlabeled target data, our method can work with as few as ten unlabeled images. We evaluate it on a number of cross-domain object and scene classification tasks and on a new dataset we release. Our experiments show that our approach, though conceptually simple, significantly improves the accuracy that existing domain adaptation techniques obtain for artistic object recognition.",
    "This paper studies the large time behavior of solutions to semi-linear Cauchy problems with quadratic nonlinearity in gradients. The Cauchy problem considered has a general state space and may degenerate on the boundary of the state space. Two types of large time behavior are obtained: i) pointwise convergence of the solution and its gradient; ii) convergence of solutions to associated backward stochastic differential equations. When the state space is R^d or the space of positive definite matrices, both types of convergence are obtained under growth conditions on model coefficients. These large time convergence results have direct applications in risk sensitive control and long term portfolio choice problems.",
    "The decaying vacuum model (DV), treating dark energy as a varying vacuum, has been studied well recently. The vacuum energy decays linearly with the Hubble parameter in the late-times, $\\rho_\\Lambda(t) \\propto H(t)$, and produces the additional matter component. We constrain the parameters of the DV model using the recent data-sets from supernovae, gamma-ray bursts, baryon acoustic oscillations, CMB, the Hubble rate and x-rays in galaxy clusters. It is found that the best fit of matter density contrast $\\Omega_m$ in the DV model is much lager than that in $\\Lambda$CDM model. We give the confidence contours in the $\\Omega_m-h$ plane up to $3\\sigma$ confidence level. Besides, the normalized likelihoods of $\\Omega_m$ and $h$ are presented, respectively. %",
    "Perpendicular MgO-based Magnetic Tunnel Junctions are optimal candidates as building block of Spin Transfer Torque (STT) magnetoresistive memories. However, up to now, the only STT is not enough to achieve switching current density below 106 A/cm2. A recent work [Wang et al., Nature Mater., vol. 11, pp 64-68, Jan. 2012] has experimentally demonstrated the possibility to perform magnetization switching assisted by an electric-field at ultra-low current density. Theoretically, this switching has been studied by using a macrospin approach only. Here, we show a full micromagnetic study. We found that the switching occurs via a complex nucleation process including the nucleation of magnetic vortexes.",
    "We present a generalisation of Rosenblatt's traditional perceptron learning algorithm to the class of proximal activation functions and demonstrate how this generalisation can be interpreted as an incremental gradient method applied to a novel energy function. This novel energy function is based on a generalised Bregman distance, for which the gradient with respect to the weights and biases does not require the differentiation of the activation function. The interpretation as an energy minimisation algorithm paves the way for many new algorithms, of which we explore a novel variant of the iterative soft-thresholding algorithm for the learning of sparse perceptrons.",
    "The radiation force exerted on an object by an acoustic wave is a widely studied phenomenon since the early work of Rayleigh, Langevin and Brillouin and has led in the last decade to tremendous developments for acoustic micromanipulation. Despite extensive work on this phenomenon, the expressions of the acoustic radiation force applied on a particle have so far been derived only for a steady particle, hence neglecting the effect of its displacement on the radiated wave. In this work we study the acoustic radiation force exerted on a monopolar source translating at a constant velocity small compared to the sound speed. We demonstrate that the asymmetry of the emitted field resulting from Doppler effect induces a radiation force on the source opposite to its motion.",
    "Modelling the base of the solar convective envelope is a tedious problem. Since the first rotation inversions, solar modellers are confronted with the fact that a region of very limited extent has an enormous physical impact on the Sun. Indeed, it is the transition region from differential to solid body rotation, the tachocline, which furthermore is influenced by turbulence and is also supposed to be the seat of the solar magnetic dynamo. Moreover, solar models show significant disagreement with the sound speed profile in this region. In this paper, we show how helioseismology can provide further constraints on this region by carrying out an inversion of the Ledoux discriminant. We compare these inversions for Standard Solar Models built using various opacity tables and chemical abundances and discuss the origins of the discrepancies between Solar Models and the Sun.",
    "There is a clear desire to model and comprehend human behavior. Trends in research covering this topic show a clear assumption that many view human reasoning as the presupposed standard in artificial reasoning. As such, topics such as game theory, theory of mind, machine learning, etc. all integrate concepts which are assumed components of human reasoning. These serve as techniques to attempt to both replicate and understand the behaviors of humans. In addition, next generation autonomous and adaptive systems will largely include AI agents and humans working together as teams. To make this possible, autonomous agents will require the ability to embed practical models of human behavior, which allow them not only to replicate human models as a technique to \"learn\", but to to understand the actions of users and anticipate their behavior, so as to truly operate in symbiosis with them. The main objective of this paper it to provide a succinct yet systematic review of the most important approaches in two areas dealing with quantitative models of human behaviors. Specifically, we focus on (i) techniques which learn a model or policy of behavior through exploration and feedback, such as Reinforcement Learning, and (ii) directly model mechanisms of human reasoning, such as beliefs and bias, without going necessarily learning via trial-and-error.",
    "Breaking down botnets have always been a big challenge. The robustness of C&C channels is increased, and the detection of botmaster is harder in P2P botnets. In this paper, we propose a probabilistic method to reconstruct the topologies of the C&C channel for P2P botnets. Due to the geographic dispersion of P2P botnet members, it is not possible to supervise all members, and there does not exist all necessary data for applying other graph reconstruction methods. So far, no general method has been introduced to reconstruct C&C channel topology for all type of P2P botnet. In our method, the probability of connections between bots is estimated by using the inaccurate receiving times of several cascades, network model parameters of C&C channel, and end-to-end delay distribution of the Internet. The receiving times can be collected by observing the external reaction of bots to commands. The results of our simulations show that more than 90% of the edges in a 1000-member network with node degree mean 50, have been accurately estimated by collecting the inaccurate receiving times of 22 cascades. In case the receiving times of just half of the bots are collected, this accuracy of estimation is obtained by using 95 cascades.",
    "In grand unified theories (GUT), non-universal boundary conditions for the gaugino masses may arise at the unification scale, and affect the observability of the neutral MSSM Higgs bosons (h/H/A) at the LHC. The implications of such non-universal gaugino masses are investigated for the Higgs boson production in the SUSY cascade decay chain gluino --> squark quark, squark --> neutralino_2 quark, neutralino_2 --> neutralino_1 h/H/A, h/H/A --> b b-bar produced in pp interactions. In the singlet representation with universal gaugino masses only the light Higgs boson can be produced in this cascade with the parameter region of interest for us, while with non-universal gaugino masses heavy neutral MSSM Higgs boson production may dominate. The allowed parameter space in the light of the WMAP constraints on the cold dark matter relic density is investigated in the above scenarios for gaugino mass parameters. We also demonstrate that combination of representations can give the required amount of dark matter in any point of the parameter space. In the non-universal case we show that heavy Higgs bosons can be detected in the studied cascade in parameter regions with the WMAP preferred neutralino relic density.",
    "Subwavelength modulators play an indispensable role in integrated photonic-electronic circuits. Due to weak light-matter interactions, it is always a challenge to develop a modulator with a nanometer scale footprint, low switching energy, low insertion loss and large modulation depth. In this paper, we propose the design of a vanadium dioxide dual-mode plasmonic waveguide electroabsorption modulator using a metal-insulator-VO$_2$-insulator-metal (MIVIM) waveguide platform. By varying the index of vanadium dioxide, the modulator can route plasmonic waves through the low-loss dielectric insulator layer during the \"on\" state and high-loss VO$_2$ layer during the \"off\" state, thereby significantly reducing the insertion loss while maintaining a large modulation depth. This ultracompact waveguide modulator, for example, can achieve a large modulation depth of ~10dB with an active size of only 200x50x220nm$^3$ (or ~{\\lambda}$^3$/1700), requiring a drive-voltage of ~4.6V. This high performance plasmonic modulator could potentially be one of the keys towards fully-integrated plasmonic nanocircuits in the next-generation chip technology.",
    "As a car becomes more connected, a countermeasure against automobile theft has become a significant task in the real world. To respond to automobile theft, data mining, biometrics, and additional authentication methods are proposed. Among current countermeasures, data mining method is one of the efficient ways to capture the owner driver's unique characteristics. To identify the owner driver from thieves, previous works applied various algorithms toward driving data. Such data mining methods utilized supervised learning, thus required labeled data set. However, it is unrealistic to gather and apply the thief's driving pattern. To overcome this problem, we propose driver identification method with GAN. GAN has merit to build identification model by learning the owner driver's data only. We trained GAN only with owner driver's data and used trained discriminator to identify the owner driver. From actual driving data, we evaluated our identification model recognizes the owner driver well. By ensembling various driver authentication methods with the proposed model, we expect industry can develop automobile theft countermeasures available in the real world.",
    "Slow oscillations (SlO) of magnetoresistance is a convenient tool to measure electronic structure parameters in quasi-two-dimensional metals. We study the possibility to apply this method to multi-band conductors, e.g. to iron-based high-temperature superconducting materials. We show that SlO can be used to measure the interlayer transfer integral in multi-band conductors similar to single-band metals. In addition, the SlO allow to measure and compare the effective masses or the electron scattering rates in various bands.",
    "Recent progress in the field of precision calculations for Standard Model processes at the LHC is reviewed, highlighting examples of weak gauge-boson and Higgs-boson production, as discussed at the 27th Rencontres de Blois, 2015.",
    "This paper proposes a speech emotion recognition method based on speech features and speech transcriptions (text). Speech features such as Spectrogram and Mel-frequency Cepstral Coefficients (MFCC) help retain emotion-related low-level characteristics in speech whereas text helps capture semantic meaning, both of which help in different aspects of emotion detection. We experimented with several Deep Neural Network (DNN) architectures, which take in different combinations of speech features and text as inputs. The proposed network architectures achieve higher accuracies when compared to state-of-the-art methods on a benchmark dataset. The combined MFCC-Text Convolutional Neural Network (CNN) model proved to be the most accurate in recognizing emotions in IEMOCAP data.",
    "In this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efficiently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the benefit of variational semantic memory in boosting few-shot recognition.",
    "The relativistic four-quark equations with the open-charm and the open-strange are found in the framework of coupled-channel formalism. The dynamical mixing of the meson-meson states with the four-quark states is considered. The four-quark amplitudes including the quarks of four flavors (u, d, s, c) are constructed. The poles of these amplitudes determine the masses of tetraquarks. The mass values of the tetraquarks with the spin-parity JP=1-,2- are calculated.",
    "The Fisher Matrix is the backbone of modern cosmological forecasting. We describe the Fisher4Cast software: a general-purpose, easy-to-use, Fisher Matrix framework. It is open source, rigorously designed and tested and includes a Graphical User Interface (GUI) with automated LATEX file creation capability and point-and-click Fisher ellipse generation. Fisher4Cast was designed for ease of extension and, although written in Matlab, is easily portable to open-source alternatives such as Octave and Scilab. Here we use Fisher4Cast to present new 3-D and 4-D visualisations of the forecasting landscape and to investigate the effects of growth and curvature on future cosmological surveys. Early releases have been available at http://www.cosmology.org.za since May 2008 with 750 downloads in the first year. Version 2.2 is made public with this paper and includes a Quick Start guide and the code used to produce the figures in this paper, in the hope that it will be useful to the cosmology and wider scientific communities.",
    "The representation of knowledge based on first-order logic captures the richness of natural language and supports multiple probabilistic inference models. Although symbolic representation enables quantitative reasoning with statistical probability, it is difficult to utilize with machine learning models as they perform numerical operations. In contrast, knowledge embedding (i.e., high-dimensional and continuous vectors) is a feasible approach to complex reasoning that can not only retain the semantic information of knowledge but also establish the quantifiable relationship among them. In this paper, we propose recursive neural knowledge network (RNKN), which combines medical knowledge based on first-order logic with recursive neural network for multi-disease diagnosis. After RNKN is efficiently trained from manually annotated Chinese Electronic Medical Records (CEMRs), diagnosis-oriented knowledge embeddings and weight matrixes are learned. Experimental results verify that the diagnostic accuracy of RNKN is superior to that of some classical machine learning models and Markov logic network (MLN). The results also demonstrate that the more explicit the evidence extracted from CEMRs is, the better is the performance achieved. RNKN gradually exhibits the interpretation of knowledge embeddings as the number of training epochs increases.",
    "Several solenoids are usually installed in electron cooler device to guide the motion of the electron beam in the cooler. However, the solenoids also have influence to the ion beam in the cooler storage ring. The transverse motion of the ion beam in storage ring will become coupled, if the solenoids installed in the electron cooler are not compensated perfectly. In this paper, the coupled transverse motion due to the uncompensated cooler's solenoids of CSRm (The main storage ring in the IMP, Lan Zhou, China) is studied, and the coupled beam envelopes are calculated by a new method.",
    "A near-infrared excess is detected at the white dwarf PHL5038 in UKIDSS photometry, consistent with the presence of a cool, substellar companion. We have obtained H- and K-grism spectra and images of PHL5038 using NIRI on Gemini North. The target is spatially and spectrally resolved into two components; an 8000K DA white dwarf, and a likely L8 brown dwarf companion, separated by 0.94\". The spectral type of the secondary was determined using standard spectral indices for late L and T dwarfs. The projected orbital separation of the binary is 55AU, and so it becomes only the second known wide WD+dL binary to be found after GD165AB. This object could potentially be used as a benchmark for testing substellar evolutionary models at intermediate to older ages.",
    "We investigate the impact of dynamical streams and substructure on estimates of the local escape speed and total mass of Milky Way-mass galaxies from modelling the high velocity tail of local halo stars. We use a suite of high-resolution, magneto-hydrodynamical cosmological zoom-in simulations, which resolve phase space substructure in local volumes around solar-like positions. We show that phase space structure varies significantly between positions in individual galaxies and across the suite. Substructure populates the high velocity tail unevenly and leads to discrepancies in the mass estimates. We show that a combination of streams, sample noise and truncation of the high velocity tail below the escape speed leads to a distribution of mass estimates with a median that falls below the true value by $\\sim 20 \\%$, and a spread of a factor of 2 across the suite. Correcting for these biases, we derive a revised value for the Milky Way mass presented in Deason et al. of $1.29 ^{+0.37}_{-0.47} \\times 10^{12}$ $\\rm M_{\\odot}$.",
    "Using no conventional measurements in position space, information extraction rates exceeding one bit per photon are achieved by employing high-dimensional correlated orbital angular momentum (OAM) states for object recognition. The correlations are shown to be insensitive to axial rotation of the target object: the information structure of an object's joint OAM coincidence spectrum is unchanged even when the object undergoes random rotations between each measurement. Additionally, OAM correlations alone are shown to be sufficient for full image reconstruction of complex, off-axis objects, and novel object symmetries are observed in the phases of OAM-object interaction transition amplitudes. Variations in mutual information rates, due to off-axis translation in the beam field, are studied, and it is shown that object symmetry signatures and information rates are independent of environmental factors sufficiently far from the beam center. The results motivate dynamic scanning applications in contexts where symmetry and small numbers of noninvasive measurements are desired.",
    "During Parker Solar Probe's first two orbits there are widespread observations of rapid magnetic field reversals known as switchbacks. These switchbacks are extensively found in the near-Sun solar wind, appear to occur in patches, and have possible links to various phenomena such as magnetic reconnection near the solar surface. As switchbacks are associated with faster plasma flows, we questioned whether they are hotter than the background plasma and whether the microphysics inside a switchback is different to its surroundings. We have studied the reduced distribution functions from the Solar Probe Cup instrument and considered time periods with markedly large angular deflections, to compare parallel temperatures inside and outside switchbacks. We have shown that the reduced distribution functions inside switchbacks are consistent with a rigid phase space rotation of the background plasma. As such, we conclude that the proton core parallel temperature is the same inside and outside of switchbacks, implying that a T-V relationship does not hold for the proton core parallel temperature inside magnetic field switchbacks. We further conclude that switchbacks are consistent with Alfv\\'enic pulses travelling along open magnetic field lines. The origin of these pulses, however, remains unknown. We also found that there is no obvious link between radial Poynting flux and kinetic energy enhancements suggesting that the radial Poynting flux is not important for the dynamics of switchbacks.",
    "Under the Nainital-Cape Survey, eight $\\delta\\,$Scuti type pulsators have been discovered with the pulsation periods in the range of several minutes to few hours. In order to understand these observed pulsational variabilities, we have performed non-adiabatic linear stability analyses in models of these stars having mass in the range of 1 to 3 M$_{\\odot}$. Several low order p-modes are found to be unstable where the pulsation periods associated with these unstable modes are in good agreement with the observed periods. Particularly for HD$\\,$118660, HD$\\,$113878, HD$\\,$102480, HD$\\,$98851, and HD$\\,$25515, we demonstrate that the observed variabilities can be explained with the low order radial p-mode pulsations.",
    "A new perspective on the classical mechanical formulation of particle trajectories in lorentz-violating theories is presented. Using the extended hamiltonian formalism, a Legendre Transformation between the associated covariant Lagrangian and Hamiltonian varieties is constructed. This approach enables calculation of trajectories using hamilton's equations in momentum space and the Euler-Lagrange equations in velocity space away from certain singular points that arise in the theory. Singular points are naturally de-singularized by requiring the trajectories to be smooth functions of both velocity and momentum variables. In addition, it is possible to identify specific sheets of the dispersion relations that correspond to specific solutions for the lagrangian. Examples corresponding to bipartite Finsler functions are computed in detail. A direct connection between the lagrangians and the field-theoretic solutions to the Dirac equation is also established for a special case.",
    "Spectrum management has been identified as a crucial step towards enabling the technology of a cognitive radio network (CRN). Most of the current works dealing with spectrum management in the CRN focus on a single task of the problem, e.g., spectrum sensing, spectrum decision, spectrum sharing or spectrum mobility. In this two-part paper, we argue that for certain network configurations, jointly performing several tasks of the spectrum management improves the spectrum efficiency. Specifically, our aim is to study the uplink resource management problem in a CRN where there exist multiple cognitive users (CUs) and access points (APs). The CUs, in order to maximize their uplink transmission rates, have to associate to a suitable AP (spectrum decision), and to share the channels used by this AP with other CUs (spectrum sharing). These tasks are clearly interdependent, and the problem of how they should be carried out efficiently and in a distributed manner is still open in the literature.",
    "A simple, exactly solvable statistical model is presented for the description of baryonic matter in the thermodynamic conditions associated to the evolution of core-collapsing supernova. It is shown that the model presents a first order phase transition in the grandcanonical ensemble which is not observed in the canonical ensemble. Similar to other model systems studied in condensed matter physics, this ensemble in-equivalence is accompanied by negative susceptibility and discontinuities in the intensive observables conjugated to the order parameter. This peculiar behavior originates from the fact that baryonic matter is subject to attractive short range strong forces as well as repulsive long range electromagnetic interactions, partially screened by a background of electrons. As such, it is expected in any theoretical treatment of nuclear matter in the stellar environment. Consequences for the phenomenology of supernova dynamics are drawn.",
    "To meet requirements of high performance THz-FEL (Free Electron Laser), a compact scheme of FEL injector was proposed. Thermionic cathode was chosen to emit electrons instead of photo-cathode with complex structure and high cost. The effective bunch charge was improved to ~200pC by adopting enhanced EC-ITC (External Cathode Independently Tunable Cells) RF gun to extract micro-bunches, and back bombardment effects were almost eliminated as well. Constant gradient accelerator structures were designed to improve energy to ~14MeV, while focusing system was applied for emittance suppressing and bunch state maintenance. Physical design and beam dynamics of key components for FEL injector were analyzed. Furthermore, start-to-end simulations with multi-pulses were performed by using homemade MATLAB and Parmela. The results show that continual high brightness electron bunches with low energy spread and emittance could be obtained stably.",
    "Several claims have been made of anomalies in the large-angle properties of the cosmic microwave background anisotropy as measured by WMAP. In most cases, the statistical significance of these anomalies is hard or even impossible to assess, due to the fact that the statistics used to quantify the anomalies were chosen a posteriori. On the other hand, the possibility of detecting new physics on the largest observable scales is so exciting that, in my opinion, it is worthwhile to examine the claims carefully. I will focus on three particular claims: the lack of large-angle power, the north-south power asymmetry, and multipole alignments. In all cases, the problem of a posteriori statistics can best be solved by finding a new data set that probes similar physical scales to the large-angle CMB. This is a difficult task, but there are some possible routes to achieving it.",
    "Multi-photon states can be produced in multiple parametric down conversion (PDC) processes. The nonlinear crystal in such a case is pumped with high power. In theory, the more populated these states are, the deeper is the conflict with local realistic description. However, the interference contrast in multi-photon PDC experiments can be quite low for high pumping. We show how the contrast can be improved. The idea employs currently accessible optical devices, the multiport beam splitters. They are capable of splitting the incoming light in one input mode to $M$ output modes. Our scheme works as a POVM filter. It may provide a feasible CHSH-Bell inequality test, and thus can be useful in e.g. schemes reducing communication complexity.",
    "The spectrum of exponents of the transfer matrix provides the localization lengths of Anderson's model for a particle in a lattice with disordered potential. I show that a duality identity for determinants and Jensen's identity for subharmonic functions, give a formula for the spectrum in terms of eigenvalues of the Hamiltonian with non-Hermitian boundary conditions. The formula is exact; it involves an average over a Bloch phase, rather than disorder. A preliminary investigation of non-Hermitian spectra of Anderson's model in D=1,2 and on the smallest exponent is presented.",
    "In this article, we improve extreme learning machines for regression tasks using a graph signal processing based regularization. We assume that the target signal for prediction or regression is a graph signal. With this assumption, we use the regularization to enforce that the output of an extreme learning machine is smooth over a given graph. Simulation results with real data confirm that such regularization helps significantly when the available training data is limited in size and corrupted by noise.",
    "To properly describe heating in weakly collisional turbulent plasmas such as the solar wind, inter-particle collisions should be taken into account. Collisions can convert ordered energy into heat by means of irreversible relaxation towards the thermal equilibrium. Recently, Pezzi et al. (Phys. Rev. Lett., vol. 116, 2016, p. 145001) showed that the plasma collisionality is enhanced by the presence of fine structures in velocity space. Here, the analysis is extended by directly comparing the effects of the fully nonlinear Landau operator and a linearized Landau operator. By focusing on the relaxation towards the equilibrium of an out of equilibrium distribution function in a homogeneous force-free plasma, here it is pointed out that it is significant to retain nonlinearities in the collisional operator to quantify the importance of collisional effects. Although the presence of several characteristic times associated with the dissipation of different phase space structures is recovered in both the cases of the nonlinear and the linearized operators, the influence of these times is different in the two cases. In the linearized operator case, the recovered characteristic times are systematically larger than in the fully nonlinear operator case, this suggesting that fine velocity structures are dissipated slower if nonlinearities are neglected in the collisional operator.",
    "In this research work, we have demonstrated the application of Mask-RCNN (Regional Convolutional Neural Network), a deep-learning algorithm for computer vision and specifically object detection, to semiconductor defect inspection domain. Stochastic defect detection and classification during semiconductor manufacturing has grown to be a challenging task as we continuously shrink circuit pattern dimensions (e.g., for pitches less than 32 nm). Defect inspection and analysis by state-of-the-art optical and e-beam inspection tools is generally driven by some rule-based techniques, which in turn often causes to misclassification and thereby necessitating human expert intervention. In this work, we have revisited and extended our previous deep learning-based defect classification and detection method towards improved defect instance segmentation in SEM images with precise extent of defect as well as generating a mask for each defect category/instance. This also enables to extract and calibrate each segmented mask and quantify the pixels that make up each mask, which in turn enables us to count each categorical defect instances as well as to calculate the surface area in terms of pixels. We are aiming at detecting and segmenting different types of inter-class stochastic defect patterns such as bridge, break, and line collapse as well as to differentiate accurately between intra-class multi-categorical defect bridge scenarios (as thin/single/multi-line/horizontal/non-horizontal) for aggressive pitches as well as thin resists (High NA applications). Our proposed approach demonstrates its effectiveness both quantitatively and qualitatively.",
    "We give a new bound on the parameter $\\lambda$ (number of common neighbors of a pair of adjacent vertices) in a distance-regular graph $G$, improving and generalizing bounds for strongly regular graphs by Spielman (1996) and Pyber (2014). The new bound is one of the ingredients of recent progress on the complexity of testing isomorphism of strongly regular graphs (Babai, Chen, Sun, Teng, Wilmes 2013). The proof is based on a clique geometry found by Metsch (1991) under certain constraints on the parameters. We also give a simplified proof of the following asymptotic consequence of Metsch's result: if $k\\mu = o(\\lambda^2)$ then each edge of $G$ belongs to a unique maximal clique of size asymptotically equal to $\\lambda$, and all other cliques have size $o(\\lambda)$. Here $k$ denotes the degree and $\\mu$ the number of common neighbors of a pair of vertices at distance 2. We point out that Metsch's cliques are \"asymptotically Delsarte\" when $k\\mu = o(\\lambda^2)$, so families of distance-regular graphs with parameters satisfying $k\\mu = o(\\lambda^2)$ are \"asymptotically Delsarte-geometric.\"",
    "Recent galaxy observations show that star formation activity changes depending on galactic environments. In order to understand the diversity of galactic-scale star formation, it is crucial to understand the formation and evolution of giant molecular clouds in an extreme environment. We focus on observational evidence that bars in strongly barred galaxies lack massive stars even though quantities of molecular gas are sufficient to form stars. In this paper, we present a hydrodynamical simulation of a strongly barred galaxy, using a stellar potential which is taken from observational results of NGC1300, and we compare cloud properties between different galactic environments: bar, bar-end and spiral arms. We find that the mean of cloud's virial parameter is ~1 and that there is no environmental dependence, indicating that the gravitationally-bound state of a cloud is not behind the observational evidence of the lack of massive stars in strong bars. Instead, we focus on cloud-cloud collisions, which have been proposed as a triggering mechanism for massive star formation. We find that the collision speed in the bar is faster than those in the other regions. We examine the collision frequency using clouds' kinematics and conclude that the fast collisions in the bar could originate from random-like motion of clouds due to elliptical gas orbits shifted by the bar potential. These results suggest that the observed regions of lack of active star-formation in the strong bar originate from the fast cloud-cloud collisions, which are inefficient in forming massive stars, due to the galactic-scale violent gas motion.",
    "Context: The mass-metallicity relationship (MMR) of star-forming galaxies is well-established, however there is still some disagreement with respect to its exact shape and its possible dependence on other observables. Aims: We measure the MMR in the Galaxy And Mass Assembly (GAMA) survey. We compare our measured MMR to that measured in the Sloan Digital Sky Survey (SDSS) and study the dependence of the MMR on various selection criteria to identify potential causes for disparities seen in the literature. Methods: We use strong emission line ratio diagnostics to derive oxygen abundances. We then apply a range of selection criteria for the minimum signal-to-noise in various emission lines, as well as the apparent and absolute magnitude to study variations in the inferred MMR. Results: The shape and position of the MMR can differ significantly depending on the metallicity calibration and selection used. After selecting a robust metallicity calibration amongst those tested, we find that the mass-metallicity relation for redshifts 0.061< z<0.35 in GAMA is in reasonable agreement with that found in the SDSS despite the difference in the luminosity range probed. Conclusions: In view of the significant variations of the MMR brought about by reasonable changes in the sample selection criteria and method, we recommend that care be taken when comparing the MMR from different surveys and studies directly. We also conclude that there could be a modest level of evolution over 0.06<z<0.35 within the GAMA sample.",
    "Based on thermodynamic considerations we derive a set of equations relating the seepage velocities of the fluid components in immiscible and incompressible two-phase flow in porous media. They necessitate the introduction of a new velocity function, the co-moving velocity. This velocity function is a characteristic of the porous medium. Together with a constitutive relation between the velocities and the driving forces, such as the pressure gradient, these equations form a closed set. We solve four versions of the capillary tube model analytically using this theory. We test the theory numerically on a network model.",
    "Nature's spectacular inventiveness, reflected in the enormous diversity of form and function displayed by the biosphere, is a feature of life that distinguishes living most strongly from nonliving. It is, therefore, not surprising that this aspect of life should become a central focus of artificial life. We have known since Darwin that the diversity is produced dynamically, through the process of evolution; this has led life's creative productivity to be called Open-Ended Evolution (OEE) in the field. This article introduces the second of two special issues on current research in OEE and provides an overview of the contents of both special issues. Most of the work was presented at a workshop on open-ended evolution that was held as a part of the 2018 Conference on Artificial Life in Tokyo, and much of it had antecedents in two previous workshops on open-ended evolution at artificial life conferences in Cancun and York. We present a simplified categorization of OEE and summarize progress in the field as represented by the articles in this special issue.",
    "The properties of MgO/Ag(001) ultrathin films with substitutional Mg atoms in the interface metal layer have been investigated by means of Auger electron diffraction experiments, ultraviolet photoemission spectroscopy, and density functional theory (DFT) calculations. Exploiting the layer-by-layer resolution of the Mg KL_23 L_23 Auger spectra and using multiple scattering calculations, we first determine the interlayer distances as well as the morphological parameters of the MgO/Ag(001) system with and without Mg atoms incorporated at the interface. We find that the Mg atoms incorporation drives a strong distortion of the interface layers and that its impact on the metal/oxide electronic structure is an important reduction of the work function (0.5 eV) related to band-offset variations at the interface. These experimental observations are in very good agreement with our DFT calculations which reproduce the induced lattice distortion and which reveal (through a Bader analysis) that the increase of the interface Mg concentration results in an electron transfer from Mg to Ag atoms of the metallic interface layer. Although the local lattice distortion appears as a consequence of the attractive (repulsive) Coulomb interaction between O2- ions of the MgO interface layer and the nearest positively (negatively) charged Mg (Ag) neighbors of the metallic interface layer, its effect on the work function reduction is only limited. Finally, an analysis of the induced work function changes in terms of charge transfer, rumpling, and electrostatic compression contributions is attempted and reveals that the metal/oxide work function changes induced by interface Mg atoms incorporation are essentially driven by the increase of the electrostatic compression effect.",
    "The need to monitor industrial processes, detecting changes in process parameters in order to promptly correct problems that may arise, generates a particular area of interest. This is particularly critical and complex when the measured value falls below the sensitivity limits of the measuring system or below detection limits, causing much of their observations are incomplete. Such observations to be called incomplete observations or left censored data. With a high level of censorship, for example greater than 70%, the application of traditional methods for monitoring processes is not appropriate. It is required to use appropriate data analysis statistical techniques, to assess the actual state of the process at any time. This paper proposes a way to estimate process parameters in such cases and presents the corresponding control chart, from an algorithm that is also presented.",
    "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",
    "We refine a stimulating study by Sarvotham et al. [2005] which highlighted the influence of peak transmission rate on network burstiness. From TCP packet headers, we amalgamate packets into sessions where each session is characterized by a 5-tuple (S, D, R, Peak R, Initiation T)=(total payload, duration, average transmission rate, peak transmission rate, initiation time). After careful consideration, a new definition of peak rate is required. Unlike Sarvotham et al. [2005] who segmented sessions into two groups labelled alpha and beta, we segment into 10 sessions according to the empirical quantiles of the peak rate variable as a demonstration that the beta group is far from homogeneous. Our more refined segmentation reveals additional structure that is missed by segmentation into two groups. In each segment, we study the dependence structure of (S, D, R) and find that it varies across the groups. Furthermore, within each segment, session initiation times are well approximated by a Poisson process whereas this property does not hold for the data set taken as a whole. Therefore, we conclude that the peak rate level is important for understanding structure and for constructing accurate simulations of data in the wild. We outline a simple method of simulating network traffic based on our findings.",
    "The Brouwer fixed-point theorem in topology states that for any continuous mapping $f$ on a compact convex set into itself admits a fixed point, i.e., a point $x_0$ such that $f(x_0)=x_0$. Under certain conditions, this fixed point corresponds to the throat of a traversable wormhole, i.e., $b(r_0)=r_0$ for the shape function $b=b(r)$. The possible existence of wormholes can therefore be deduced from purely mathematical considerations without going beyond the existing physical requirements.",
    "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.",
    "As well known, the spectrum of a non-relativistic two-body system interacting by the Coulomb potential is the Balmer series $E_n=\\frac{\\alpha^2m}{4n^2}$ produced by the Schr\\\"odinger equation. In 1954, Wick and Cutkosky have found, in the Bethe-Salpeter equation framework, that for $\\alpha>\\frac{\\pi}{4}$ the relativistic effects result in new levels (in addition to the Balmer series). However, the physical nature of these new states remained unclear and therefore their existence was being questioned. We have recently shown that these extra states are dominated by the exchange (massless) particles, moving with speed of light. That's why they did not appear in the non-relativistic (Schr\\\"odinger) framework.",
    "We study the fundamental properties of the quantum f-relative entropy, where f(.) is an operator convex function. We give the equality conditions under monotonicity and joint convexity, and these conditions are more general than, since they hold for a class of operator convex functions, and different for f(t) = -ln(t) from, the previously known conditions. The quantum f-entropy is defined in terms of the quantum f-relative entropy and we study its properties giving the equality conditions in some cases. We then show that the f-generalizations of the Holevo information, the entanglement-assisted capacity, and the coherent information also satisfy the data processing inequality, and give the equality conditions for the f-coherent information.",
    "Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch. H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.",
    "We measure and analyze reflection spectra of directly coupled systems of waveguides and cavities. The observed Fano lines offer insight in the reflection and coupling processes. Very different from side-coupled systems, the observed Fano line shape is not caused by the termini of the waveguide, but the coupling process between the measurement device fiber and the waveguide. Our experimental results and analytical model show that the Fano parameter that describes the Fano line shape is very sensitive to the coupling condition. A movement of the fiber well below the Rayleigh range can lead to a drastic change of the Fano line shape.",
    "The strength and vertical distribution of atmospheric turbulence is a key factor determining the performance of optical and infrared telescopes, with and without adaptive optics. Yet, this remains challenging to measure. We describe a new technique using a sequence of short-exposure images of a star field, obtained with a small telescope. Differential motion between all pairs of star images is used to compute the structure functions of longitudinal and transverse wavefront tilt for a range of angular separations. These are compared with theoretical predictions of simple turbulence models by means of a Markov-Chain Monte-Carlo optimization. The method is able to estimate the turbulence profile in the lower atmosphere, the total and free-atmosphere seeing, and the outer scale. We present results of Monte-Carlo simulations used to verify the technique, and show some examples using data from the second AST3 telescope at Dome A in Antarctica.",
    "We define an n-plectic structure as a commutative and torsionless Lie Rinehart pair, together with a distinguished cocycle from its Chevalley-Eilenberg complex. This 'n-plectic cocycle' gives rise to an extension of the Chevalley-Eilenberg complex by so called symplectic tensors. The cohomology of this extension generalizes Hamiltonian functions and vector fields to tensors and cotensors in a range of degrees, up to certain coboundaries and has the structure of a Lie oo-algebra. Finally we show, that momentum maps appear in this context just as weak Lie oo-morphisms from an arbitrary Lie oo-algebra into the Lie oo-algebra of Hamiltonian (co)tensors.",
    "Amorphous solids or glasses are known to exhibit stretched-exponential decay over broad time intervals in several of their macroscopic observables: intermediate scattering function, dielectric relaxation modulus, time-elastic modulus etc. This behaviour is prominent especially near the glass transition. In this Letter we show, on the example of dielectric relaxation, that stretched-exponential relaxation is intimately related to the peculiar lattice dynamics of glasses. By reformulating the Lorentz model of dielectric matter in a more general form, we express the dielectric response as a function of the vibrational density of states (DOS) for a random assembly of spherical particles interacting harmonically with their nearest-neighbours. Surprisingly we find that near the glass transition for this system (which coincides with the Maxwell rigidity transition), the dielectric relaxation is perfectly consistent with stretched-exponential behaviour with Kohlrausch exponents $0.56 < \\beta < 0.65$, which is the range where exponents are measured in most experimental systems. Crucially, the root cause of stretched-exponential relaxation can be traced back to soft modes (boson-peak) in the DOS.",
    "To highlight the challenges of achieving representation disentanglement for text domain in an unsupervised setting, in this paper we select a representative set of successfully applied models from the image domain. We evaluate these models on 6 disentanglement metrics, as well as on downstream classification tasks and homotopy. To facilitate the evaluation, we propose two synthetic datasets with known generative factors. Our experiments highlight the existing gap in the text domain and illustrate that certain elements such as representation sparsity (as an inductive bias), or representation coupling with the decoder could impact disentanglement. To the best of our knowledge, our work is the first attempt on the intersection of unsupervised representation disentanglement and text, and provides the experimental framework and datasets for examining future developments in this direction.",
    "This paper proposes a hybrid quantum-classical algorithm to solve a fundamental power system problem called unit commitment (UC). The UC problem is decomposed into a quadratic subproblem, a quadratic unconstrained binary optimization (QUBO) subproblem, and an unconstrained quadratic subproblem. A classical optimization solver solves the first and third subproblems, while the QUBO subproblem is solved by a quantum algorithm called quantum approximate optimization algorithm (QAOA). The three subproblems are then coordinated iteratively using a three-block alternating direction method of multipliers algorithm. Using Qiskit on the IBM Q system as the simulation environment, simulation results demonstrate the validity of the proposed algorithm to solve the UC problem.",
    "It has also been suggested that the detection of a wealth of very low amplitude modes in Delta Sct stars was only a matter of signal--to--noise ratio. Access to this treasure, impossible from the ground, is one of the scientific aims of the space mission CoRoT, a space mission developed and operated by CNES. This work presents the results obtained on HD 50844: the 140,016 datapoints were analysed using independent approaches and several checks performed. A level of 10^{-5} mag was reached in the amplitude spectra of the CoRoT timeseries. The frequency analysis of the CoRoT timeseries revealed hundreds of terms in the frequency range 0--30 d^{-1}. All the cross--checks confirmed this new result. The initial guess that Delta Sct stars have a very rich frequency content is confirmed. The spectroscopic mode identification gives theoretical support since very high--degree modes (up to ell=14) are identified. We also prove that cancellation effects are not sufficient in removing the flux variations associated to these modes at the noise level of the CoRoT measurements. The ground--based observations indicate that HD 50844 is an evolved star that is slightly underabundant in heavy elements, located on the Terminal Age Main Sequence. Probably due to this unfavourable evolutionary status, no clear regular distribution is observed in the frequency set. The predominant term (f_1=6.92 d^{-1}) has been identified as the fundamental radial mode combining ground-based photometric and spectroscopic data. This work is also based on observations made with ESO telescopes under the ESO Large Programme LP178.D-0361 and on data collected at the Observatorio de Sierra Nevada, at the Observatorio Astronomico Nacional San Pedro Martir, and at the Piszkesteto Mountain Station of Konkoly Observatory.",
    "The article deals with observations of star-forming regions S231-S235 in 'quasi-thermal' lines of ammonia (NH$_3$), cyanoacetylene (HC$_3$N) and maser lines of methanol (CH$_3$OH) and water vapor (H$_2$O). S231-S235 regions is situated in the giant molecular cloud G174+2.5. We selected all massive molecular clumps in G174+2.5 using archive CO data. For the each clump we determined mass, size and CO column density. After that we performed observations of these clumps. We report about first detections of NH$_3$ and HC$_3$N lines toward the molecular clumps WB89 673 and WB89 668. This means that high-density gas is present there. Physical parameters of molecular gas in the clumps were estimated using the data on ammonia emission. We found that the gas temperature and the hydrogen number density are in the ranges 16-30 K and 2.8-7.2$\\times10^3$ cm$^{-3}$, respectively. The shock-tracing line of CH$_3$OH molecule at 36.2 GHz is newly detected toward WB89 673.",
    "We report the lowest frequency measurements of gamma-ray burst (GRB) 171205A with the upgraded Giant Metrewave Radio Telescope (uGMRT) covering a frequency range from 250--1450 MHz and a period of $4-937$ days. It is the first GRB afterglow detected at 250--500 MHz frequency range and the second brightest GRB detected with the uGMRT. Even though the GRB is observed for nearly 1000 days, there is no evidence of transition to non-relativistic regime. We also analyse the archival ${\\it Chandra}$ X-ray data on day $\\sim 70$ and day $\\sim 200$. We also find no evidence of a jet break from the analysis of combined data. We fit synchrotron afterglow emission arising from a relativistic, isotropic, self-similar deceleration as well as from a shock-breakout of wide-angle cocoon. Our data also allow us to discern the nature and the density of the circumburst medium. We find that the density profile deviates from a standard constant density medium and suggests that the GRB exploded in a stratified wind like medium. Our analysis shows that the lowest frequency measurements covering the absorbed part of the light curves are critical to unravel the GRB environment. Our data combined with other published measurements indicate that the radio afterglow has contribution from two components: a weak, possibly slightly off-axis jet and a surrounding wider cocoon, consistent with the results of Izzo et al. (2019). The cocoon emission likely dominates at early epochs, whereas the jet starts to dominate at later epochs, resulting in flatter radio lightcurves.",
    "The recently developed theory of quasi-Lie schemes is studied and applied to investigate several equations of Emden type and a scheme to deal with them and some of their generalisations is given. As a first result we obtain t-dependent constants of the motion for particular instances of Emden equations by means of some of their particular solutions. Previously known results are recovered from this new perspective. Finally some t-dependent constants of the motion for equations of Emden type satisfying certain conditions are recovered.",
    "We undertake the study of the charged Higgs bosons predicted by the model with gauge symmetry $SU(3)_c \\otimes SU(3)_L \\otimes U(1)_X$. By considering Yukawa mixing couplings between small ($\\sim$ GeV) and large ($\\sim$ TeV) scales, we show that the hypercharge-one $H_1^{\\pm}$ and hypercharge-two $H_2^{\\pm}$ Higgs bosons predicted by the model, can be simultaneously produced in $pp$ collisions at different production rates. At low energy, the $H_1^{\\pm}$ bosons exhibit the same properties as the charged Higgs bosons from a two Higgs doublet model (2HDM), while $H_2^{\\pm}$ are additional like-charged Higgs bosons from the underlying 3-3-1 model. Thus, the identification of multiple like-charged Higgs boson resonances may test the compatibility of theoretical models with experimental data. We study $H_{1,2}^{\\pm}$ pair and associated $tbH_{1,2}^{\\pm}$ productions at CERN LHC collider. In particular, we obtain that pair production can be as large as the single production in gluon-gluon collisions due to the interchange of a heavy neutral $Z'$ gauge boson predicted by the model. By considering decays to leptons $H_{1,2}^{\\pm} \\rightarrow \\tau \\nu_{\\tau}$, we obtain scenarios where small peaks of $H_{2}^{\\pm}$-boson events in transverse mass distributions can be identified over the $H_{1}^{\\pm}$ background.",
    "Isospin breaking in the $K_{\\ell 4}$ form factors induced by the difference between charged and neutral pion masses is discussed within a framework built on suitably subtracted dispersion representations. The $K_{\\ell 4}$ form factors are constructed in an iterative way up to two loops in the low-energy expansion by implementing analyticity, crossing, and unitarity due to two-meson intermediate states. Analytical expressions for the phases of the two-loop form factors of the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ channel are presented, allowing one to connect the difference of form-factor phase shifts measured experimentally (out of the isospin limit) and the difference of $S$- and $P$-wave $\\pi\\pi$ phase shifts studied theoretically (in the isospin limit). The dependence with respect to the two $S$-wave scattering lengths $a_0^0$ and $a_0^2$ in the isospin limit is worked out in a general way, in contrast to previous analyses based on one-loop chiral perturbation theory. The results on the phases of the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ form factors obtained by the NA48/2 collaboration at the CERN SPS are reanalysed including isospin-breaking correction to extract values for the scattering lengths $a_0^0$ and $a_0^2$."
  ],
  "sampled": [
    "Imagine building software where different parts run at the same time without crashing. That's what concurrent programming is all about, and it's tricky!\n\nSome smart folks have been using logic systems, like those found in $\\pi$DILL and CP, to make this easier and prevent those dreaded deadlocks (where everything grinds to a halt). However, there's a bit of a problem: the way we build these logic proofs doesn't quite match up with how we actually write concurrent programs. It's like trying to fit square blocks into round holes.\n\nOne attempt to bridge this gap is Hypersequent Classical Processes (HCP).  They use a clever trick called \"hypersequents\" to better represent how things run in parallel.  Think of it like organizing your tools into multiple toolboxes instead of just one.  \n\nBut HCP is still a work in progress. It's like having a fancy new toolbox without knowing how to use all the tools yet. It doesn't have a clear way to simulate how a program would actually run, and some of its features don't play nicely with existing systems.\n\nThat's where HCP- comes in!  We've streamlined HCP, giving it a proper \"engine\" (reduction semantics) and removing some unnecessary complexities. We've proven that HCP- avoids deadlocks, behaves predictably, and can handle the same communication tasks as before.  \n\nThink of HCP- as a more practical and user-friendly version of HCP, ready to tackle real-world concurrency challenges!\n",
    "We introduce a simplified version of the BDDC preconditioner, which cleverly manages constraints on specific parts of the problem domain (like edges, faces, and vertices within and between subdomains). This method offers impressive performance, with the condition number  a measure of how well-suited a problem is for numerical solution  demonstrably limited by  $C \\big(1+\\log (L/h)\\big)^2$. Here, $C$ is a constant, while $h$ and $L$ represent the typical sizes of the mesh elements and these chosen subobjects, respectively.\n\nThe beauty of this approach lies in the freedom to select $L$ almost arbitrarily. By strategically choosing its value, we can theoretically shrink the condition number to practically a constant ($O(1)$), leading to significantly faster computations.  \n\nThis work further delves into the advantages and disadvantages of this streamlined preconditioner, highlighting its effectiveness in tackling problems with varying material properties or other heterogeneities.  We present compelling numerical results achieved on high-performance computing clusters to showcase its real-world applicability. \n",
    "In this presentation, we will explore instances where the Heun function arises as a solution to wave equations within the framework of general relativity.  It is known that the Dirac equation, formulated against the backdrop of the Nutku helicoid metric, yields Mathieu functions as solutions in a four-dimensional spacetime.  We demonstrate that a straightforward extension to five dimensions leads to solutions expressible in terms of the double confluent Heun function.  Furthermore, we establish a reduction of this solution to the familiar Mathieu function via a series of transformations. It is crucial to note that the presence of a singularity in the metric at the origin necessitates the imposition of Atiyah-Patodi-Singer spectral boundary conditions on this system. \n",
    "Extensive research has demonstrated that the gradual decline in X-ray emissions observed during the decay phase of long-duration flares (LDEs) strongly suggests continuous magnetic reconnection and energy release in the coronal region of the flare.  This study utilizes RHESSI data to address two key questions: the effectiveness of these processes during the LDE decay phase, and the precise calculation of the energy release rate from this data.\n\nTo investigate these questions, we reconstructed images of selected LDEs during their decay phases. A comprehensive spectral analysis of these images allowed for the extraction of physical parameters associated with coronal sources, enabling an evaluation of the energy release process efficiency.  Furthermore, a thorough examination of the terms within the energy equation was conducted to determine the accuracy associated with each term's determination. \n",
    "Using a multi-scale approach, we characterize the typical geometry of clusters in random media under the FK measure.  This result applies to dimensions two and higher, assuming slab percolation holds under the averaged measure, a condition expected throughout the supercritical phase.  Our work builds upon Pisztora's findings and offers a crucial tool for analyzing the supercritical regime in disordered FK models, as well as their corresponding Ising and Potts counterparts. \n",
    "Young stars called Classical T Tauri stars (CTTS) are known for their faint photospheric absorption lines, a phenomenon called \"veiling.\" This is usually attributed to extra light emission from hot gas near the star's surface, heated by accretion streams. \n\nWe studied four CTTS with unusually strong veiling to see if this effect matches standard models of accretion, where varying accretion rates should lead to predictable changes in brightness and emission line strength.\n\nOur observations showed that while veiling in these stars is highly variable, it doesn't behave as expected. The amount of veiling often implies an unrealistically high energy release, and it doesn't correlate well with brightness or emission line strength. Additionally, rapid changes in veiling occur independently of the stars' rotation.\n\nWe found that the strong veiling in at least three stars is actually caused by emission filling in the photospheric absorption lines, rather than an increase in continuous emission. This means veiling can't be reliably used to estimate accretion rates in CTTS with strong emission lines. \n",
    "Giant low surface brightness (GLSB) galaxies, characterized by their diffuse nature, have been traditionally considered as systems dominated by dark matter. This assumption, however, stems from analyses based on poorly constrained rotation curves. Our study presents a refined investigation of two archetypal GLSB galaxies, Malin 1 and NGC 7589, utilizing re-analyzed HI observations to construct new rotation curves.\n\nContrary to previous findings, our results reveal a steep central rise in the rotation curves of both galaxies, a feature typically associated with high surface brightness (HSB) systems.  Mass decompositions incorporating a dark matter halo indicate that baryonic matter might be the dominant factor influencing the dynamics of their inner regions.  \n\nRemarkably, a \"maximum disk\" fit yields stellar mass-to-light ratios consistent with those observed in HSB galaxies. These findings, along with recent studies, suggest that GLSB galaxies may possess a dual structure: an inner, early-type spiral galaxy exhibiting HSB characteristics, embedded within a diffuse outer LSB disk.\n\nFurthermore, we tested the MOND (Modified Newtonian Dynamics) framework.  While NGC 7589's rotation curve aligns well with MOND predictions, Malin 1 presents a significant challenge to the theory. \n",
    "This study investigates the characteristics of target evaporated fragments produced in forward and backward hemispheres during high-energy nuclear collisions. We analyzed data from interactions of $^{4}$He, $^{16}$O, $^{84}$Kr, and $^{197}$Au beams with AgBr emulsion targets at various energies. \n\nOur analysis focused on several key parameters of the fragment distributions: multiplicity distribution, multiplicity moments, scaled variance, and entropy.  We found that the multiplicity distribution of target evaporated fragments in both hemispheres can be well-described by a Gaussian distribution.  \n\nExamining the multiplicity moments, we observed a consistent increase with the order of the moment (q).  Furthermore, the second-order multiplicity moment exhibited energy independence across all studied interactions, indicating a consistent behavior across different energy regimes.\n\nThe scaled variance, a measure of multiplicity fluctuations, remained close to unity for all interactions.  This suggests a weak correlation among the produced particles. \n\nFinally, our results demonstrate that the entropy of target evaporated fragments in the forward and backward hemispheres are comparable within experimental uncertainties. This suggests a similar degree of disorder in the fragment production mechanism regardless of the emission direction. \n",
    "Harnessing the power of fast acoustic phonons, we propose a novel scheme for ultrafast state preparation in quantum dots using off-resonant optical excitation.  Our theoretical investigation reveals three distinct stages in this process: an initial dressing of states upon laser pulse arrival, followed by phonon-mediated relaxation, and culminating in an adiabatic undressing as the pulse subsides. \n\nCrucially, we demonstrate that achieving adiabatic undressing through tailored pulse shapes dramatically enhances the fidelity of target state preparation. This highlights the critical role of pulse shaping in short-pulse protocols.\n\nFurthermore, our findings establish a powerful framework for selective state preparation in exciton-biexciton systems. By strategically manipulating laser parameters such as pulse detuning and duration, as well as exploiting the biexciton binding energy, we can precisely steer the quantum dot into a desired quantum state.  This opens exciting avenues for manipulating and controlling quantum states in semiconductor nanostructures. \n",
    "Imagine a detective arriving at the scene of a crime.  The standard \"Hilbert space\" approach to quantum mechanics is like finding a jumbled mess of clues and trying to make sense of it after the fact. We observe that nature seems to behave probabilistically, so we bolt on this idea of probability to our equations, explaining things after we've already seen them.\n\nBut what if we could find a more elegant approach, one where probability is built into the very fabric of our theory?  That's where \"quantum logics\" come in, like a detective meticulously cataloging every piece of evidence with its own unique probability from the start. \n\nPicture this: a sprawling mansion, its many rooms representing different possible quantum states.  In this framework, known as \"quantum logics with unique conditional probabilities,\" we can map out every possible outcome, every \"event,\" with its own likelihood.  The famous \"collapse\" of the quantum state, the moment we gain knowledge of a particle's property, becomes a smooth transition in this house of probabilities, guided by the rules of measurement.\n\nWithin this mansion, there are different levels of \"compatibility\" between events, like clues that might be connected or completely independent.  Do these events interfere with each other, or can they be measured simultaneously without consequence?  Can we predict the outcome of one event based on another?  These questions unravel as we navigate the five levels of compatibility, uncovering the hidden relationships between quantum events.\n\nWhile these levels might seem distinct in the grand mansion of quantum logic, they beautifully converge in certain well-known rooms, like the familiar Hilbert space of quantum mechanics or the elegant formalisms of von Neumann algebras. It's as if our detective, after meticulously analyzing every detail, finally pieces together a clear and consistent narrative from the seemingly disparate clues.\n\nThis is the promise of quantum logics  to unveil a deeper, more intuitive understanding of the quantum world, where probability isn't just an afterthought, but an essential thread woven into the very fabric of reality.\n",
    "This study presents a rigorous analysis of wave-vector dispersion within elliptically birefringent stratified magneto-optic media exhibiting one-dimensional periodicity.  We demonstrate that discrepancies in the local normal-mode polarization states between consecutive layers induce inter-mode coupling, profoundly influencing the wave-vector dispersion characteristics and the nature of the system's Bloch states.\n\nThis coupling mechanism gives rise to additional terms in the dispersion relation, absent in uniform circularly birefringent magneto-optic stratified media. Notably, normal mode coupling can lift the degeneracy at frequency band crossover points under specific conditions, leading to the emergence of a magnetization-dependent optical band gap.\n\nWe establish the criteria for the formation of this band gap, demonstrating that it can be characterized by a coupling parameter directly related to the polarization state mismatch between local normal modes in adjacent layers.\n\nFurthermore, we delve into the analysis of Bloch states within this system, elucidating the conditions for maximizing the magnitude of the induced band splitting.  Our findings provide insights into the interplay between structural anisotropy, magnetic effects, and photonic band structures in magneto-optic media, with implications for optical device design and control. \n",
    "This study delves into a novel extension of classical empirical risk minimization (ERM), where the hypothesis space is no longer deterministic but rather a randomly chosen subspace of the original hypothesis space. We focus on data-dependent subspaces formed by the span of randomly selected data points, a framework encompassing Nystrm methods commonly employed in kernel methods as a special case.\n\nWhile employing random subspaces inherently reduces computational burden, a critical question arises: does this strategy compromise learning accuracy?  Recent investigations have explored these statistical-computational trade-offs in the context of least squares and smooth self-concordant loss functions, such as the logistic loss.  \n\nOur work extends these analyses to encompass non-smooth convex Lipschitz loss functions, exemplified by the hinge loss prevalent in support vector machines. This generalization necessitates the development of novel proof techniques to accommodate the non-smooth nature of the loss function. \n\nOur primary findings reveal the existence of distinct regimes, contingent on the learning problem's inherent difficulty, where computational efficiency can be significantly enhanced without sacrificing learning performance.  Specifically, we establish theoretical guarantees for the performance of our proposed method, demonstrating its ability to achieve optimal statistical rates while substantially reducing computational cost.\n\nThese theoretical insights are corroborated by numerical experiments, showcasing the practical benefits of our approach across various learning scenarios. Our results provide a compelling argument for considering random subspaces in ERM, particularly when dealing with large-scale datasets and complex loss functions. \n",
    "Traditional patient consent methods, relying on static paper forms, are being superseded by integrated e-Health systems that empower patients with more dynamic consent management. However, capturing the nuances of consent across diverse situations using authorization policies is complex and error-prone. To address this, we introduce ACTORS, a novel goal-driven consent management approach.  ACTORS leverages the adaptability of Teleo-Reactive (TR) programming to dynamically adjust consent permissions based on changes in the patient's context and the specific domain of data access. \n",
    "This paper delves into the intricate interplay between stochastic processes and fractional differential equations, focusing on the inverse random source problem for the time-fractional diffusion equation.  We consider a scenario where the source term is not deterministic but rather driven by the stochastic dynamics of a fractional Brownian motion.\n\nOur investigation unfolds in two parts. First, we address the well-posedness of the direct problem: given a fractional Brownian motion as the driving force, can we ensure the existence and uniqueness of a solution for the resulting stochastic time-fractional diffusion equation?  We establish affirmative answers to these questions under certain conditions, providing a rigorous foundation for analyzing the system's behavior.\n\nThe second part tackles the more challenging inverse problem: if we only have access to the statistical properties of the solution at a final time point (specifically, its expectation and variance), can we \"rewind\" the system's evolution and recover information about the underlying random source?  We demonstrate the uniqueness of this inverse problem, implying that the final time statistics contain sufficient information to identify the source's distribution.  Furthermore, we characterize the problem's instability, highlighting the sensitivity of the source reconstruction to perturbations in the final time data.\n\nOur analysis hinges on the unique properties of the Mittag-Leffler function, a generalization of the exponential function that plays a central role in fractional calculus, and the intricacies of stochastic integration with respect to fractional Brownian motion. This study sheds light on the intricate nature of inverse problems in the context of fractional diffusion and paves the way for further explorations within this rich mathematical landscape.\n",
    "Imagine trying to map a vast, uncharted territory, but the only information you have are the distances between scattered landmarks. That's the challenge we face when dealing with high-dimensional data sets, where the true structure hides within a complex web of interconnected points.\n\nManifold learning methods, particularly those based on graphs, have emerged as powerful tools for navigating this terrain. Picture a network of dots, each representing a data point, connected by lines whose thickness corresponds to their similarity. By analyzing this network, we can uncover the hidden low-dimensional landscape upon which the data actually resides.\n\nTraditional methods have relied heavily on the Euclidean distance, the familiar straight-line measurement between points. But what if our data's true geometry is more nuanced, better captured by alternative notions of distance? \n\nThis paper embarks on a journey to explore the uncharted territory of non-Euclidean manifold learning. We venture beyond the familiar Euclidean norm, equipping ourselves with a more diverse set of tools for measuring similarity.  \n\nThrough a blend of differential geometry and convex analysis, we unveil the mathematical principles governing these generalized graph Laplacians. Our results reveal how the choice of norm shapes the resulting map of the data, providing insights into which distance metric might best capture the essence of a given problem.\n\nTo illustrate the power of these new tools, we turn our attention to the intricate dance of large molecules, their shapes continuously changing over time.  We find that the Earthmover's distance, a measure sensitive to the distribution of mass within each molecule, proves remarkably adept at capturing these subtle transformations.\n\nOur modified Laplacian eigenmaps algorithm, guided by the Earthmover's distance, not only outperforms its classic Euclidean counterpart in mapping accuracy, but also does so with remarkable efficiency, requiring fewer data points and less computation time.\n\nThis exploration beyond the familiar Euclidean realm opens exciting new possibilities for manifold learning, empowering us to chart the complex landscapes hidden within data, no matter how intricate their geometry. \n",
    "Imagine a heat wave spreading across a complex, intricate surface, its boundaries pulsing with energy.  Capturing this intricate dance of heat is no easy feat, but mathematicians have a powerful tool at their disposal: the heat equation.\n\nTraditional methods for solving this equation resemble a meticulous grid search, dividing the surface into countless tiny squares and tracking the temperature at each point. While effective, this approach can quickly become computationally expensive, especially for complex geometries.\n\nThis paper unveils a more elegant solution, a mathematical symphony that captures the flow of heat with remarkable efficiency. Instead of painstakingly tracking every point in space and time, our approach begins by slicing time into thin, manageable slivers.\n\nWithin each time slice, the heat equation transforms into a modified Helmholtz equation, a familiar friend in the world of wave phenomena. We then summon the power of potential theory, representing the temperature distribution as a combination of two elegant components: a volume potential, capturing the heat emanating from within the surface, and a double layer potential, reflecting the influence of the boundary.\n\nTo tame the computational complexity, we enlist the help of the fast multipole method (FMM), a mathematical virtuoso that can efficiently calculate the interactions between distant points.  This allows us to gracefully handle the volume potential, reducing the computational burden significantly.\n\nNext, we turn our attention to the boundary, the pulsating edge of our heat wave. Here, an integral equation emerges, its solution holding the key to satisfying the boundary conditions.  Once again, the FMM steps in, accelerating our calculations and ensuring a swift resolution.\n\nThe result?  A remarkably efficient method for simulating heat flow, even across the most intricate of surfaces.  With a computational cost that scales linearly or near-linearly with the number of points, our approach outperforms traditional methods, opening new avenues for exploring the fascinating world of heat transfer. \n",
    "This research explores the exciting potential of qudits, higher-dimensional quantum systems, for enhancing the security and efficiency of quantum communication.  We investigate a scheme where sequential state-discrimination measurements are employed to decipher the initial state of a qudit chosen from a set of non-orthogonal quantum states.\n\nWhile perfectly distinguishing these states is impossible due to their inherent overlap, our approach leverages the power of unambiguous state discrimination. This technique guarantees error-free measurements while accepting the possibility of inconclusive outcomes.\n\nBy harnessing qudits, which can encode more information per unit than their two-level counterparts (qubits), we unlock new possibilities for communication efficiency. Our analysis focuses on a scenario where a sender, Alice, transmits one qudit chosen from a set of N, with each qudit possessing N dimensions. We explore two distinct cases: one with uniform state overlap and another with varying overlaps between two distinct subsets of qudits.\n\nExcitingly, we demonstrate that this qudit-based approach offers increased robustness against eavesdropping compared to traditional qubit-based schemes.  The higher dimensionality of qudits makes it significantly more likely for an eavesdropper to introduce detectable errors when attempting to intercept the transmitted information.  Our findings highlight the potential of qudits for developing more secure and efficient quantum communication protocols.\n",
    "This work decisively enhances access control security within the Hyperledger Fabric blockchain framework. We achieve this by seamlessly integrating multiple IDs, attributes, and policies into the core access control mechanisms.\n\nOur approach begins with a comprehensive analysis of Hyperledger Fabric's existing access control system. We then introduce a novel implementation that builds upon this foundation, empowering users and developers with streamlined and flexible access control decisions based on combinations of IDs, attributes, and policies.\n\nThis enhanced system incorporates a modified Fabric CA client, simplifying the process of attribute addition and certificate management for new users.  We demonstrate unequivocally that integrating multiple IDs, attributes, and policies is not only feasible but also highly effective in bolstering security within the Hyperledger Fabric ecosystem.\n\nFurthermore, our performance evaluation confirms that the added security measures have a negligible impact on real-world applications compared to the inherent risks of unrestricted resource access. This research delivers a practical and robust solution for significantly improving access control within Hyperledger Fabric. \n",
    "This presentation provides a definitive update on the ongoing solar axion search conducted by the CERN Axion Solar Telescope (CAST).  We present compelling results from the initial phase of CAST Phase II, where the magnet bores were systematically filled with 4He gas at varying pressures. This technique enabled a comprehensive scan for axion masses up to 0.4 eV.\n\nOur analysis, based on the absence of excess X-ray signals when the magnet was directed at the Sun, establishes a stringent upper limit on the axion-photon coupling constant.  For axion masses below 0.4 eV, we confidently constrain the coupling to g < 2.17 x 10^10 GeV^-1 at a 95% confidence level, with the precise value contingent upon the specific pressure setting.\n\nCAST Phase II is currently exploring the axion mass range up to 1.2 eV, utilizing 3He as a buffer gas. We will present projected sensitivities for this ongoing search.  \n\nLooking beyond CAST Phase II, we will outline near-future experimental endeavors and explore long-term options for next-generation helioscope experiments, paving the way for increasingly sensitive explorations of the axion parameter space. \n",
    "Get this:  the Arctic sea ice is vanishing before our eyes, while down south, the Antarctic ice is actually expanding! This perplexing mismatch has been a real head-scratcher for climate scientists, especially since our fancy climate models usually show both polar regions losing ice, just not at such a dramatic pace.\n\nBut wait!  Could the models actually hold the key to cracking this icy mystery? Recent studies have hinted that hidden within the simulations, masked by natural climate wobbles, might lie a more accurate picture.  \n\nWell, we dove headfirst into the latest and greatest climate simulations from CMIP5 and CESM-LE, armed with the knowledge that global temperature and sea ice extent are like two peas in a pod.  And guess what?  The models simulating a rapidly melting Arctic like we see in the real world are all running way too hot! They show far more global warming than actual observations.\n\nUsing not one, but two different methods to account for the real-world temperature trends, we discovered that the chances of these models producing such a rapid Arctic ice meltdown are incredibly slim -  less than 1%! Clearly, something's not quite adding up.\n\nDown in the Antarctic, things are a bit murkier. While the simulations that match the observed ice growth tend to underestimate global warming, the results aren't as clear-cut. However, the models still struggle to capture the stark contrast between the two poles.\n\nThis leads to a fascinating possibility: are our models accidentally getting the right sea ice trends for the wrong reasons? This intriguing puzzle highlights the need to refine our understanding of the complex interplay driving changes at both poles, ensuring our models paint a truly accurate picture of our planet's future. \n",
    "Biometric authentication is rapidly emerging as a critical component for securing the ever-expanding Internet of Things (IoT).  This paper provides a comprehensive analysis of the challenges and opportunities presented by biometric authentication in the context of IoT devices.\n\nWe begin by examining the key factors hindering the widespread adoption of biometric models, encompassing both physiological (e.g., face, iris, fingerprint) and behavioral (e.g., gait, voice, keystroke) biometrics.  We delve into the limitations imposed by data availability, computational constraints, privacy concerns, and the unique security threats present in the IoT ecosystem.\n\nNext, we provide a detailed review of machine learning and data mining techniques employed in biometric authentication and authorization schemes for mobile IoT devices. This encompasses a wide range of approaches, from traditional statistical methods to cutting-edge deep learning algorithms.\n\nRecognizing the importance of robust security, we present a thorough analysis of common threat models and corresponding countermeasures employed by biometric-based authentication schemes for mobile IoT. This includes discussions on spoofing attacks, adversarial machine learning, and privacy-preserving techniques.\n\nFinally, we synthesize our findings to highlight key challenges and promising research directions for the future of biometric-based authentication in the IoT. These encompass areas such as developing lightweight and privacy-preserving algorithms, addressing adversarial attacks, and ensuring fairness and inclusivity in biometric systems. \n",
    "Device fingerprinting, a technique used to identify devices on the web, has gained significant traction. However, existing methods rely on software-based features that users can easily manipulate, compromising their effectiveness.\n\nThis paper argues that the advent of HTML5 presents a unique opportunity to leverage hardware-based fingerprinting, which is significantly more resistant to manipulation. We propose several novel techniques that enable HTML5 web applications to extract identifying information from a device's hardware.\n\nAs a proof of concept, we present an initial experiment demonstrating the feasibility of fingerprinting a device's GPU using HTML5. Our findings highlight the potential of hardware-based fingerprinting for enhancing device identification accuracy and resilience against manipulation. \n",
    "We establish a compelling connection between Chern-Simons theory with exceptional gauge groups on the three-sphere and the refined closed topological string.  Specifically, we demonstrate that the Chern-Simons partition function can be expressed in terms of the refined topological string partition function, with a precise relation between their parameters: $2\\tau=g_s(1-b)$. Here, $\\tau$ represents the single Khler parameter, $g_s$ is the string coupling constant, and $b$ is the refinement parameter, taking on specific rational values for each exceptional group ($b=\\frac{5}{3},\\frac{5}{2},3,4,6$ for $G_2, F_4, E_6, E_7, E_8$, respectively).\n\nOur derivation hinges on the universal form of the Chern-Simons partition function on the three-sphere, as characterized by Vogel's parameters.  By restricting this universal form to a specific line in Vogel's parameter space defined by $\\gamma=2(\\alpha+\\beta)$ (which includes all exceptional groups), we arrive at our result. Notably, the non-zero BPS invariants in this case are found to be $N^2_{0,\\frac{1}{2}}=1$ and $N^{11}_{0,1}=1$, with the remaining contributions corresponding to refined constant maps in string theory.\n\nFurthermore, we demonstrate the robustness of this connection by considering a different line in Vogel's parameter space given by $\\gamma=\\alpha+\\beta$. This line encompasses the groups $SU(4)$, $SO(10)$, and $E_6$. Remarkably, we obtain analogous results, with non-zero BPS invariants  $N^2_{0,\\frac{1}{2}}=1$ and $N^{7}_{0,1}=1$.\n\nIn both cases, the refinement parameter $b$ (equivalent to $-\\epsilon_2/\\epsilon_1$ in Nekrasov's notation) can be directly expressed in terms of the universal parameters restricted to the respective lines as $b=-\\beta/\\alpha$. This result underscores a deep and universal connection between Chern-Simons theory with specific gauge groups and the refined topological string, potentially leading to new insights into both frameworks. \n",
    "Imagine trying to find a \"good representative\" for a scattered collection of data points.  In one dimension, we often use the median, which ensures that half the points lie on either side. This concept extends to higher dimensions with the centerpoint: a point that guarantees any half-space containing it also contains a significant portion (at least 1/(d+1)) of the data.\n\nBut what if we want multiple representatives, like using quantiles instead of just the median? This paper explores extending this idea to higher dimensions. \n\nInstead of a single centerpoint, we seek a small set of points called \"Q.\" This set has a special property: as you capture more points from \"Q\" within a half-space, you're guaranteed to capture an increasingly larger portion of the original data set.\n\nThis concept sits between two well-known ideas in discrete geometry: weak -nets and weak -approximations. It's more powerful than simply covering a certain fraction of the data (-nets), but less restrictive than accurately representing the data's distribution within every possible range (-approximations).  This paper paves the way for exploring this new notion of \"multi-representative\" points in higher dimensions, offering a new tool for understanding and summarizing complex data sets. \n",
    "This paper introduces \\textsc{PsrPopPy}, a powerful new software package designed for simulating realistic populations of pulsars. Building upon the foundation of the existing \\textsc{Psrpop} package, \\textsc{PsrPopPy} boasts a completely redesigned Python codebase, leveraging object-oriented programming principles for enhanced modularity and flexibility.\n\nThis significant overhaul offers several advantages.  First, it provides users with a more accessible and adaptable platform for conducting pulsar population synthesis studies. Pre-built scripts are available for standard simulations, while the modular architecture empowers users to easily customize simulations and incorporate new features, such as updated models for pulsar period or luminosity distributions.\n\nTo illustrate \\textsc{PsrPopPy}'s capabilities, we present two compelling applications.  First, by synthesizing pulsar populations and comparing them to multi-frequency survey data, we determine that pulsar spectral indices are best described by a normal distribution with a mean of -1.4 and a standard deviation of 1.0.\n\nSecond, we delve into pulsar spin evolution to refine the relationship between a pulsar's radio luminosity and its spin parameters (period and period derivative).  Replicating the analysis of Faucher-Gigu\\`ere & Kaspi, we utilize \\textsc{PsrPopPy} to optimize the power-law relationship between these quantities.  Our findings indicate that the underlying pulsar population's luminosity is best described by $L \\propto P^{-1.39 \\pm 0.09} \\dot{P}^{0.48 \\pm 0.04}$, remarkably similar to the relationship found for gamma-ray pulsars. \n\nLeveraging this refined relationship, we generate a model pulsar population and examine the intriguing age-luminosity relation for the entire population, a relationship that future large-scale surveys with the Square Kilometer Array may be poised to unravel. \n\n\n",
    "Imagine an intricate dance between light and matter, where a chorus of spins interacts with the rhythmic pulse of a confined light field. We delve into this fascinating realm, exploring the dynamics of a spin ensemble coupled intensely to a single-mode resonator, energized by carefully crafted external pulses.\n\nOur investigation reveals a captivating phenomenon: when the spin ensemble's average frequency resonates with the cavity mode, they engage in a damped tango  Rabi oscillations. We've developed a precise theoretical model that captures not just these oscillations, but also the subtle dephasing effects arising from variations in the spins' frequencies.\n\nHere's the surprising twist:  precise knowledge of this frequency spread is paramount, holding the key to unlocking a deeper understanding of the temporal dynamics of this coupled system.  Armed with this understanding, we can choreograph the external pulses to match specific resonance conditions, boosting the coherence of the oscillations between the spin ensemble and the cavity by orders of magnitude!\n\nBut it's not all theory!  We've put our ideas to the test in an experiment where an ensemble of negatively charged nitrogen-vacancy (NV) centers in diamond takes center stage.  These NV centers, renowned for their remarkable quantum properties, are coupled strongly to a superconducting coplanar waveguide resonator  our stage for this intricate dance. The experimental results beautifully confirm our theoretical predictions, paving the way for exploring novel regimes of light-matter interaction in cavity quantum electrodynamics.  \n\n\n",
    "Imagine a chain of tiny quantum magnets, each pointing up or down, immersed in a magnetic field.  This system, known as the quantum Ising model, is a playground for exploring magnetism at the microscopic level. \n\nThis work digs into a more complex version of this model where the interactions between neighboring magnets can be different throughout the chain, making it \"inhomogeneous.\" We then explore how to characterize different phases of this quantum system using geometric tools.\n\nFirst, we find a way to express the system's energy in a simplified form by mapping it onto a collection of non-interacting fermions. This allows us to define a geometric space, a circle (S^1), that captures the system's essential properties through a \"twist operator.\"\n\nOn this circle, we can define a distance function, the \"ground-state cyclic quantum distance,\" which tells us how different two points in this space are in terms of their quantum properties.  We also define a \"ground-state Riemannian metric,\" which tells us how distances change as we move around this space.\n\nOur key finding is that this geometric framework provides a powerful way to distinguish between different phases of matter.  For example, the \"quantum ferromagnetic phase,\" where all the magnets tend to align, can be identified by a constant Riemannian metric and an invariant cyclic quantum distance. On the other hand, in the \"paramagnetic phase,\" where the magnets are disordered, the metric rapidly decays to zero. \n\nThis study provides a new perspective on characterizing quantum phases of matter in inhomogeneous systems, paving the way for a deeper understanding of these fascinating phenomena. \n",
    "Rotation measure synthesis is like having a special lens that allows us to peer into the invisible magnetic fields sprinkled throughout the cosmos. By cleverly analyzing the way light waves are twisted as they travel through space, this technique reveals a wealth of information about these enigmatic fields.\n\nThis research unlocks an exciting new chapter in our ability to study cosmic magnetism. We demonstrate that rotation measure synthesis, which uses a Fourier transform to map out magnetic fields, is mathematically analogous to the way interferometers combine light from multiple telescopes to produce sharper images. This realization is a game-changer!\n\nThis powerful analogy allows us to borrow techniques from the world of interferometry and apply them to the analysis of cosmic magnetic fields. Specifically, we can now accurately model the impact of \"channel averaging\"  a common observational constraint  during the reconstruction of Faraday rotation, a key measure of magnetic field strength and direction.  Previously, this effect had hampered our ability to fully exploit wide-band observations, but no more!\n\nThrough simulations, we showcase the power of this approach, demonstrating its ability to unveil signals from extremely strong magnetic fields that were once hidden from view. This breakthrough is especially relevant for low-frequency, wide-band observations, opening up a treasure trove of new data for studying the most extreme environments in the Universe.\n\nBut wait, there's more! We've taken these ideas even further by introducing a technique akin to \"mosaicking\" in Faraday depth, allowing us to seamlessly stitch together data from multiple telescopes. This exciting development promises to revolutionize polarimetric science, yielding magnetic field maps of unprecedented quality and detail.\n\nThe future of cosmic magnetism research is bright! With these new tools in hand, we are poised to unravel the secrets of magnetic fields surrounding pulsars, Fast Radio Bursts, and other extreme objects, ultimately utilizing them as probes to chart the vast magnetic fields that thread the fabric of our Universe. \n",
    "This study compares different statistical models in their ability to describe the production of charged particles in high-energy collisions between hadrons and atomic nuclei.  We utilize four distinct probability distributions: the Negative Binomial, shifted Gompertz, Weibull, and Krasznovszky-Wagner distributions. Each of these models offers a unique mathematical framework, some based on empirical parameterizations and others rooted in theoretical descriptions of the underlying physics.  \n\nBy comparing the model predictions with experimental data, we assess their relative successes in capturing the characteristic features of particle production. Our analysis focuses on a range of physical observables, providing insights into the strengths and limitations of each statistical approach for understanding particle production in high-energy nuclear collisions. \n",
    "Imagine a cloud of data points scattered in multi-dimensional space. How do you define the \"deepest\" point within this cloud, a point that best represents its center?  This question lies at the heart of data depth, a powerful concept in statistics pioneered by John Tukey and further developed by David Donoho and Miriam Gasko.\n\nTukey's idea was simple yet elegant: the deepest point is the one most surrounded by other data points. Donoho and Gasko formalized this notion by considering all possible hyperplanes (think of lines in 2D, planes in 3D, and their higher-dimensional counterparts) passing through a given point.  The depth of this point is determined by the smallest fraction of data points that can be separated from it by any such hyperplane. \n\nThis intuitive concept of data depth has blossomed into a rich field of statistical methods.  Numerous depth functions have been proposed, each with its strengths and weaknesses in terms of computational complexity, robustness to outliers, and sensitivity to asymmetries in the data distribution. \n\nA particularly useful application of data depth lies in constructing \"depth-trimmed regions.\" These regions are defined as sets of points with depth exceeding a certain threshold, providing a visual and quantitative representation of the data's center, spread, and overall shape. The innermost region serves as a generalization of the median to higher dimensions.\n\nThe concept of data depth extends beyond mere data clouds to encompass general probability distributions, enabling theoretical analysis and the establishment of rigorous statistical properties.  Moreover, it has been successfully generalized to handle complex data objects residing in function spaces, such as curves and surfaces.\n\nData depth has become an indispensable tool for exploring and analyzing complex data sets, offering a powerful way to uncover hidden patterns, detect outliers, and gain insights into the underlying structure of data.\n",
    "Strain engineering, the fine art of manipulating a material's properties by stretching or compressing its atomic lattice, holds immense potential for crafting next-generation optoelectronic devices. This is particularly relevant for SiGe nanostructures, which are highly sensitive to strain.\n\nThis study unveils a novel strategy for achieving large tensile strain in SiGe nanostructures, paving the way for enhanced device performance.  Our approach leverages lateral confinement by the silicon substrate itself, eliminating the need for external stressors and offering significant advantages in terms of scalability and fabrication simplicity.\n\nWe focus on Ge-rich SiGe nano-stripes, meticulously crafted using advanced epitaxial growth techniques. To probe their strain state with unprecedented detail, we employ a powerful combination of experimental and theoretical tools.\n\nTip-enhanced Raman spectroscopy, a technique that combines the spatial precision of atomic force microscopy with the sensitivity of Raman spectroscopy, allows us to map the strain distribution within these nano-stripes with a remarkable lateral resolution of approximately 30 nanometers.  Our measurements reveal a striking pattern: a large tensile hydrostatic strain component concentrated at the center of the top surface, gradually diminishing towards the edges.\n\nThis strain distribution is further confirmed and explained through finite element method simulations, which provide a comprehensive picture of the mechanical stresses and strains within the nano-stripe.  The simulations highlight the key role of the lateral constraint imposed by the substrate sidewalls, which inhibits relaxation in the out-of-plane direction. This confinement, coupled with the inherent misfit strain between SiGe and Si, leads to the observed tensile strain enhancement.\n\nBut how does this strain impact the material's electronic properties? To answer this, we turn to X-ray photoelectron emission microscopy, a technique that allows us to map the work function  a key parameter governing electron emission  with a spatial resolution better than 100 nanometers.  Our measurements reveal a positive work function shift in the nano-stripes compared to bulk SiGe, indicating a modification of the electronic band structure due to the applied strain.\n\nThese experimental observations are corroborated by ab initio electronic structure calculations, which provide a theoretical underpinning for the observed work function shift.  Our calculations confirm that tensile strain directly modifies the electronic band structure of SiGe, impacting its optoelectronic properties.\n\nThis study establishes a novel and scalable route to achieving high tensile strain in SiGe nanostructures, opening exciting avenues for developing high-performance optoelectronic devices with tailored properties.  \n\n\n",
    "Imagine a race against time to combat a global pandemic.  Sharing vital medical data and insights across borders could be the key to developing life-saving treatments and containment strategies. However, directly applying one region's data or models to another often stumbles due to subtle but significant differences in disease presentation, healthcare practices, or population demographics  a challenge known as distribution shift.\n\nThis research delves into the potential of deep transfer learning, a cutting-edge machine learning technique, to overcome this hurdle and unlock the full potential of global data collaboration during pandemics. We put two powerful data-based algorithms (domain-adversarial neural networks and maximum classifier discrepancy) and model-based transfer learning to the test, challenging them with infectious disease detection tasks.\n\nTo truly understand their strengths and limitations, we crafted realistic synthetic scenarios mimicking the real-world complexities of data distribution shifts between regions.  Our findings reveal a powerful synergy between data availability and transfer learning efficacy:\n\n* When the source and target regions exhibit similarities, and the target region has limited labeled training data, transfer learning emerges as a game-changer, significantly boosting disease detection accuracy.\n* In situations where the target region lacks labeled training data entirely, model-based transfer learning takes center stage, showcasing impressive performance comparable to data-based approaches.\n\nIntriguingly, our results highlight the critical need to dissect and understand the nuances of real-world data distribution shifts, paving the way for even more effective transfer learning strategies.  This research underscores the immense potential of transfer learning to unlock a new era of global health collaboration, where data transcends borders to combat infectious diseases with unprecedented speed and precision. \n",
    "Here are the key points from the text, formatted as a bullet list:\n\n* This research focuses on the study of quasi-bound states in the continuum (quasi-BICs) in simple dielectric structures, specifically during the transition from a solid cylinder to a thin ring. \n* The study reveals a crossover behavior of quasi-BICs from a strong-coupling to a weak-coupling regime as the inner radius of the cylinder increases.\n* This crossover is marked by a transition from avoided crossing of resonant branches to their intersection, with the quasi-BIC persisting only on a single, straight branch.\n* In the strong-coupling regime, the far-field radiation pattern arises from the interference of three waves: two from the resonant modes and one from the overall scattering of the structure.\n* This three-wave interference challenges the conventional understanding of Fano resonance, which typically applies to weak-coupling scenarios involving only two-wave interference. \n",
    "When temperature variations meet turbulent flows, an intriguing phenomenon known as turbulent thermal diffusion emerges, particularly impacting the movement of small particles. This effect generates a non-diffusive transport of particles, essentially pushing them along the direction of the turbulent heat flux. The strength of this unusual particle flux depends on the average particle concentration and a characteristic velocity determined by the particles' inertia.\n\nPrevious theoretical descriptions of this phenomenon were limited to scenarios with small temperature gradients and low particle inertia (characterized by the Stokes number).  This study presents a comprehensive theoretical framework that extends to arbitrary temperature gradients and Stokes numbers, allowing for the analysis of a much broader range of turbulent flows.\n\nWe conducted laboratory experiments using both oscillating grid turbulence and multi-fan generated turbulence to rigorously test our generalized theory. These experiments confirmed the theoretical predictions, demonstrating that in turbulent flows with strong temperature stratification:\n\n- The characteristic velocity of inertial particles remains lower than the typical vertical turbulent velocity at high Reynolds numbers.\n- Both the effective particle velocity and the turbulent thermal diffusion coefficient increase with increasing particle inertia up to a certain point (small Stokes numbers), after which they begin to decline.\n-  Larger temperature gradients lead to a reduction in the effectiveness of turbulent thermal diffusion.\n\nThe strong agreement between our experimental observations and the predictions of our generalized theory provides a powerful tool for understanding and predicting particle transport in a wide array of turbulent flows with temperature variations, from industrial processes to atmospheric and oceanic flows. \n",
    "Understanding the intricate details of pulsar radio emission has challenged astronomers for decades. A prevailing model, the rotating vector model (RVM), simplifies this challenge by assuming that the emission originates from a narrow cone around a rotating magnetic field line, always tangent to the line of sight. While insightful, this model is an approximation.\n\nOur study revisits a more precise treatment, the \"tangent model,\" which acknowledges that the point of emission on the magnetic field line actually shifts with the pulsar's rotation. This shift traces a distinct trajectory on a sphere of radius \"r.\"\n\nWe delve into the geometric intricacies of this model, deriving expressions for both the trajectory of the emission point and its angular velocity. This investigation is particularly timely given recent claims suggesting that this motion might be observable using the novel technique of interstellar holography (Pen et al. 2014).\n\nOur analysis quantifies the discrepancies between the simplified RVM and the more accurate tangent model.  We find that for pulsars exhibiting emission over a broad rotational phase range, the RVM can significantly underestimate the true extent of visible emission.\n\nFurthermore, our findings imply that, based on geometric arguments alone, visible pulsar radio emission likely originates at altitudes exceeding ten percent of the light-cylinder distance.  This intriguing finding underscores the limitations of neglecting retardation effects, particularly at such significant distances from the pulsar. \n",
    "Imagine trying to teach a computer to recognize new things it's never seen before. That's the challenge of zero-shot learning (ZSL) in image recognition.  It's like teaching a child what a \"zebra\" looks like by showing them pictures of horses and telling them about stripes!\n\nThe trick is to use descriptions, or \"semantic information,\" about the objects. Our new approach, the Global Semantic Consistency Network (GSC-Net), takes full advantage of these descriptions for both known and unknown objects, making it a whiz at zero-shot learning!\n\nThink of it like this: GSC-Net learns a common language between images and descriptions. It uses this language to recognize even those objects it hasn't seen before, just by understanding their descriptions.\n\nWe've also taught GSC-Net to be extra clever by understanding relationships between different objects.  For example, it knows that \"zebra\" is closer to \"horse\" than \"table.\"\n\nTo make GSC-Net even more practical, we added a special feature: a \"novelty detector.\" This helps it deal with situations where it needs to identify both known and unknown objects in the mix, like figuring out if that striped animal is a horse or a zebra.\n\nWe put GSC-Net to the test on three challenging datasets and it aced every single one!  It outperformed all other methods, proving that understanding the language of descriptions is key to unlocking the power of zero-shot learning. \n\n\n",
    "There's a common belief that category theory, a branch of math focused on relationships and patterns, naturally supports the idea of mathematical structuralism. Structuralism, broadly speaking, suggests that math is all about understanding abstract structures rather than specific objects.\n\nHowever, this paper argues that this view is mistaken. While structural math deals with unchanging forms, category theory is all about transformations -  changes and relationships that don't always have fixed points of reference.\n\nInstead of supporting structuralism, this paper proposes a different philosophical interpretation of category theory: one that emphasizes the dynamic and interconnected nature of mathematical concepts.\n\nThis new perspective has important implications for how we understand the historical development of math and how we teach it effectively. By focusing on the transformative aspects of category theory, we can gain a deeper appreciation for the evolving nature of mathematical thinking. \n\n\n",
    "This study explores a novel approach to information processing using the intriguing properties of exciton-polariton condensates. Exciton-polaritons, quasiparticles arising from the strong coupling of light and matter within semiconductor microcavities, can form macroscopic quantum states known as condensates. These condensates exhibit fascinating phenomena, including the formation of quantized vortices  tiny whirlpools of polariton flow carrying topological charge.\n\nWe theoretically demonstrate that in a non-equilibrium exciton-polariton condensate driven by incoherent pumping, a ring-shaped pump spot can create stable vortex states with topological charges of +1 or -1. These vortex states act as robust memory elements, storing information encoded in their topological charge.\n\nFurthermore, we show that by strategically placing potential barriers within the condensate, we can control the flow of these vortices and even copy or invert their topological charges onto spatially separated ring pumps. This ability to manipulate topological charges offers a new paradigm for information processing, where information is encoded in robust, topologically protected vortex states. \n\nThis research paves the way for exploring novel computing architectures based on the unique properties of exciton-polariton condensates, potentially leading to faster and more energy-efficient information processing technologies.\n",
    "The LOFT mission, vying for the European Space Agency's coveted M3 launch opportunity, underwent a rigorous three-year assessment phase to evaluate its technological readiness. A crucial aspect of this assessment involved characterizing the potential impact of space radiation on the mission's silicon drift detectors (SDDs), responsible for capturing vital X-ray data.\n\nOur team meticulously subjected these detectors to a barrage of radiation, simulating the harsh conditions of space.  We exposed the SDDs to high-energy protons (0.8 and 11 MeV) to quantify the detrimental effects of displacement damage, meticulously measuring the resulting increase in leakage current and any degradation in charge collection efficiency.\n\nFurthermore, we simulated the impact of hypervelocity dust grains, mimicking the constant bombardment by micrometeoroids in space. This allowed us to assess the robustness of the SDDs against these potentially destructive events.\n\nThis paper provides a comprehensive overview of our experimental findings, contextualizing their implications for the LOFT mission's overall performance and longevity in the unforgiving environment of space. \n",
    "This study investigates the efficacy of incorporating low-level multimodal features into content-based movie recommendation systems.  We posit that leveraging information from textual, audio, and visual modalities, in conjunction with traditional metadata, can enhance the accuracy of movie similarity assessments.\n\nOur approach centers on developing robust multimodal representation models for movies. In the textual domain, we employ topic modeling techniques applied to movie subtitles, extracting thematic representations that discriminate effectively between movies. For the visual domain, we extract semantically salient features characterizing camera motion, color palettes, and facial information.  Audio analysis leverages pre-trained models to derive meaningful classification aggregates.\n\nThese multimodal features are then integrated with standard movie metadata (e.g., directors, actors) to create a comprehensive content representation. To evaluate our approach, we constructed a dataset comprising 160 well-known movies and implemented a content-based recommendation system that generates ranked lists of similar movies based on various feature combinations.\n\nOur experiments demonstrate that incorporating low-level features from all three modalities (textual, audio, and visual) significantly enhances recommendation performance compared to relying solely on metadata. Notably, we observe a relative increase in performance exceeding 50% across multiple evaluation metrics. \n\nTo the best of our knowledge, this is the first study to leverage such a diverse range of low-level multimodal features to augment content similarity estimation in movie recommendation, surpassing the limitations of traditional metadata-driven approaches. \n",
    "Imagine a cosmic dance between gravity and quantum mechanics, playing out at the edge of a black hole.  We embark on a journey to understand how these titans of physics interact, focusing on the enigmatic radiation emitted by black holes as they slowly evaporate into the cosmos.\n\nOur stage is a charged Reissner-Nordstrm black hole, a more intricate cousin of the classic Schwarzschild black hole. Armed with the tools of quantum gravity, we embark on a quest to solve a fundamental equation known as the Wheeler-De Witt equation. This equation, a cornerstone of quantum cosmology, governs the very fabric of spacetime.\n\nOur journey takes us through three distinct regions:\n\nFirst, we explore the familiar territory between the event horizon, the point of no return, and the vast expanse of spacetime. Here, our calculations reveal a fascinating result: the rate at which the black hole sheds its mass through thermal radiation perfectly matches the predictions of classical physics, providing a reassuring consistency between the quantum and classical worlds.\n\nNext, we venture into the heart of the black hole, delving into the region between the singularity, a point of infinite density, and the inner horizon, a boundary cloaked in mystery. To our surprise, the same equation yields the same answer  the black hole's mass loss rate remains consistent, hinting at an unexpected harmony even in the most extreme corners of the universe.\n\nFinally, we navigate the treacherous territory between the inner and outer horizons, a region where spacetime itself twists and turns in bizarre ways.  Remarkably, even here, the equation holds firm, revealing a consistent picture of black hole evaporation across all realms.\n\nThis work is more than just a mathematical exercise; it's a testament to the power of quantum gravity to illuminate even the darkest corners of the cosmos.  By demonstrating the consistency of black hole radiation across different spacetime regions, we gain a deeper understanding of how gravity and quantum mechanics intertwine to shape the universe we inhabit. \n",
    "Imagine a vast cosmic tapestry, woven with billions of galaxies, each a swirling island of stars, gas, and dust. Now, envision teaching a machine to see this tapestry through the eyes of an astronomer, distinguishing between the elegant ellipses of early-type galaxies, the majestic spiral arms of their younger counterparts, and the deceptively compact points of light emanating from distant quasars or artifacts of our telescopes.\n\nThis study embarks on this ambitious task, employing the power of machine learning to decipher the language of galactic morphology. We train an artificial neural network, a versatile algorithm inspired by the human brain, on a subset of objects from the Sloan Digital Sky Survey (SDSS) that have been meticulously categorized by citizen scientists participating in the Galaxy Zoo project.\n\nOur goal is to determine whether this artificial apprentice can learn to replicate the discerning eye of its human mentors. We find that the neural network's success hinges critically on the information it receives - the set of input parameters chosen to characterize each galaxy.\n\nFeeding the network with basic colors and profile measurements allows it to make rudimentary distinctions, but its performance flourishes when presented with a richer palette of information.  Adding parameters that capture the subtle textures, asymmetric quirks, and concentrated cores of galaxies proves to be a revelation, significantly boosting the network's accuracy.\n\nIntriguingly, we discover that certain combinations of parameters, while powerful, have limitations.  For instance, relying solely on shapes, textures, and concentrations can leave the network struggling to differentiate between smooth, featureless early-type galaxies and the compact dots of point sources.\n\nHowever, with a carefully selected set of twelve parameters, the neural network truly shines, replicating the human classifications with an impressive accuracy exceeding 90% for all three galaxy types.  Remarkably, this performance holds even when the training data is incomplete, showcasing the robustness of our approach.\n\nOur findings underscore the immense potential of machine learning to transform how we analyze the vast datasets emerging from next-generation sky surveys. With the Galaxy Zoo catalogue as an invaluable training ground, these algorithms promise to become essential tools for astronomers, enabling us to unravel the mysteries of galaxy evolution and map the cosmos with unprecedented detail. \n",
    "The Lambek calculus, a logical system inspired by the structure of natural language, has been a cornerstone for modeling syntax, particularly in the realm of context-free languages. However, the intricacies of real-world language often demand a more expressive framework.\n\nMorrill and Valentin (2015) addressed this need by introducing an extension to the Lambek calculus, incorporating \"exponential\" and \"bracket\" modalities. These additions aim to capture complex linguistic phenomena involving dependencies and discontinuities that reach beyond context-free grammars.  Their system, however, relies on a non-standard contraction rule, deviating from the typical behavior of logical systems.\n\nThis paper delves into the computational properties of this extended Lambek calculus, focusing on the \"derivability problem\"  determining whether a given linguistic structure can be derived within the system.\n\nOur primary finding reveals that, in its full generality, the derivability problem in this extended calculus is undecidable. This implies that no single algorithm can effectively determine the derivability of all possible expressions within this system, highlighting its computational complexity.\n\nHowever, we don't stop there.  We further investigate specific fragments of this calculus, previously identified by Morrill and Valentin, where the derivability problem becomes decidable. We prove that these decidable fragments belong to the NP complexity class. This classification provides valuable insights into the computational resources required to solve the derivability problem within these restricted yet expressive fragments.\n\n\n",
    "Previous studies suggested that the phase transition in 4D Euclidean Dynamical Triangulation (EDT) is first-order, contradicting earlier beliefs of a second-order transition. However, these studies relied on numerical methods that might have influenced the results.\n\nWe address these concerns by employing improved simulation techniques: allowing volume fluctuations, measuring after a fixed number of attempted moves, and mitigating critical slowing down using an optimized parallel tempering algorithm.  Our findings, based on systems significantly larger than those previously studied, confirm the first-order nature of the phase transition.\n\nFurthermore, we introduce a local criterion for distinguishing between different phases within a triangulation and establish a novel correspondence between EDT and the \"balls in boxes\" model. This correspondence leads to a refined partition function with an additional coupling constant. \n\nFinally, we propose modifications to the path-integral measure that could potentially eliminate the observed metastability and drive the EDT system towards a second-order transition. \n",
    "This work delves into the intersection of geometric group theory and theoretical computer science, focusing on the delicate interplay between a group's growth properties and the decidability of its Domino Problem.  The Domino Problem, a classic question in computability theory, asks whether there exists an algorithm to determine if a given set of tiles can tile the plane, subject to certain matching rules.\n\nWe provide a complete characterization of finitely generated groups exhibiting virtually nilpotent growth (or equivalently, by Gromov's celebrated theorem, groups of polynomial growth) for which the Domino Problem admits a decision algorithm.  Our findings reveal a strikingly elegant dichotomy: the only such groups are those that are virtually free, encompassing finite groups and groups containing the integers ($\\mathbb{Z}$) as a subgroup of finite index.  This result establishes a deep connection between the algebraic structure of a group and its combinatorial tiling properties.\n",
    "Indirect detection, seeking telltale signatures of dark matter interactions in the cosmos, stands as a pillar in our quest to understand this elusive component of the universe.  Among these cosmic messengers, gamma rays produced by the annihilation of dark matter particles in the galactic halo shine particularly bright.\n\nThis study unveils the significant potential of spectral features imprinted on these gamma-ray signals to dramatically enhance our ability to not only detect dark matter but also decipher its fundamental nature. Unlike the smooth, featureless gamma-ray spectra expected at lower energies, many dark matter models predict pronounced spectral features near the mass energy of the dark matter particle. These features act as unique fingerprints, providing invaluable clues about the particle's properties and interactions.\n\nWe conduct a comprehensive analysis of the sensitivity of gamma-ray telescopes to such spectral features, including the well-studied case of line signals arising from the direct annihilation of dark matter particles into photons.  Our results demonstrate that these spectral features offer a powerful probe, significantly surpassing the sensitivity offered by broad, model-independent spectral analyses at lower energies.\n\nFurthermore, we derive projected limits on the strength of these spectral features, setting stringent constraints on various dark matter models.  These projected limits underscore the power of focusing on these unique signatures, paving the way for a deeper understanding of dark matter and its role in the universe.\n\n\n",
    "Achieving carbon neutrality in the electricity sector requires not only adopting renewable energy sources like wind and solar, but also tackling the operational challenges they bring to the power grid.  As we increase the share of these variable renewable energy (VRE) sources, balancing electricity supply and demand becomes increasingly complex. \n\nThis paper outlines the key research priorities for managing a grid dominated by VRE. We highlight the need for advancements in:\n\n* **Grid Planning and Operation:** Designing systems that can handle the fluctuating nature of VRE and optimizing their operation in real-time.\n* **Grid Stability:** Ensuring the grid remains stable despite the intermittent nature of VRE.\n* **Energy Storage and Demand Response:**  Integrating energy storage solutions and enabling demand-side participation to balance supply and demand.\n* **Decentralized Control:**  Developing distributed control and estimation strategies for managing a more complex and dynamic grid.\n* **Energy Sector Coupling:**  Analyzing the interactions between the electricity sector and other energy sectors (e.g., heating, transportation) in a carbon-neutral system. \n\nWe identify existing research gaps in these areas and showcase our recent studies that contribute to filling them, particularly regarding improved grid operation and real-time estimation techniques. We also provide practical case studies demonstrating the stability and economic viability of high-VRE grids, offering valuable insights for stakeholders navigating the transition to a carbon-neutral future. \n",
    "Convolutional neural networks (CNNs) are all the rage in computer vision, gobbling up tons of data to learn how to recognize images.  But here's the catch: the better they get, the bigger and more memory-hungry they become!\n\nWe're tackling this problem with our cool new invention: Frequency-Sensitive Hashed Nets (FreshNets). Think of it like a super-efficient compression algorithm for CNNs. \n\nHere's the gist:  we noticed that the learned filters in CNNs are usually smooth and don't change too abruptly.  So, we use a neat trick called the Discrete Cosine Transform (DCT) to analyze these filters in the frequency domain.  Then, we use a simple hash function to randomly group similar frequencies together.\n\nThe magic happens next:  instead of storing separate values for each frequency, we make them share a single value, which the network learns during training. It's like having a giant potluck where everyone brings the same dish, saving a ton of space and resources!\n\nTo squeeze out even more efficiency, we prioritize the important, low-frequency stuff, giving them more \"hash buckets\" than the less-important high-frequency components. \n\nWe tested FreshNets on eight different datasets, and guess what?  It crushed the competition!  Our networks were drastically smaller and still performed like champs, proving that you can teach a CNN to be both smart and efficient. \n",
    "We know that diving into the world of software development can feel daunting, especially when it comes to the often-overlooked, yet crucial stage of Requirements Development. That's why we wanted to find a fun and engaging way to make this process click for students.  \n\nOur solution? Integrating the captivating art of Japanese manga! We recognized that the techniques used in manga, such as creating compelling characters and crafting engaging storylines, could bring a fresh perspective to understanding and defining software requirements.\n\nThrough this unique project-based learning experience, students were able to tap into their creativity and collaboratively build a shared vision for their projects.  The result?  They not only grasped the core principles of Requirements Development but also flourished in defining innovative and high-quality system ideas right from the get-go. This approach not only demystified a critical stage in software development but also empowered students to approach it with newfound confidence and enthusiasm. \n",
    "Challenging the long-held belief that black holes evaporate completely, our research uncovers a fascinating twist in the tale of these cosmic behemoths.  By incorporating the subtle yet profound effects of quantum gravity, we reveal a mechanism that could halt the evaporation process, leaving behind a tantalizing remnant.\n\nFocusing on a 5-dimensional rotating black string, a fascinating object in theoretical physics, we analyze the quantum tunneling of fermions  the building blocks of matter. Our calculations demonstrate that the black string's temperature, a key factor governing its evaporation rate, is not solely determined by its properties.  Instead, it is profoundly influenced by the quantum characteristics of the emitted particles and the presence of an extra spatial dimension.\n\nCrucially, these quantum corrections act as a cosmic brake, slowing down the black string's temperature increase as it radiates energy. This intriguing interplay between gravity and quantum mechanics naturally leads to the formation of a remnant, a small but potentially profound object that defies complete evaporation.  This discovery has profound implications for our understanding of black holes, quantum gravity, and the ultimate fate of these enigmatic objects in the universe. \n",
    "This work explores a novel perspective on word representations, challenging the conventional reliance on first-order embeddings alone. We introduce a new approach: capturing the essence of a word by examining its neighborhood in a pre-trained contextual embedding space.  This method allows us to construct second-order vector representations, encoding not only a word's individual meaning but also its relationship to semantically similar words.\n\nSurprisingly, we find that this localized neighborhood information encapsulates much of the power attributed to pre-trained word embeddings.  When used as input features for deep learning models tackling natural language tasks like named entity recognition and textual entailment, second-order embeddings demonstrate impressive performance, often rivaling their first-order counterparts.\n\nOur investigation reveals a compelling trade-off: second-order embeddings excel in handling diverse and heterogeneous data, demonstrating greater adaptability at the cost of some specificity compared to first-order representations.  Furthermore, we observe intriguing synergies when augmenting contextual embeddings with these second-order features, leading to further performance enhancements in certain tasks.\n\nOur findings also highlight the inherent randomness in embedding initialization.  By leveraging nearest neighbor information from multiple instances of first-order embeddings, we uncover a potential avenue for boosting downstream performance, suggesting that ensemble approaches could be particularly fruitful.\n\nThis exploration into the realm of second-order embeddings unveils rich and underexplored characteristics.  Their increased density and the nuanced semantic interpretation of cosine similarity within these spaces offer fertile ground for future research, promising a deeper understanding of word representations and their role in shaping intelligent language processing systems. \n",
    "Imagine a world where wireless signals can navigate around obstacles and reach even the most challenging corners of your home. That's the promise of millimeter wave (mmWave) technology, powered by reconfigurable intelligent surfaces (RIS)  think of them like smart mirrors that can redirect and focus wireless beams.\n\nBut here's a cool bonus:  not only can these RIS-aided systems deliver super-fast internet, they can also pinpoint your location with remarkable accuracy! It's like having a built-in GPS, even indoors.\n\nHow does it work?  By cleverly analyzing the subtle ways in which wireless signals bounce off the RIS and reach your device, we can extract precise information about your surroundings. Think of it like a virtual echolocation system.\n\nHowever, dealing with the massive amounts of data generated by these systems can be tricky. That's where our research comes in. We've developed a smarter way to process these signals using a technique called multidimensional orthogonal matching pursuit. \n\nImagine trying to find a specific grain of sand on a beach. Instead of searching aimlessly, our method divides the beach into smaller sections and looks for clues in each section independently. This makes the search much faster and more efficient, allowing us to pinpoint your location with remarkable precision.\n\nOur simulations of a realistic indoor environment show that this approach significantly improves localization accuracy in RIS-aided mmWave systems. This opens up exciting possibilities for a wide range of applications, from indoor navigation and augmented reality to smart homes and beyond! \n\n\n",
    "Protecting sensitive information from leaking through subtle timing side channels is crucial for maintaining confidentiality. While static analysis methods are commonly used to detect these leaks, they struggle to handle the complexity of real-world applications and often provide only \"yes\" or \"no\" answers regarding the presence of a leak. \n\nThis paper introduces a powerful dynamic analysis technique that overcomes these limitations. Our approach tackles the challenge of timing side-channel analysis in two stages.  First, we train a neural network to learn the program's timing behavior, capturing the subtle relationships between execution time and secret information.  \n\nNext, we analyze this learned timing model to not only detect potential leaks but also quantify their severity, providing a more nuanced understanding of the potential threat. Our experimental results demonstrate that both stages of our approach are practically feasible, offering significant advantages over existing methods.\n\nThe success of our technique rests on two key innovations: a specialized neural network architecture designed to uncover timing side channels and an efficient algorithm based on Mixed-Integer Linear Programming (MILP) for quantifying the strength of these leaks.  \n\nWe showcase the effectiveness of our method through extensive evaluation on a range of benchmarks and real-world applications. Our neural network models accurately capture timing behaviors of programs with thousands of methods, and crucially, we demonstrate that these complex models can be efficiently analyzed to detect and quantify even minute information leaks. This research provides a practical and robust solution for enhancing software security and safeguarding sensitive data. \n",
    "The inner asteroid belt, located between 2.1 and 2.5 astronomical units (AU) from the Sun, is a region of great interest to astronomers. It's the primary source of chondritic meteorites and near-Earth asteroids, making it crucial for understanding the early solar system and potential threats to Earth.\n\nAsteroids in this region face a gravitational obstacle course. Bounded by a secular resonance and Jupiter's powerful 1:3 mean motion resonance, large asteroids (diameter > 30 km) struggle to escape unless their orbits intersect Mars' path, allowing for gravitational scattering.  \n\nOur study focuses on the chaotic effects of Mars' gravity on asteroids near its 1:2 mean motion resonance.  We find that while this chaotic dance increases the spread of asteroid orbital eccentricities and inclinations, it doesn't significantly shift their average values. Interestingly, while the 1:2 resonance initially amplifies this chaotic scattering, at high eccentricities it acts as a protective barrier, shielding asteroids from close encounters with Mars and extending their lifetime within the belt. \n\nHowever, our most significant finding is that gravitational forces alone, even those amplified by the 1:2 resonance, cannot explain the observed distribution of asteroid eccentricities.  This suggests that non-gravitational forces, such as the Yarkovsky effect (where sunlight subtly nudges an asteroid's orbit), likely play a crucial role in driving asteroids out of the inner belt and towards Earth. \n",
    "The quest to unravel the mysteries of neutrinos, those elusive particles that hold the key to physics beyond the Standard Model, is hindered by the presence of non-standard interactions (NSI).  These interactions, if they exist, could significantly impact the precision measurements at upcoming neutrino oscillation experiments.  \n\nThis research highlights a compelling and complementary approach to pin down the elusive NSI parameters: leveraging the power of electron-positron colliders.  Our analysis reveals the immense potential of current and future colliders, including Belle II, STCF, and CEPC, to impose stringent constraints on NSI involving electrons.\n\nBelle II and STCF, with their dedicated physics programs, are poised to deliver constraints on electron-type NSI parameters that rival the precision achieved by global analyses of existing neutrino data.  Moreover, these colliders offer substantial improvements in constraining tau-type NSI, a sector that has long remained challenging to probe.\n\nLooking towards the future, the proposed CEPC collider emerges as a game-changer in the hunt for NSI. Our findings demonstrate that CEPC alone possesses the capability to dramatically shrink the allowed parameter space for electron NSI.  \n\nCrucially, we show how combining data from different CEPC running modes allows us to break the troublesome degeneracy between left-handed and right-handed NSI parameters.  This breakthrough enables us to establish remarkably tight constraints, limiting the magnitudes of both left-handed and right-handed electron NSI parameters to less than 0.002, even when both types are present simultaneously.\n\n This research underscores the transformative impact electron-positron colliders can have on our understanding of neutrino interactions, providing invaluable and complementary insights to dedicated neutrino experiments.  By combining these approaches, we can pierce the veil of NSI and illuminate the path towards a more complete picture of neutrino physics. \n",
    "The Deep Underground Neutrino Experiment (DUNE) stands at the forefront of neutrino physics and proton decay research, poised to unlock fundamental secrets of the universe.  At the heart of this ambitious endeavor lies a massive far detector, comprising four 10-kton Liquid Argon (LAr) Time Projection Chambers (TPCs) utilizing both single-phase and dual-phase technologies. The dual-phase TPCs, in particular, enable unparalleled sensitivity through charge amplification in the gaseous argon phase.\n\nTo optimize the design of these cutting-edge detectors, two large-scale prototype detectors have been diligently collecting data at CERN since 2018.  These prototypes build upon the success of a previous 4-tonne dual-phase demonstrator, which showcased exceptional charge and light collection performance during its 2017 cosmic muon exposure.\n\nThe light detection system plays a critical role in the TPCs, providing a trigger for the charge readout and delivering complementary information through the analysis of scintillation light emitted during particle interactions.  Our 4-tonne demonstrator, equipped with five cryogenic photomultipliers employing various configurations of base polarity and wavelength shifting, collected a wealth of scintillation light data under diverse drift and amplification field conditions. \n\nThrough rigorous analysis of this data, we have gained an unparalleled understanding of the light production and propagation processes within LAr, ultimately optimizing the design of the DUNE TPCs. This paper presents a comprehensive overview of the light detection system's performance and highlights our key findings regarding scintillation light in LAr, underscoring the crucial role of our prototype studies in ensuring the success of the DUNE experiment. \n",
    "In the relentless pursuit of computing power, chip manufacturers have unleashed a new breed of processors: multithreaded behemoths capable of juggling countless tasks simultaneously. But just like a conductor coordinating a complex symphony, these processors rely on a critical component to orchestrate this intricate dance of data: the operating system (OS).\n\nEnter a new era of resource management with our innovative OS scheduling algorithm, meticulously crafted to harness the full potential of multithreaded, multi-core processors. Imagine a system where the OS acts as a keen-eyed maestro, constantly monitoring the threads, those individual strands of a program's execution, for opportunities to exploit untapped parallelism.\n\nOur algorithm dives deep, analyzing each thread's potential for memory-level parallelism (MLP)  the hidden harmony within a program waiting to be unleashed. By intelligently distributing these threads across the system's resources, the OS ensures that no ounce of processing power goes to waste.\n\nThis is not merely a theoretical exercise. Our qualitative analysis demonstrates the superiority of our approach compared to existing hardware and software solutions. But we haven't rested on our laurels. \n\nThis is a call to action!  Quantitative evaluation and further optimization of our scheduling algorithm hold the key to unlocking unprecedented levels of performance in the multithreaded world. Imagine a future where computers effortlessly handle the most demanding tasks, from complex simulations to groundbreaking artificial intelligence, all thanks to a symphony of threads orchestrated by an OS that truly understands the music of parallelism. \n",
    "This work tackles the challenging task of multi-source morphological reinflection, a significant generalization of the traditional single-source approach.  We demonstrate unequivocally that leveraging multiple source forms and their corresponding tags, rather than relying on a single input, delivers substantial benefits for morphological reinflection. This richer input space provides complementary information, such as diverse stems, which enhances the model's ability to capture the nuances of word structure. \n\nTo fully exploit this multi-source paradigm, we introduce a novel extension to the encoder-decoder recurrent neural network architecture, incorporating multiple encoders to effectively process and integrate information from the diverse source forms. Our experiments decisively demonstrate that this multi-encoder architecture surpasses the performance of state-of-the-art single-source reinflection models.\n\nFurthermore, we present a new dataset specifically designed for multi-source morphological reinflection, publicly releasing it as a valuable resource to foster further research in this domain.  This work establishes a new frontier in morphological reinflection, providing a robust foundation for future advancements in this critical area of natural language processing.\n",
    "Okay, picture this: we're drowning in a sea of data streaming from the web and all those smart gadgets (IoT) popping up everywhere.  We need a way to fish out valuable insights from this data deluge  and fast! \n\nThat's where Laser comes in. It's our brand new reasoning engine, built to handle really complex queries on these massive data streams.  Think of it like a super-powered detective that can connect the dots and solve intricate puzzles in real-time.\n\nHere's the secret sauce:  Laser uses a clever trick to avoid repeating the same work over and over again. It takes notes (annotations) on formulas it's already solved, so it can instantly recall the answers when they pop up again. Smart, right?\n\nWe also turbocharged Laser with some seriously efficient code, making it way faster than other systems out there, like C-SPARQL, CQELS, and even Clingo, which are no slouches themselves!\n\nThe bottom line? Laser makes it a piece of cake to apply heavy-duty logical reasoning to massive streams of data. This opens up a whole new world of possibilities for understanding our data and making smarter decisions on the fly. \n",
    "This study investigates how intentionally introducing defects into a transition metal dichalcogenide (TMDC) layer can be used to fine-tune the spin-orbit coupling (SOC) in an adjacent graphene layer. Using density functional theory, we simulate layered structures of graphene on alloyed WSe2-MoSe2 with varying compositions and defect arrangements. \n\nOur findings reveal that while individual defects significantly alter the local electronic environment, the overall electronic and spin properties, including the induced SOC, can be effectively described by a simple model based on the average composition of the alloyed TMDC.  \n\nThis remarkable result highlights a robust and predictable relationship between alloy composition and induced SOC in graphene-TMDC heterostructures. We further demonstrate that this relationship can be exploited to control the system's topological state, opening up new possibilities for engineering spin-dependent electronic properties in two-dimensional materials. \n",
    "In the grand cosmic ballet of stellar evolution, the humble atomic mass emerges as a critical player, dictating the fate of stars and the creation of elements. Yet, for the most exotic nuclei, forged in the fiery hearts of supernovae and neutron star mergers, experimental data remains elusive.\n\nThis challenge has ignited a global race, a quest to develop cutting-edge instruments capable of measuring the masses of these fleeting nuclei.  Among these, Time-of-Flight (TOF) mass spectrometry has emerged as a powerful contender, complementing the exquisite precision of Penning trap measurements. \n\nAt the National Superconducting Cyclotron Laboratory (NSCL), a new chapter in this scientific saga is being written.  Harnessing the facility's unique capabilities, we've implemented a state-of-the-art TOF-Brho technique, pushing the boundaries of our understanding.  Our focus:  neutron-rich nuclei in the iron region, crucial for unraveling the secrets of the r-process, the rapid cosmic forge responsible for creating half the elements heavier than iron. \n\nThese measurements extend far beyond mere academic curiosity.  They provide vital input for astrophysical models, shedding light on the dynamics of neutron star crusts, the ultra-dense remnants of stellar explosions.  With each measurement, we inch closer to deciphering the intricate processes that govern the cosmos, revealing the profound connections between the subatomic world and the grand tapestry of the universe. \n",
    "Broad Emission Lines (BELs) serve as distinctive signatures of active galactic nuclei (AGNs), setting them apart from their smaller counterparts, X-ray binaries (XRBs).  The absence of detectable BELs in systems with black hole masses below 10^5 solar masses, as observed in SDSS data, raises a crucial question: do such low-mass AGNs truly not exist, or are their BELs too faint to be detected?\n\nWe address this question by systematically calculating the expected equivalent widths (EWs) of prominent ultraviolet and optical emission lines (Ly, H, CIV, MgII) for a wide range of black hole masses, spanning 10 to 10^9 solar masses. Our calculations employ the Locally Optimally Emitting Cloud (LOC) model to characterize the BEL region (BELR) and utilize realistic ionizing spectral energy distributions (SEDs) tailored to different black hole masses.\n\nContrary to expectations, we find that the hardening of the SED with decreasing black hole mass does not diminish the BEL EWs. Instead, the primary factor governing BEL detectability is the size of the BELR, which correlates with black hole mass. \n\nOur results reveal a peak in BEL EWs for black hole masses around 10^8 solar masses, characteristic of typical AGNs.  Below this mass, BEL EWs decrease sharply, dropping significantly below detection thresholds for masses below 10^6 solar masses. This intrinsic faintness of BELs in low-mass AGNs provides a compelling explanation for their apparent absence in current SDSS observations. \n",
    "Get this - we've achieved super-precise synchronization in wireless networks using the awesome power of pulse-coupled oscillators!  For the first time ever, we've implemented these algorithms on FPGA-based radios and the results are incredible!\n\nOur measurements show that we can synchronize these radios with an accuracy of just a few microseconds, all done directly in the physical layer. That's crazy precise!  \n\nBut we didn't stop there. We took it a step further and developed an algorithm extension that compensates for tiny timing variations in the hardware itself.  The result?  We smashed our previous record and achieved sub-microsecond synchronization!\n\nThis breakthrough opens up a world of possibilities for decentralized wireless systems. Imagine a network where devices can seamlessly coordinate their transmissions and sleep cycles without relying on a central clock  it's like a perfectly synchronized dance, but for radios! \n\nOur algorithm makes this a reality, especially in situations where traditional centralized synchronization methods simply won't cut it. This is a game-changer for applications like wireless sensor networks, ad-hoc networks, and beyond! \n\n\n",
    "So, predicting where a person will go next - that's Human Trajectory Prediction (HTP) in a nutshell.  It's a pretty hot topic these days, and people are coming up with all sorts of fancy solutions.\n\nBut hold on a sec!  How can we tell if one solution is actually better than another? That's where benchmarking comes in  comparing different methods on a level playing field. \n\nHere's the thing, though:  not all datasets are created equal.  Some are tougher to crack than others, right? So we decided to figure out what makes a dataset particularly tricky for HTP.\n\nWe came up with a bunch of indicators that measure things like how predictable the paths are, how smooth and regular they are, and how much the surrounding environment comes into play.  Basically, we're giving each dataset a difficulty rating.\n\nWe then took a bunch of popular HTP datasets and put them to the test.  The results are pretty interesting, giving us insights into why some methods might work well on one dataset but struggle on another. \n\nWe're all about sharing the love, so we've put all our code up on Github. Feel free to check it out and see how your favorite HTP methods stack up against these challenging datasets! \n",
    "This paper demonstrates how to build physical implementations of classical linear stochastic systems using components from the world of quantum optics (think lasers and beamsplitters).  Because quantum optical systems often operate at far higher speeds than traditional electronics, they hold the potential for faster response times and improved performance in controlling dynamic systems. \n\nWe provide a step-by-step procedure for constructing these quantum optical realizations of classical systems.  Furthermore, we illustrate how these realizations can be incorporated into measurement-based feedback loops, enabling real-time control applications.  The power and practicality of our approach are highlighted through illustrative examples.  \n",
    "In the field of systems biology, we strive to model the intricate workings of biological cells, spanning from the molecular to the cellular level, using complex networks of biochemical reactions.  A central challenge in analyzing these systems lies in grappling with their inherent multi-scale nature, where processes unfold across vastly different timescales.\n\nThe dynamics of such dissipative reaction networks, characterized by a wide separation of timescales, can be conceptualized as a series of successive equilibrations involving distinct subsets of system variables. For polynomial systems exhibiting timescale separation, equilibration occurs when at least two monomials, bearing opposite signs, attain comparable magnitudes, effectively dominating all other terms.\n\nTropical analysis, a branch of mathematics concerned with the asymptotic behavior of functions, provides a natural framework for describing these equilibrations. This formalism allows us to derive truncated dynamical descriptions by systematically eliminating the dominated terms, leading to powerful model reduction techniques.  \n\n",
    "We conducted a detailed spectral analysis of Suzaku X-ray observations targeting both the galactic disk and outflow regions of the starburst galaxy M82.  Our findings reveal distinct spectral characteristics in these two regions:\n\n**Central Disk Region:**\n\n* Accurate thermal modeling necessitates at least three distinct temperature components.\n*  The observed Ly line fluxes for O VIII and Ne X significantly exceed predictions based on collisional ionization equilibrium (CIE).\n*  Elevated Ly/Ly line ratios for O VIII and Ne X, compared to CIE values, suggest a significant contribution from charge exchange processes.\n\n**Outflow Region:**\n\n* Spectra are well-described by a two-temperature thermal model.\n* We derived abundances for O, Ne, Mg, and Fe in the outflow, finding super-solar ratios of O/Fe, Ne/Fe, and Mg/Fe  (approximately 2, 3, and 2 times solar, respectively, relative to Lodders 2003).\n* Notably, the absence of charge exchange signatures in the outflow region strengthens the reliability of these abundance measurements.\n* This abundance pattern strongly indicates that the outflow is enriched by Type II supernova ejecta, highlighting the role of starburst activity in dispersing metals into the intergalactic medium. \n\n\n",
    "The origin of cosmic dust, the building blocks of planets and life itself, has long been shrouded in mystery.  While aging stars known as asymptotic giant branch (AGB) stars have traditionally been considered the primary dust factories, a new contender has emerged on the cosmic stage: supernovae, the spectacular death throes of massive stars.\n\nTo unravel the relative contributions of these celestial dust makers, we embarked on a journey to simulate the perilous journey of freshly formed dust within the turbulent aftermath of a supernova explosion.  Our newly developed code, GRASH\\_Rev, acts as a virtual time machine, tracking the fate of dust grains as they navigate the onslaught of shockwaves and intense radiation.\n\nFocusing on four iconic supernovae  SN1987A, CasA, the Crab Nebula, and N49  our simulations unveiled a remarkable story of resilience.  Despite the chaotic environment, a significant fraction of the newly forged dust, between 1% and 8%, endures the fiery passage of the reverse shock, eventually finding its way into the vast interstellar medium.\n\nOur calculations reveal that supernovae in our Milky Way galaxy contribute a staggering (3.9  3.7)  10^-4 solar masses of dust per year  a production rate exceeding that of AGB stars by an order of magnitude!\n\nWhile this prolific output is still insufficient to fully counterbalance the destructive power of supernovae, it highlights their crucial role in enriching the interstellar medium with the building blocks of future generations of stars and planets.  This research illuminates a vital chapter in the cosmic cycle of creation and destruction, where the ashes of dying stars seed the universe with the seeds of future worlds. \n",
    "This study employs numerical simulations to investigate optimal hatching strategies for electron beam additive manufacturing, aiming to maximize build speed and beam power utilization.  We utilize a three-dimensional thermal free surface lattice Boltzmann method, previously validated against experimental data for beam powers up to 1.2 kW.\n\nInitial simulations using a basic hatching strategy, where a cuboid is built layer-by-layer, reveal limitations in terms of achievable relative density and surface smoothness at higher beam powers and scan velocities.  We systematically explore these limitations to understand the potential of next-generation high-power electron beam guns (up to 10 kW).\n\nTo circumvent these constraints, we introduce modified hatching strategies designed to minimize build time while ensuring full density and a smooth top surface. These strategies strategically adjust parameters like hatch spacing, scan speed, and beam power to optimize the melt pool dynamics and solidify each layer effectively.\n\nOur results demonstrate that these modified strategies significantly reduce build time and cost, maximizing beam power utilization and unlocking the full potential of high-power electron beam additive manufacturing. \n\n\n",
    "Imagine searching for a hidden treasure in a vast, uncharted landscape.  Each step you take costs valuable resources, and you want to find the treasure with as few steps as possible. That's the challenge of optimization, and Bayesian Optimization (BO) is a powerful tool for navigating this complex terrain.\n\nTraditional BO algorithms operate on a fixed budget of steps, assuming each step has the same cost. But what if the cost varies depending on where you are in this landscape?  Imagine traversing treacherous mountains where each step is arduous and time-consuming, compared to strolling across open plains. \n\nCost-aware BO methods tackle this challenge by considering not just the number of steps but also their individual costs, measured in terms of time, energy, or even money.  Our new approach, Cost Apportioned BO (CArBO), is a master navigator in this cost-conscious world.\n\nCArBO starts by strategically exploring the landscape, focusing on cost-effective initial steps to gain a quick understanding of the terrain. It then enters a \"cost-cooled\" phase, where it balances exploration with exploitation, guided by a learned cost model.  Think of it like a seasoned adventurer who learns to navigate treacherous paths while conserving precious resources.\n\nWe put CArBO to the test on a challenging set of 20 black-box optimization problems, mimicking the complexity of real-world scenarios like tuning the hyperparameters of deep learning models. The results are compelling: given the same cost budget, CArBO consistently discovers significantly better solutions than its competitors. \n\nThis research unveils a new dimension in the world of optimization, where every step counts, not just in terms of progress but also in terms of cost.  CArBO, our cost-aware champion, paves the way for more efficient and effective optimization across a wide range of applications, from machine learning to engineering design. \n",
    "This work unveils a groundbreaking robotic system-of-systems poised to revolutionize exploration in challenging and complex environments.  Our innovative marsupial system combines the strengths of a highly mobile legged robot with the aerial prowess of a drone, creating a synergistic partnership capable of conquering previously inaccessible terrains. \n\nThe quadrupedal robot, with its exceptional endurance and agility, serves as the mothership, navigating treacherous landscapes and confined spaces with ease.  However, when confronted with insurmountable obstacles or vertical structures, the system unleashes its secret weapon: a deployable aerial drone.  \n\nThis tightly integrated partnership enables unprecedented exploration capabilities. The drone, leveraging its 3D navigation capabilities, conducts targeted aerial surveys, extending the reach of the system while operating within its energy constraints.  \n\nAutonomy lies at the heart of our design. The two robots collaborate seamlessly, sharing LiDAR-based maps to co-localize and construct a unified environmental representation. Each robot independently plans its exploration path, while a sophisticated graph search algorithm onboard the legged robot intelligently determines the optimal deployment locations and timing for the aerial drone.\n\nExtensive experimental validation across diverse environments unequivocally demonstrates the transformative power of our marsupial system.  It conquers challenging terrains, expands exploration horizons, and unlocks access to areas previously deemed unreachable by individual robots. This technology promises to revolutionize a wide range of applications, from disaster response and search-and-rescue operations to environmental monitoring and infrastructure inspection. \n\n\n",
    "Imagine a single strand of DNA or RNA, a microscopic thread of genetic code. As we heat this delicate molecule or gently pull on its ends, it undergoes a fascinating transformation, contorting and folding like a microscopic origami masterpiece.  This intricate dance between folded and unfolded states is governed by the formation of loops, tiny bends in the strand that hold the key to its structure and function.\n\nWe've developed a theoretical model that captures this mesmerizing dance, taking into account the subtle energetic cost of forming loops.  Our model reveals a remarkable phase transition, a tipping point where the molecule abruptly shifts between a compact, folded state and a loose, unfolded state, driven by changes in temperature and applied force.\n\nOur findings uncover a hidden world of critical exponents and universal scaling laws, revealing how the size and frequency of loops dramatically influence the molecule's behavior. We discover a critical threshold for loop formation:  only within a narrow range of loop exponents can the molecule transition between folded and unfolded states through changes in temperature alone.\n\nBut the story doesn't end there. Applying a gentle force to the molecule can trigger this unfolding transition for a wider range of loop exponents, a phenomenon readily observable through single-molecule force spectroscopy experiments.\n\nOur insights extend beyond single strands to illuminate the dance of two strands intertwined in a DNA double helix.  Challenging the traditional Poland-Scheraga model of DNA melting, we demonstrate that the formation of loops within single strands can significantly alter the stability of the double helix, leading to lower melting temperatures than previously predicted.  \n\nThis deeper understanding of loop formation reshapes our view of DNA and RNA behavior, offering new avenues for exploring the fundamental processes governing life's blueprint. \n\n\n",
    "Imagine a tiny wire, thinner than a human hair, etched onto a crystal of gallium arsenide.  Within this wire, electrons' more massive siblings, holes  the absence of electrons  behave in unexpected and fascinating ways.\n\nOur study explores the \"Zeeman spin-splitting\" of these holes, a phenomenon where their energy levels split in the presence of a magnetic field.  Using a high-mobility crystal carefully aligned along specific directions, we discovered a remarkable ability to control this spin-splitting with a simple twist.\n\nBy rotating the magnetic field from parallel to perpendicular to the wire, we can effectively switch the spin-splitting \"on\" or \"off,\" like a microscopic quantum switch.  Intriguingly, the wire's properties remain identical regardless of its orientation relative to the underlying crystal structure, hinting at a universal underlying mechanism.\n\nFurthermore, as we squeeze the wire, narrowing its confines, the \"g-factor,\" a measure of the spin-splitting strength, decreases. This behavior contrasts sharply with electrons, whose g-factor typically increases as they are confined.  This surprising observation provides compelling evidence for a unique type of spin-splitting in holes, arising from their complex spin-3/2 nature, where the splitting depends on their momentum (k) within the wire. \n\nThis research opens a new window into the rich and intricate world of hole physics, offering tantalizing possibilities for manipulating spin at the nanoscale and developing novel spintronic devices.  \n",
    "Drawing inspiration from the rich mathematical structure of real Clifford algebras, we propose a novel framework for assigning spacetime dimensions to algebraic systems relevant to physics.  Specifically, we consider algebras represented over complex Hilbert spaces that possess two self-adjoint involutions and an anti-unitary operator satisfying specific commutation relations.  By analogy with Clifford algebras on even-dimensional vector spaces, we assign a pair of spatial and temporal dimensions modulo 8 to each such algebra.\n\nA key feature of this assignment is its compatibility with the tensor product operation: the spacetime dimensions of a tensor product algebra are simply the sums of the dimensions of its constituent factors. This property offers a potential interpretation for the prevalence of such algebras in diverse physical contexts, including PT-symmetric Hamiltonians and the description of topological matter.\n\nFurthermore, we leverage this construction to develop an indefinite, pseudo-Riemannian generalization of spectral triples, fundamental objects in noncommutative geometry.  Our approach replaces the traditional Hilbert space framework with Krein spaces, naturally accommodating indefinite inner products.  This enables us to express the Lagrangian, encompassing both bosonic and fermionic terms, for Lorentzian almost-commutative spectral triples.\n\nRemarkably, we identify a space of physical states within this framework that elegantly resolves the long-standing fermion-doubling problem.  To illustrate the power of our approach, we explicitly describe the case of quantum electrodynamics, demonstrating its consistency with established physics.\n\n\n",
    "This paper explores the symmetries hidden within actions describing a massive relativistic particle moving at speeds close to the speed of light. By expanding these actions around the simpler, non-relativistic Galilean limit, we unlock a rich structure of spacetime symmetries.\n\nWorking in the elegant framework of canonical variables, we uncover all point spacetime symmetries for these \"post-Galilean\" actions.  Furthermore, we discover an infinite family of generalized Schrdinger algebras, each labeled by an integer \"M.\" The familiar Schrdinger algebra, governing the symmetries of the non-relativistic Schrdinger equation, emerges as a special case (M=0).\n\nWe then investigate the Schrdinger equations associated with these generalized algebras, delving into their solutions and the intriguing \"projective phases\" that characterize them. This exploration sheds light on the intricate interplay between spacetime symmetries, relativistic effects, and the fundamental equations governing particle dynamics. \n",
    "Accretion disk theory, while still a vibrant and evolving field, has yet to reach the same level of maturity as stellar evolution theory.  However, this presents an exciting opportunity for groundbreaking research and discovery!\n\nOne of the most promising avenues for advancing accretion disk theory lies in better understanding the crucial role of magnetic fields in transporting angular momentum.  While numerical simulations have illuminated the importance of these fields, especially the magnetorotational instability (MRI), integrating these insights into practical models for comparison with observations remains a compelling challenge.\n\nThis paper champions the need to more accurately incorporate non-local transport processes into these models. We revisit the classic Shakura-Sunyaev (1973) approach, highlighting its inherent nature as a mean-field theory and its limitations in capturing large-scale transport.\n\nObservations of coronae and jets, alongside interpretations of even local shearing box simulations of the MRI, strongly suggest that a substantial fraction of angular momentum transport within accretion disks is non-local. We delve into the intricate physics of MRI-driven transport, demonstrating that Maxwell stresses in the saturated state are dominated by large-scale contributions, challenging the traditional notion of a simple viscosity-based description.\n\nLooking forward, the next generation of global simulations holds immense potential for informing and refining mean-field theories.  We envision a unified framework that seamlessly integrates mean-field accretion and dynamo theories, enabling us to predict the time evolution of spectra and luminosity from individual contributions of the disk, corona, and outflow.\n\nFinally, we emphasize the importance of quantifying the inherent predictive limitations of any mean-field theory.  This crucial step will enable more robust and meaningful comparisons between theoretical predictions and observational data, driving further progress in our understanding of these fascinating astrophysical systems. \n\n\n",
    "This paper tackles the complex behavior of two mathematical models that describe how mixtures of immiscible fluids (like oil and water) move and interact within a confined space. These models, called the Navier-Stokes-Allen-Cahn system (for fluids with different densities) and the Euler-Allen-Cahn system (for fluids with the same density), incorporate the effects of both fluid motion and surface tension between the fluids.\n\nOur main achievement is proving that these models are \"well-posed\"  meaning they have unique solutions that behave predictably over time, even when starting from complex initial conditions. We establish this for both \"weak\" and \"strong\" types of solutions, offering a comprehensive understanding of the models' mathematical properties.\n\nThe key to our proof lies in a clever combination of techniques, including careful analysis of the energy and entropy associated with the systems, a new mathematical tool for estimating products of functions, and a refined approach to handling the complexities of fluid viscosity.  Our results provide a solid foundation for further exploration and numerical simulation of these important models in fluid dynamics. \n",
    "In this presentation, we shall elucidate a novel construction of Fock-space projection operators tailored to represent physically realistic final states encountered in scattering experiments.  Our proposed operators possess two key features: they inherently account for the summation over unobserved quanta and incorporate the constraint of non-emission into specific sub-regions of momentum space.  These attributes render our formalism particularly well-suited for the analysis of experimental data where complete information about the final state is not accessible. \n\n\n",
    "Imagine Feynman diagrams, those intricate scribbles that physicists use to describe the interactions of fundamental particles.  They're not just pretty pictures; they're the backbone of how we calculate what happens in the quantum world.\n\nBut behind these diagrams lies a hidden world of intricate mathematical structures, waiting to be explored. This talk is an invitation to that world, a journey through the elegant landscapes that underpin these quantum calculations. \n\nWe'll uncover the surprising connection between Feynman integrals and \"periods,\" special numbers that appear in geometry and number theory. We'll explore the rhythmic dance of \"shuffle algebras,\" where mathematical objects combine and rearrange in intricate patterns, mirroring the interactions of particles.  And we'll delve into the fascinating realm of \"multiple polylogarithms,\" powerful mathematical functions that capture the essence of these quantum interactions. \n\nThis exploration is not just for mathematicians; it's a quest to unlock new tools and algorithms for tackling the toughest problems in quantum field theory.  By understanding the deep mathematical structures beneath the surface, we can push the boundaries of our knowledge and explore the universe's most fundamental secrets. \n\n\n",
    "We present a calculation of generalized parton distributions (GPDs) for the photon, considering scenarios with non-zero momentum transfer in both transverse and longitudinal directions. These GPDs provide a rich description of the photon's internal structure, encoding information about the spatial distribution and momentum correlations of its constituent partons.\n\nTo gain further insights into this structure, we perform Fourier transforms of the calculated GPDs with respect to both transverse and longitudinal momentum transfer.  This transformation yields the parton distributions of the photon in position space, allowing us to visualize the spatial distribution of partons within the photon. \n",
    "Transformers, a powerful type of neural network, have become superstars in understanding and generating sequences, like text or music.  But they have a bit of a memory problem: they need to store a ton of information about every single element in the sequence, which can make them slow and inefficient when dealing with really long sequences.\n\nThat's where our new model, Memformer, comes in. It's like giving a transformer an external hard drive to store all those memories! Instead of keeping everything in its internal memory, Memformer cleverly encodes and retrieves information from this external dynamic memory.\n\nThis means that even when processing super long sequences, Memformer can keep up, running much faster and using way less memory.  We also developed a special training method called \"memory replay back-propagation\" (MRBP).  This helps Memformer learn long-range dependencies in the data without hogging all the memory.\n\nWe put Memformer to the test and it performed just as well as existing models, but using 8 times less memory and running 3 times faster!  It's like having your cake and eating it too!\n\nWe also peeked inside Memformer's memory and found that it's really good at storing the most important information over time.  This means Memformer can learn from the past and use that knowledge to make better predictions about the future.\n\nSo, if you're looking for a powerful and efficient sequence model that won't break the bank (or your computer's memory), Memformer is the way to go! \n\n\n",
    "This work delivers a powerful theoretical calculation of the cross-section for pseudoscalar Higgs boson production via gluon-gluon fusion, a process of paramount importance in particle physics. Our calculation achieves unprecedented accuracy, capturing the leading logarithmic behavior to all orders in perturbation theory in the high-energy limit.\n\nWe go beyond previous approaches by explicitly incorporating the contributions of both top and bottom quarks, including their crucial interference effects. Our results are presented in terms of compact single and double integrals, which we evaluate analytically up to next-to-next-to-leading order (NNLO).\n\nFurthermore, we leverage our findings to refine the existing NNLO inclusive cross-section calculated within the effective theory framework, where heavy fermion loop contributions are integrated out. Our analysis definitively demonstrates that finite fermion mass effects on the inclusive cross-section are minimal, reaching only a few percent even for large pseudoscalar masses. \n\nThis rigorous calculation provides a benchmark for experimental searches at high-energy colliders, establishing a precise theoretical foundation for exploring the nature of the elusive pseudoscalar Higgs boson and its potential role in extending the Standard Model of particle physics. \n",
    "We understand that identifying outliers in complex datasets can feel like searching for a needle in a haystack.  Take, for example, the challenging task of creating fair and equitable political districts under the Voting Rights Act. Finding districting plans that maximize the number of majority-minority districts, ensuring fair representation for all communities, is incredibly difficult.\n\nTraditional approaches, like unbiased random walks through the vast space of possible districting plans, are often unsuccessful. While biased random walks, favoring plans with more majority-minority districts, offer some improvement, they can get stuck in local optima, missing truly exceptional solutions.\n\nThat's why we've developed a new approach called \"short bursts.\"  Imagine exploring the districting landscape in short, focused bursts of activity.  Starting from a promising plan, we take a few random steps, always keeping track of the best plan encountered. Then, we restart our exploration from this newly discovered high point, repeating the process in a series of focused bursts.\n\nOur research shows that short bursts outperform both unbiased and biased random walks in finding districting plans with a high number of majority-minority districts.  This approach offers a more effective way to navigate the complex landscape of possibilities, revealing solutions that might otherwise remain hidden. \n\nWe've gone beyond our specific case study to explore the effectiveness of short bursts in various settings. Our findings, based on simplified models and more complex scenarios, provide valuable insights into the strengths and limitations of this method.  This research offers a powerful new tool for tackling challenging optimization problems, particularly those where finding outliers is crucial for achieving fairness and equity. \n\n\n",
    "Here are the key points of the text, formatted as a bullet list:\n\n* **Study Focus:** Molecular dynamics investigation of how well non-ionic surfactants with long hydrophilic chains (linear or T-shaped) wet graphitic surfaces. \n* **Surfactant Concentrations:** 1-8 wt%.\n* **Surfactant Length:** Up to 160 Angstroms.\n* **Computational Method:** Coarse-grained molecular dynamics simulations using the MARTINI force field with polarizable water.\n* **Rationale for Coarse-Graining:** Simulating large surfactant molecules requires a substantial number of solvent particles, making coarse-graining necessary for computational efficiency.\n* **Advantages of MARTINI:** Enables exploration of longer timescales and has broader applicability compared to other coarse-grained models.\n* **Accuracy:** While accurate for pure water wetting, MARTINI overestimates micelle formation in surfactant solutions.\n* **Simulation Setup:** To mimic experimental conditions, droplets are prepared with surfactants initially placed near the contact line.\n* **Results:** Simulated contact angles are higher than experimental values.\n* **Value:** Despite limitations, the findings offer valuable insights for initial assessment and screening of surfactant candidates. \n",
    "Within the hushed confines of nanofluidic chambers, a delicate dance unfolds. Superfluid helium-3, a quantum liquid of ethereal grace, pirouettes under the watchful eye of experimentalists. This review celebrates the triumphs of ingenuity and perseverance, where researchers have mastered the art of confining this elusive substance, orchestrating its movements with exquisite precision.\n\nChallenges abound in this realm of the ultra-small, where surfaces whisper secrets to the quantum dancers within.  Yet, like skilled choreographers, scientists have overcome these hurdles, unveiling a stage where the mysteries of superfluidity in thin films can be systematically explored.\n\nThese meticulously crafted nano-stages promise a glimpse into the captivating world of topological superfluids, where exotic excitations pirouette along surfaces and edges, beckoning us to understand their intricate quantum choreography.  The curtain has risen on a new era of exploration, where superfluid helium-3, guided by the hands of artistry and innovation, will reveal the hidden symmetries and profound beauty of the quantum world. \n",
    "We understand the challenges faced by multilingual communities when it comes to accessing information and technology.  Bridging the language gap is crucial, and machine translation plays a vital role in making this happen. \n\nThat's why we're excited about our work on code-mixed machine translation, focusing on the vibrant mix of English and Hindi known as Hinglish.  We participated in the WMT 2022 shared tasks, tackling the task of translating between English+Hindi and Hinglish, recognizing the importance of supporting both Roman and Devanagari scripts.\n\nWe're proud to share that our approach, leveraging the power of the mBART model with some clever pre-processing and post-processing tricks (including transliteration between scripts), achieved top-notch results!  Our system achieved impressive scores in terms of ROUGE-L and WER metrics, demonstrating its effectiveness in capturing the nuances of Hinglish.\n\nThis paper delves into the details of our method for the monolingual to code-mixed translation task, sharing the steps we took and the experiments we conducted. We also discuss our efforts in tackling the reverse direction, translating code-mixed Hinglish into English.\n\nWe're passionate about making technology more inclusive and accessible for everyone.  This research is a step towards breaking down language barriers and empowering multilingual communities around the world. \n",
    "In the quest to unlock the secrets of self-supervised video understanding, contrastive learning has emerged as a beacon of hope. Yet, a hidden danger lurks within this powerful paradigm: the insidious allure of the background scene.  \n\nCurrent approaches naively pit video clips against each other, forcing the model to distinguish between them.  But this strategy often backfires, leading the model astray.  The stark differences in scenes, like a bustling city versus a serene forest, overshadow the more subtle nuances of motion, leaving the model fixated on static backgrounds instead of the dynamic dance of actions.\n\nThis paper unveils a bold new approach, a dual contrastive learning framework that shatters the tyranny of the background scene.  We dissect the visual tapestry of video, teasing apart the static and dynamic elements that intertwine within.\n\nImagine splitting the essence of a video into two complementary streams: one capturing the timeless backdrop, the other pulsating with the rhythm of motion. Our method, dubbed DCLR (Dual Contrastive Learning for Spatio-temporal Representation), compels the model to embrace both these aspects simultaneously.  \n\nWe further refine this dual representation by surgically dissecting the feature space, using activation maps to distill the essence of static and dynamic information.  This meticulous approach ensures that both scene and motion are woven into the very fabric of the learned representation.\n\nThe results are striking. Extensive experiments demonstrate that DCLR empowers models to achieve state-of-the-art performance on challenging video understanding benchmarks like UCF-101, HMDB-51, and Diving-48. This victory signals a new era in video analysis, where the allure of the background scene is overcome, and the true power of spatio-temporal representation is unleashed. \n\n\n",
    "We've gained exciting new insights into the intricate electronic structure of the ferromagnetic material CeRh3B2! Using a combination of cutting-edge computational techniques (FLAPW and LSDA+U methods), we've successfully calculated its electronic bandstructure and Fermi surfaces.\n\nOne of the most intriguing findings is the nature of the ground state for the 4f electrons in this material.  Our calculations suggest a novel scenario:  a fully orbital- and spin-polarized state, where the electrons occupy a specific orbital and spin configuration (|lz=0, sx=1/2>). This challenges the conventional expectation of a ground state dominated by the typical LS coupling observed in many other materials containing 4f electrons.\n\nWhat's even more encouraging is that this unconventional ground state beautifully explains experimental observations! Both the measured magnetic moment and the de Haas-van Alphen (dHvA) oscillation frequencies align perfectly with our theoretical predictions. \n\nOur analysis suggests that this unique ground state arises from a strong \"direct mixing\" interaction between the 4f electrons of neighboring cerium (Ce) atoms.  This interaction is amplified by the remarkably short distance between these atoms along the c-axis of the hexagonal crystal structure.\n\nThis research opens new avenues for understanding and potentially manipulating the magnetic and electronic properties of CeRh3B2 and related materials. It's a testament to the power of combining theoretical calculations with experimental observations to unravel the mysteries of complex quantum materials.  \n",
    "Deep within the Earth's embrace, a silent drama unfolds as acidic fluids etch their way through porous rock, a transformative dance known as matrix acidization.  Simulating this intricate process, with its ever-shifting porosity, poses a formidable challenge to those who seek to understand the subterranean ballet.\n\nThe improved DBF framework, a beacon of insight in this shadowy realm, attempts to capture this fluid choreography.  Its numerical scheme, a delicate balance of mass and momentum, intertwines pressure and velocity into a coupled linear system, a mathematical pas de deux.\n\nYet, this entwined system, with its elusive zeros lurking along the diagonal, demands the brute force of direct solvers, computationally costly partners in this intricate dance.  For large-scale simulations, such a brute force approach becomes untenable, its computational burden too great to bear.\n\nThis work unveils a new choreography, a decoupled scheme that gracefully separates pressure and velocity, allowing them to pirouette independently.  Now, parallel and iterative solvers, nimble and efficient, can join the dance, ensuring a performance both elegant and computationally swift.\n\nThrough a numerical experiment, we demonstrate the harmonious grace and computational prowess of this decoupled scheme.  With this newfound freedom, large-scale simulations of matrix acidization become a reality, allowing us to peer deeper into the Earth's secrets and orchestrate the flow of fluids with unprecedented precision. \n",
    "Sensemaking and narrative are intertwined processes through which humans understand the world. Sensemaking involves connecting new information with existing knowledge, while narratives provide a structured and holistic representation of that understanding.  Both are crucial for human comprehension and hold immense potential for enhancing computational systems.\n\nThis paper examines the theoretical foundations of sensemaking and narrative, highlighting their interconnectedness and exploring how they contribute to human meaning-making. We argue that incorporating these principles can significantly enhance the field of visual storytelling, where computational systems aim to create coherent and engaging narratives from visual data.\n\nWe introduce our novel system for visual storytelling, which leverages sensemaking and narrative principles to generate compelling narratives from images. Through concrete examples, we demonstrate the effectiveness of our approach in creating meaningful and engaging stories, showcasing the potential of integrating human-like understanding into computational systems. \n",
    "Imagine a world where language assessment tools are blind to the rich tapestry of dialects, penalizing those who speak in the vibrant voices of underrepresented communities. This is the unfortunate reality we face with current NLG (natural language generation) evaluation metrics, which often fail to capture the nuances and complexities of dialect variation.\n\nThis paper shines a spotlight on this critical issue, calling for a fundamental shift in how we evaluate generated language. We introduce two crucial concepts: dialect robustness and dialect awareness. These are not mere buzzwords; they are essential goals for building fair and inclusive evaluation tools.\n\nWe equip researchers with a powerful toolkit, a suite of methods and statistical tests, to rigorously assess the performance of metrics in light of these goals. Our analysis reveals a stark truth: current state-of-the-art metrics are woefully inadequate when it comes to dialect robustness.  Shockingly, we discover that introducing dialect features often results in larger penalties than introducing outright semantic errors.\n\nThis unsettling finding underscores the urgent need for change. As a first step towards a more equitable future, we propose NANO, a novel training scheme that infuses regional and language information into the very fabric of a metric's pretraining process.\n\nThink of it as giving these metrics a crash course in dialect appreciation!  Our experiments demonstrate that NANO not only enhances dialect robustness but also improves overall performance on standard benchmarks, all without ballooning the model size.\n\nThis work is a call to action for the NLG community.  By embracing dialect awareness and robustness, we can pave the way for evaluation tools that truly reflect the richness and diversity of human language. \n\n\n",
    "You know how sometimes you use a map to figure out the best way to get somewhere? Well, that's kind of like geographic routing in sensor networks.  It uses location information to help guide data packets through the network.\n\nBut here's the thing:  geographic routing can be tricky in the real world.  Some researchers make assumptions about how the network works that are hard to prove, while others use complex methods that are too expensive for simple sensor networks.\n\nSo, we asked ourselves:  when does geographic routing actually make sense? How can we tell if a network is a good fit for it?\n\nWe came up with four basic principles that define geographic routing and explored what they mean for the network's structure. Then, we came up with a cool new concept called \"geographic eccentricity.\"  Think of it like a measure of how \"map-friendly\" the network is.\n\nFinally, we designed a clever algorithm that can figure out if geographic routing will work well in a given network.  If it's a good fit, the algorithm makes it happen.  If not, it tells you why. \n\nSo, next time you're using your GPS to navigate, think of those tiny sensor networks out there, maybe using a little bit of \"map logic\" themselves! \n",
    "Imagine a microscopic landscape where electrons dance in pairs, forming a delicate ballet of superconductivity.  In this quantum realm, fluctuations  subtle ripples in the superconducting order  hold the key to understanding the emergence of this extraordinary phenomenon.\n\nOur research unveils a hidden complexity within two-band superconductors, materials where electrons waltz in two distinct energy bands. We discover that the spatial variations and correlations of these superconducting fluctuations are governed by not one, but two characteristic length scales.\n\nThink of it like a musical duet, where two instruments weave a richer tapestry of sound than a solo performance. These two length scales create a more intricate pattern of fluctuating superconductivity, far more complex than in their single-band counterparts.\n\nA striking consequence of this two-band symphony is the persistent presence of short-range correlations, even near the critical point where superconductivity emerges.  It's like a whisper of superconducting harmony that persists even as the music of long-range order begins to fade.\n\nThis discovery highlights the profound richness of two-band superconductors, revealing a more nuanced and intricate interplay of quantum fluctuations than previously imagined.  \n\n\n",
    "In a groundbreaking development for forecasting, researchers have unveiled a new class of online prediction algorithms capable of tackling the complexities of real-world time series data.  These algorithms, known as NonSTOP (NonSTationary Online Prediction), directly address the challenge of non-stationarity  those pesky trends and seasonal patterns that plague traditional prediction methods. \n\n\"Our approach is revolutionary,\" said the lead researcher.  \"By applying carefully chosen transformations to the time series data before making predictions, we can significantly improve accuracy.\"\n\nThe key innovation lies in the use of a \"learning with experts\" framework, which allows the algorithms to adapt to different types of non-stationarity, including seasonality, trends in single time series, and even co-integration relationships between multiple time series.\n\n\"This is a major leap forward,\" explained another researcher.  \"Our algorithms not only have strong theoretical guarantees but also perform exceptionally well in practice, outperforming existing methods on both simulated and real-world data.\"\n\nThe researchers emphasize that this work opens up exciting new possibilities for forecasting in diverse fields, from finance and economics to weather prediction and climate modeling. \n\n\n",
    "In the intricate world of logic programming, the question of termination, whether a program's execution will eventually halt, has long haunted computer scientists. This problem, as elusive as a phantom, has been deemed undecidable, its secrets seemingly beyond the reach of any universal algorithm.\n\nThis work presents a daring new approach, not to definitively prove or disprove termination, but to peer into the mists of computation and predict its fate.  Like a seasoned oracle, our heuristic framework offers guidance where certainty remains elusive.\n\nWe introduce the concept of \"termination prediction,\" a beacon of insight illuminating the shadowy paths of program execution. Our approach hinges on a profound understanding of infinite derivations, those unending journeys through the labyrinth of logical inferences.\n\nThrough meticulous analysis, we've crafted an algorithm that deciphers the patterns within these infinite paths, allowing us to forecast a program's destiny with remarkable accuracy.  Our termination prediction tool, a testament to this algorithmic artistry, has been tested on a vast collection of benchmark programs, its performance surpassing even the most sophisticated existing analyzers.\n\nThis research is a testament to the power of human ingenuity, venturing beyond the boundaries of the decidable to illuminate the shadowy realms of computation.  Our framework, a blend of mathematical rigor and intuitive insight, offers a powerful new tool for navigating the complexities of logic programming, empowering us to predict the fate of programs even in the face of undecidability.\n\n\n",
    "Random forests are widely used machine learning models, yet their theoretical underpinnings remain elusive. This paper advances our understanding of random forests in two key ways:\n\n1. **New Theoretical Model:**  We introduce a novel variant of random regression forests specifically designed for theoretical analysis. We rigorously prove that this algorithm is consistent, guaranteeing its convergence to the optimal solution as the amount of data grows.\n\n2. **Empirical Evaluation:** We conduct a comprehensive empirical evaluation, comparing our theoretically tractable model with other simplified random forest variants and the standard, more complex algorithm used in practice.  This comparison sheds light on the trade-offs between theoretical tractability and practical performance, revealing the relative impact of various simplifications made for analytical purposes. \n\nOur work bridges the gap between theory and practice in the realm of random forests, offering valuable insights into their behavior and paving the way for a deeper theoretical understanding of these powerful models. \n",
    "This paper addresses the scalability limitations of Factorial Hidden Markov Models (FHMMs) when applied to long sequential datasets. We propose a novel inference and learning algorithm that leverages concepts from stochastic variational inference, neural networks, and copula modeling to achieve enhanced scalability.\n\nIn contrast to existing techniques that rely on computationally expensive message passing among latent variables, our algorithm circumvents this bottleneck, enabling efficient distributed computation across multiple machines. This distributed architecture significantly accelerates the learning process, particularly for large FHMMs and extensive datasets.\n\nEmpirical evaluations demonstrate that our proposed algorithm maintains the accuracy of the established structured mean-field approach, while exhibiting superior performance when confronted with long sequences and complex FHMMs.  These findings highlight the efficacy and practicality of our algorithm, paving the way for broader application of FHMMs to challenging sequential data analysis tasks. \n",
    "This study uses molecular dynamics simulations to investigate how strong electric fields impact the structure of liquids at the molecular level.  We examine pure water, salt solutions, and polymer solutions, aiming to understand the mechanisms behind the formation of liquid bridges and jets, crucial phenomena in applications like nanofiber production.\n\nOur simulations reveal a striking structural organization within these electrically-induced liquid structures. Molecules align their dipole moments parallel to the applied field, forming chains spanning the entire sample volume.  However, the presence of ions can disrupt this chain-like structure, eventually causing the liquid to break up into droplets.  We quantify this effect, determining the critical electric field strength required to maintain a stable liquid column as a function of ion concentration.\n\nFurthermore, we observe significant conformational changes in polymers during the jetting process, highlighting the impact of electric fields on molecular structure. These findings provide valuable insights into the fundamental physics governing liquid behavior under strong electric fields, with implications for diverse fields ranging from nanotechnology to electrochemistry.  \n\n\n",
    "Recommender systems are transforming how we discover and engage with content, but helping new users find their niche remains a challenge, especially in rapidly growing platforms like podcasting. \n\nThis work tackles the \"cold-start\" problem in podcast recommendations, exploring how to effectively connect new users with engaging audio content. We focus on leveraging music listening habits to infer preferences for podcasts, recognizing the potential synergy between these two audio domains.\n\nOur research utilizes a massive dataset of Spotify users, exploring two innovative techniques for predicting podcast preferences based on their music consumption patterns.  The results are incredibly promising!  We observe a remarkable increase in podcast engagement, with consumption rates boosted by up to 50% in both offline and online experiments.\n\nWe delve deep into the performance of our models, analyzing their strengths and providing a detailed assessment of potential biases introduced by relying on music data as an input source. This research paves the way for creating more personalized and effective podcast recommendations, helping users navigate the ever-expanding world of audio content and discover their next favorite podcast. \n",
    "This study delves into the intriguing realm of the Casimir effect, a quantum phenomenon arising from the fluctuations of the electromagnetic vacuum. We focus on a system consisting of two perfectly conducting spheres, meticulously calculating the Casimir energy and entropy in both the large and short separation limits.\n\nOur calculations reveal a surprising non-monotonic dependence of the Helmholtz free energy on both the separation distance and temperature. This unusual behavior leads to specific parameter ranges where the Casimir entropy, a measure of the system's disorder, becomes negative.  \n\nFurthermore, we uncover non-monotonic trends in the entropy's dependence on temperature and sphere separation.  For small separations, the entropy initially decreases with increasing temperature, defying the traditional expectation of entropy increase with thermal energy.  Similarly, at fixed temperatures, the entropy can exhibit both increasing and decreasing trends as the spheres are moved closer together.\n\nThese intriguing anomalies challenge our conventional understanding of thermodynamics.  We delve into a detailed discussion of the physical origins of this negative entropy, exploring its implications for the system's stability and equilibrium conditions.  \n\nThis research highlights the rich and complex interplay between geometry, temperature, and quantum vacuum fluctuations, offering new insights into the Casimir effect and its profound consequences for the thermodynamics of nanoscale systems. \n",
    "Many real-world problems present us with a vast and complex array of possible actions, making it impossible to explore every option.  This poses a unique challenge for reinforcement learning algorithms, which strive to learn optimal strategies by interacting with their environment.\n\nThis paper introduces a powerful new framework for tackling these challenges, empowering reinforcement learning algorithms to navigate vast action spaces efficiently. Our approach, called sample-based policy iteration, provides a principled way to evaluate and improve policies by strategically sampling subsets of actions, rather than attempting to explore the entire space.\n\nThis framework is incredibly versatile! It can be applied to a wide range of reinforcement learning algorithms based on policy iteration, boosting their ability to handle complex and high-dimensional action spaces.\n\nTo showcase its power, we introduce Sampled MuZero, an extension of the renowned MuZero algorithm. Sampled MuZero excels in learning within domains with incredibly intricate action spaces by cleverly planning over a carefully selected sample of actions.\n\nWe demonstrate the impressive capabilities of Sampled MuZero across diverse domains, including the intricate strategy game of Go and two challenging continuous control benchmarks: the DeepMind Control Suite and the Real-World RL Suite. These successes highlight the transformative potential of our sample-based framework, unlocking new possibilities for tackling real-world problems with complex action spaces. \n\n\n",
    "Imagine a world where we can effortlessly isolate the voices we want to hear, even in noisy and crowded environments.  That's the promise of beamforming, a powerful signal processing technique used in everything from hearing aids to conference calls.\n\nThis paper unveils a new era in beamforming, harnessing the power of deep neural networks (DNNs) to achieve unprecedented levels of accuracy and robustness.  We introduce two innovative mask-based beamforming methods, trained using sophisticated multichannel loss functions.\n\nThink of it like teaching a DNN to become a master conductor, able to orchestrate a symphony of sound waves and extract the desired voices with crystal clarity.  Traditional approaches relied on simpler training methods that didn't directly reflect the true goal of beamforming  separating multiple sound sources in space.\n\nOur multichannel loss functions, inspired by the Itakura-Saito divergence, guide the DNN to learn precise representations of the spatial relationships between sound sources.  This leads to remarkably effective and versatile beamformers that can handle even the most challenging acoustic environments.\n\nThe results are nothing short of transformative.  Our experiments demonstrate the superior performance and robustness of these DNN-powered beamformers, regardless of microphone placement.  This breakthrough opens up exciting possibilities for a wide range of applications, from enhancing speech recognition systems to creating immersive audio experiences. \n\nThis research is a testament to the power of innovation, where the fusion of deep learning and signal processing creates a symphony of clarity, empowering us to hear the world in a whole new way.\n\n\n",
    "Nano-FTIR imaging, a technique that combines the chemical specificity of infrared spectroscopy with the nanoscale resolution of scanning near-field optical microscopy, offers a powerful tool for studying materials at the molecular level.  However, acquiring large-scale images with nano-FTIR is a time-consuming process due to its sequential data acquisition method.\n\nTo address this challenge, researchers have proposed various mathematical approaches that rely on acquiring only a small fraction of randomly selected measurements. While promising in theory, these random subsampling schemes pose practical difficulties for scanning procedures and often fall short of delivering the desired time savings.\n\nOur research explores alternative, more practical subsampling strategies that enable faster data acquisition. We systematically evaluate various schemes, including modified Lissajous patterns and a novel random reflection approach.\n\nOur findings reveal that these strategically designed subsampling schemes, even with a reduction in measurements down to 10%, yield results comparable to those obtained with random subsampling at the same rate.  This suggests that the randomness inherent in previous approaches is not essential for efficient data acquisition.  \n\nThis discovery paves the way for significantly accelerating nano-FTIR imaging, making this powerful technique more accessible for a wider range of scientific investigations. \n\n\n",
    "This study embarks on a journey to explore the intricate connections between two seemingly disparate worlds: the deconfined phase of the (3+1)-dimensional SU(3) pure gauge theory at finite temperature and the broken phase of the 3-state Potts model in three dimensions.  \n\nUtilizing Polyakov loop correlators, powerful probes of the underlying order in these systems, we extract the screening masses associated with different channels of angular momentum and parity. This allows us to compare the relative behavior of these massive excitations in both systems near their respective phase transition points.\n\nWe further investigate the inverse decay length of correlations between specific components of the Polyakov loop, comparing our findings with the predictions of both perturbation theory and mean-field Polyakov loop models. This analysis sheds light on the interplay between different theoretical frameworks in capturing the essence of these complex phenomena.\n\nOur findings provide a glimpse into the subtle relationships between different theoretical models and their ability to describe the intricate dynamics of phase transitions in both gauge theories and statistical mechanics models. This comparative study deepens our understanding of the universality of critical phenomena and the profound connections that unify seemingly distinct areas of physics. \n\n\n",
    "In the realm of anomaly detection, the Mahalanobis distance-based confidence score has recently emerged as a formidable technique, achieving remarkable success in detecting both out-of-distribution (OoD) and adversarial examples. However, this success is predicated upon an assumption that, upon closer examination, appears implausible: the presumption of tied covariance matrices for class conditional distributions of pre-trained features.\n\nThis presentation seeks to unravel the underlying reasons for this method's exceptional performance despite its seemingly flawed theoretical foundation.  Our analysis reveals a compelling insight: the superior performance of the Mahalanobis distance-based approach stems from information that, while irrelevant for classification, proves highly effective for anomaly detection.\n\nThis observation challenges the conventional interpretation, suggesting that the true mechanism behind the success of the Mahalanobis confidence score deviates significantly from its purported reliance on classification prediction confidence.  Furthermore, it highlights a fundamental distinction between this method and ODIN, another widely adopted OoD detection technique, which demonstrably utilizes different information derived from prediction confidence.\n\nMotivated by this newfound perspective, we propose a novel approach that combines the strengths of both methods.  Our combined detector exhibits demonstrably enhanced performance and robustness, showcasing the benefits of integrating complementary information sources. These findings offer valuable insights into the intricate behavior of neural classifiers when confronted with anomalous inputs. \n\n\n\n",
    "Okay, so imagine you have a computer program that can do calculus. It takes a mathematical expression, like \"x^2 + 2x\", and figures out its derivative, which is \"2x + 2\".  But how do you actually describe what this program is doing in a precise and formal way?\n\nWell, you need to talk about two things:  the *syntax* of the expression (what it looks like, the symbols and their arrangement) and the *semantics* (what it actually means mathematically). \n\nA \"syntax framework\" is like a blueprint for describing how a program manipulates these expressions.  It tells you how to break down an expression into its basic parts, how to talk about those parts using a special language, and how to go back and forth between the expression itself and its meaning.\n\nThis paper looks at two different ways to build a syntax framework and use it to formally describe a math algorithm. \n\nOne way is to define the syntactic parts within the formal system itself, but keep the \"quotation\" and \"evaluation\" (going from expression to meaning and back) outside, in a separate \"metatheory.\"\n\nThe other way is to represent *every* expression within the system with its syntactic counterpart, and include \"quotation\" and \"evaluation\" as built-in operations. \n\nSo, it's like two different ways to teach a computer to speak the language of math  one with a separate dictionary and another with the dictionary built right in!  The paper compares these two approaches, helping us understand the subtle ways we can make computers understand and manipulate the language of mathematics. \n",
    "Imagine a microscopic world where two pairs of consumers and their resources are locked in a delicate dance of survival.  The consumers, quick and agile, feast upon their resources, while the resources replenish themselves at a slower, more deliberate pace. This difference in tempo, a separation of timescales, creates a fascinating interplay of dynamics.\n\nOur study unravels this intricate ballet by treating the consumers as fast-moving variables and the resources as their slow-paced partners.  We dissect their interactions, first imagining them as independent pairs, each with a stable equilibrium, a point of harmonious coexistence.\n\nThen, we introduce a subtle twist: a whisper of competition between the consumers, a gentle push and pull that ripples through the system.  This seemingly insignificant coupling unlocks a hidden world of complexity.  The entire system awakens, bursting into a rhythmic pulse of self-sustained oscillations. The period of this rhythmic dance, the time between bursts of activity, stretches far beyond the intrinsic timescales of either individual pair, a testament to the emergent complexity of their interconnectedness.\n\nOur model, a mathematical lens through which we observe this microscopic world, transcends specific biological details. It captures the essence of competition in diverse systems, from populations of organisms vying for limited resources to lasers jostling for dominance through shared cavity losses.  This universal language of coupled oscillators, whispering tales of competition and coexistence, reveals the profound beauty and complexity that emerges from seemingly simple interactions. \n\n\n",
    "Imagine a busy cafe where everyone is trying to upload their vacation photos using the same Wi-Fi.  Some people, closer to the router, enjoy lightning-fast speeds, while others, stuck in a corner, struggle with frustratingly slow connections. This uneven distribution of performance, a common problem in wireless networks, stems from the fickle nature of radio waves.\n\nCooperative MAC protocols, like the cleverly named CoopMAC, emerged as a potential solution, allowing devices to work together and share the wireless medium more effectively. However, our research reveals a hidden trade-off within these cooperative schemes:  a delicate balance between data throughput (how fast you can send data) and energy efficiency (how much energy is needed to send each bit).\n\nWe delve into the theoretical underpinnings of this trade-off, showing how it varies depending on the level of cooperation within the network.  For networks based on the common carrier sense multiple access (CSMA) protocol, we derive a precise mathematical relationship that describes this trade-off curve.\n\nBut we don't stop at theory!  We introduce fairMAC, a new distributed CSMA protocol specifically designed to navigate this trade-off with grace. Our theoretical analysis proves that fairMAC can achieve any desired balance between throughput and energy efficiency, becoming increasingly precise as the size of data packets grows.\n\nWe put fairMAC to the test through extensive simulations, confirming our theoretical predictions.  This research paves the way for building fairer and more efficient wireless networks, ensuring everyone in that crowded cafe can share their vacation photos without a hitch. \n\n\n\n",
    "Social tagging, where users collaboratively annotate web resources with keywords, offers a powerful way to enhance navigation and search. This paper explores integrating social tagging with Wikipedia's structured knowledge base to further improve its usability.\n\nWe propose introducing an interface allowing users to add tags to Wikipedia articles, enriching their metadata and providing alternative navigation pathways.  These tags can enable users to:\n\n* **Discover related articles through \"pivot-browsing.\"**\n* **Explore popular topics based on tag frequency.**\n* **Filter articles based on specific interests.**\n\nFurthermore, social tags can uncover hidden semantic connections not captured by article content, leading to more effective search results.  To evaluate the impact of this approach, we developed a prototype system that enables tagging within Wikipedia and demonstrate its potential for enhancing article discovery and exploration. \n\n\n",
    "Quantum computing, and particularly quantum machine learning, has exploded onto the research scene, igniting a surge of excitement and innovation worldwide!  This burgeoning field holds immense promise for revolutionizing pattern classification, with researchers developing a growing array of models that harness the unique power of quantum principles.\n\nWhile much of the initial focus has been on testing these models with synthetic data, this work takes a bold step forward, applying a complete quantum classifier to real-world image datasets.  The results are incredibly encouraging!\n\nOur quantum classifier exhibits exceptional performance in tackling both balanced and imbalanced classification problems. Notably, it excels in scenarios where the minority class is the most important, a crucial advantage for applications like medical diagnosis, where identifying rare but critical conditions is paramount.\n\nThese findings are a testament to the potential of quantum machine learning to revolutionize real-world applications. This research paves the way for a future where quantum algorithms unlock unprecedented accuracy and efficiency in solving complex classification tasks, particularly in domains where identifying rare events holds the key to groundbreaking discoveries and life-saving advancements. \n\n\n",
    "In our quest to understand the dynamic interplay between supernova remnants and the interstellar medium, we turn our attention to HB 21, a supernova remnant showcasing a captivating interaction between its expanding shockwave and a dense molecular cloud.  This study presents new infrared observations of this interaction zone, located in the southern region of HB 21, obtained using the Infrared Camera (IRC) onboard the AKARI satellite and the Wide InfraRed Camera (WIRC) at the Palomar 5-meter telescope.\n\nOur observations span a range of near- and mid-infrared wavelengths, encompassing the IRC's N4 (4 m), S7 (7 m), and S11 (11 m) bands, as well as the WIRC's H2 v=1->0 S(1) line emission at 2.12 m.  These multi-wavelength images reveal striking diffuse features surrounding a previously identified CO cloud, a clear signpost of the ongoing shock-cloud interaction.\n\nTo unravel the physics behind these emissions, we compare our observations with theoretical models of shocked molecular hydrogen (H2).  We find that the IRC color data is well-described by a thermal admixture model, where the distribution of H2 gas temperatures follows a power-law relation.  This model yields key parameters characterizing the shocked gas: a total H2 column density of approximately 3.9 x 10^4 cm^-2, a power-law index of ~4.2, and a column density of hot H2 gas (T > 100 K) of about 2.8 x 10^21 cm^-2.\n\nWe explore various physical scenarios to interpret these parameters, including multiple planar C-shocks, bow shocks, and a collection of shocked clumps. Each scenario offers potential insights into the shock-cloud interaction, but also presents its own set of limitations.\n\nIntriguingly, the observed intensity of the H2 v=1->0 S(1) line emission is significantly higher (four times) than the prediction from our power-law admixture model. This discrepancy, also observed in the northern part of HB 21, highlights potential limitations of the thermal admixture model and suggests the presence of additional physical processes not fully captured by our current understanding.\n\nOur study provides a detailed glimpse into the complex interplay between a supernova remnant's shockwave and a molecular cloud, emphasizing the power of multi-wavelength infrared observations in revealing the dynamics and physical conditions within these turbulent regions of the interstellar medium. \n",
    "This work presents ParC-Net, a novel convolutional neural network (ConvNet) architecture that achieves state-of-the-art performance for resource-constrained devices while surpassing the efficiency of existing lightweight ConvNets and vision transformer-based models.\n\nDespite the recent success of vision transformers, ConvNets retain distinct advantages in terms of performance and complexity for mobile and resource-limited settings. ParC-Net amplifies these advantages by seamlessly integrating key strengths of vision transformers into a purely convolutional framework.\n\nCentral to our approach is the position-aware circular convolution (ParC), a highly efficient convolutional operation that achieves a global receptive field while preserving location sensitivity, akin to traditional local convolutions. By combining ParCs with squeeze-excitation operations, we construct a meta-former-like block that incorporates an attention mechanism similar to that found in transformers.\n\nThis versatile block can be readily integrated into existing ConvNet or transformer architectures, enhancing their performance without significant computational overhead.  Extensive evaluations across diverse vision tasks and datasets demonstrate ParC-Net's superior performance. \n\nSpecifically, on ImageNet-1k classification, ParC-Net attains 78.6% top-1 accuracy with a mere 5.0 million parameters. This represents an 11% reduction in parameters and a 13% reduction in computational cost compared to MobileViT, while simultaneously delivering a 0.2% improvement in accuracy and a 23% increase in inference speed on an ARM-based Rockchip RK3288 processor.  Compared to DeIT, ParC-Net achieves a 2.7% accuracy gain using only half the number of parameters. \n\nParC-Net also exhibits superior performance on MS-COCO object detection and PASCAL VOC segmentation tasks, solidifying its position as a leading architecture for resource-constrained vision applications.  The source code for ParC-Net is publicly available at https://github.com/hkzhang91/ParC-Net. \n",
    "This research delves into the fascinating interplay between algebraic equations and special functions, revealing hidden connections and generating new mathematical identities. Our journey begins with the deceptively simple polynomial equation: \n\n*  x^n - x + t = 0\n\nWe solve this equation algebraically for the cases n=2, 3, and 4, expressing the solutions in terms of radicals.  Simultaneously, we derive solutions for these same equations using hypergeometric functions, a powerful class of special functions that encompass a wide range of mathematical objects.\n\nThis dual approach unlocks a treasure trove of insights. By comparing the algebraic solutions with their hypergeometric counterparts, we uncover a set of reduction formulas that simplify complex hypergeometric functions into more elementary forms.\n\nOur exploration doesn't stop there.  We further manipulate these reduction formulas, employing differentiation, integration, and known identities involving other special functions. This intricate mathematical dance yields a symphony of new reduction formulas, simplifying a diverse array of special functions and expressing them in terms of more familiar elementary functions.\n\nAs a testament to the power of our approach, we showcase its ability to compute seemingly intractable infinite integrals, reducing them to elegant expressions involving elementary functions. This work not only deepens our understanding of the interconnectedness between algebraic equations and special functions but also provides a valuable toolbox of new identities for mathematicians and physicists alike. \n\n\n",
    "Air-gapped computers, long considered impenetrable fortresses of data security, are increasingly vulnerable to covert channels  stealthy communication pathways that bypass traditional network defenses.  While electromagnetic, acoustic, and optical channels have been demonstrated, this paper unveils a novel and insidious attack vector:  vibrational (seismic) covert channels.\n\nWe exploit a ubiquitous yet overlooked phenomenon: the subtle vibrations produced by computer fans.  These inaudible vibrations, correlated with fan speed, propagate through the structures supporting the computer, creating a hidden communication medium.\n\nOur attack leverages malware's ability to manipulate these vibrations by precisely controlling fan speeds.  The malicious code, dubbed AiR-ViBeR, encodes binary information and transmits it via a low-frequency vibrational carrier.  \n\nAstonishingly, these covert vibrations can be detected by a nearby smartphone using its built-in accelerometer.  The insidious nature of this attack stems from the unrestricted access that apps have to accelerometer data, requiring no user permissions and leaving no trace of malicious activity.\n\nWe detail the attack model, provide the necessary technical background, and present a comprehensive evaluation of AiR-ViBeR's implementation. Our results demonstrate successful data exfiltration from an air-gapped computer to a compromised smartphone situated on the same surface, or even an adjacent table.\n\nThis research raises significant concerns about the security of air-gapped systems, exposing a previously unexplored vulnerability.  To mitigate this emerging threat, we propose several countermeasures, including stricter fan speed control mechanisms, vibration dampening materials, and enhanced monitoring of accelerometer data access patterns. \n",
    "As we search for sustainable and efficient refrigeration technologies, magnetic refrigeration stands out as a promising alternative to conventional vapor-compression systems. This study delves into the economic feasibility of a 25 W average load magnetic refrigerator, utilizing commercially available gadolinium (Gd) as the magnetocaloric material.\n\nOur comprehensive numerical model takes into account not only the initial costs of materials (magnetocaloric material and magnets) but also the long-term operating expenses. We find that for a device with a 15-year lifespan, the total cost falls within a reasonable range of $150 to $400, depending on the market prices of Gd and magnets.\n\nInterestingly, the cost breakdown reveals that the magnets contribute the most to the overall expense, closely followed by operating costs. The price of the magnetocaloric material itself turns out to be negligibly small.\n\nOur analysis identifies a set of optimal design parameters that minimize the total cost. These include a magnetic field strength of 1.4 T, a Gd particle size of 0.23 mm, a regenerator length of 40-50 mm, and a utilization factor of approximately 0.2. These parameters remain remarkably consistent across different device lifetimes and material costs.  The optimal operating frequency, however, exhibits some variation depending on the desired device lifespan.\n\nTo assess the competitiveness of magnetic refrigeration, we compare its lifetime cost to that of a standard high-efficiency (A+++) vapor-compression unit.  Encouragingly, our findings indicate comparable costs, with the magnetic refrigerator potentially holding a slight economic edge if the magnet cost can be recovered at the end of its life. \n\nThis research provides valuable insights into the economic viability of magnetic refrigeration, demonstrating its potential as a competitive and sustainable cooling technology. Further exploration of material advancements and magnet recycling strategies could solidify its position as a frontrunner in the quest for environmentally friendly refrigeration solutions.\n",
    "This work establishes the existence of initial data sets characterized by two distinct asymptotic regions: an asymptotically flat end and an asymptotically cylindrical end. Such geometrical configurations are commonly referred to as \"trumpets\" within the field of numerical relativity. \n\n\n",
    "Enzymes, those remarkable catalysts of life, employ intricate protein structures to achieve astonishing rate enhancements in biochemical reactions. This research delves into the active site of ketosteroid isomerase (KSI), a key enzyme involved in steroid hormone metabolism, to unravel the role of quantum mechanics in its catalytic prowess.\n\nOur investigation combines experimental observations with state-of-the-art ab initio simulations, explicitly incorporating the quantum nature of protons.  We focus on a triad of tyrosine residues within KSI's active site, linked by a network of strong hydrogen bonds.\n\nOur findings reveal a remarkable phenomenon: quantum proton delocalization.  The protons involved in these hydrogen bonds are not confined to specific locations but rather exist in a delocalized state, shared between the oxygen atoms of the tyrosine residues. This delocalization has a profound impact on the enzyme's chemistry.\n\nFirstly, it significantly stabilizes the deprotonated form of a key tyrosine residue, making it a potent base for catalyzing the isomerization reaction. This stabilization manifests as a large isotope effect, with the rate of reaction significantly altered when hydrogen is replaced with its heavier isotope, deuterium.\n\nSecondly, when a molecule mimicking the reaction intermediate binds to the active site, it seamlessly integrates into this hydrogen bond network, extending the range of proton delocalization. This extended delocalization further stabilizes the intermediate, promoting the catalytic process.\n\nThis study provides compelling evidence for the significance of quantum effects in enzyme catalysis, particularly within networks of strong hydrogen bonds. It sheds light on the intricate dance of protons within biological systems, highlighting the importance of considering quantum phenomena to fully comprehend the intricacies of life's molecular machinery.\n\n\n",
    "Imagine wanting to use a powerful image recognition AI model without revealing your private photos to anyone, not even the company running the model. That's where secure inference comes in, and our new framework, ENSEI, makes it faster and more efficient than ever before!\n\nENSEI uses a clever trick called \"frequency-domain secure convolution\" (FDSC). It's like taking a secret message, scrambling it up using special codes (homomorphic encryption and secret sharing), and then doing the calculations in a different domain (frequency domain) where things are much simpler.\n\nThink of it like solving a jigsaw puzzle by first sorting the pieces by color  it makes the whole process much easier!\n\nWe carefully designed and optimized ENSEI to make it super fast and efficient. Compared to existing methods, ENSEI slashes the online processing time by 5 to 11 times, the setup time by up to 33 times, and the total inference time by up to 10 times!  It's like getting your results overnight instead of waiting a whole week!\n\nWe even found a way to reduce the amount of data that needs to be transferred by a third, all while maintaining high accuracy. \n\nENSEI makes private image recognition a reality, allowing you to benefit from powerful AI models without compromising your privacy. \n",
    "In our modern world, awash in a deluge of information and choices, recommender systems have emerged as indispensable guides, expertly curating personalized experiences tailored to individual tastes.  While countless algorithms strive to predict our preferences, most rely on the fundamental principle of similarity, exemplified by collaborative filtering and mass diffusion techniques.\n\nThis work unveils a novel vertex similarity index, christened CosRA, which harmoniously blends the strengths of the cosine similarity index and the resource-allocation (RA) index.  CosRA, like a skilled artisan, weaves together the elegance of geometric relationships with the nuanced insights of resource distribution, forging a powerful tool for uncovering hidden connections within vast datasets.\n\nThrough rigorous evaluation on renowned recommender system benchmarks, including MovieLens, Netflix, and RateYourMusic, we demonstrate the superiority of the CosRA-based method.  It consistently outperforms established benchmarks, delivering enhanced accuracy, diversity, and novelty in its recommendations.\n\nA notable advantage of CosRA lies in its inherent simplicity, requiring no parameter tuning, a boon for practical deployment.  Further experimentation confirms that introducing adjustable parameters fails to yield significant performance gains, underscoring the inherent elegance and efficacy of CosRA's parameter-free design.\n\nThis research paves the way for a new generation of recommender systems, empowered by the discerning eye of CosRA to navigate the ever-expanding sea of choices and deliver truly personalized experiences to users worldwide. \n\n\n",
    "As the volume and velocity of data continue to surge, many real-world applications rely on multi-label data streams, where each data instance can belong to multiple categories simultaneously.  However, these dynamic data streams often exhibit \"concept drift\"  shifts in the underlying data distribution that can cripple the performance of existing classification models.\n\nTo address this challenge, we introduce LD3, the Label Dependency Drift Detector.  LD3 is a novel, unsupervised concept drift detection algorithm specifically designed for multi-label data streams.  It cleverly leverages the inherent relationships between labels, identifying concept drift by detecting changes in these relationships over time.\n\nImagine LD3 as a vigilant watchdog, constantly monitoring the intricate dance of labels within the data stream.  It employs a sophisticated label influence ranking method, powered by a data fusion algorithm, to identify subtle shifts in these relationships, signaling the presence of concept drift.\n\nRemarkably, LD3 is the first of its kind  an unsupervised concept drift detector for multi-label classification. We rigorously evaluated its performance against 14 leading supervised concept drift detectors, adapting them for the multi-label setting. Our experiments, encompassing 12 diverse datasets and a baseline classifier, demonstrate LD3's superior performance.\n\nLD3 consistently outperforms comparable detectors, achieving a remarkable 19.8% to 68.6% improvement in predictive performance across both real-world and synthetic data streams.  This breakthrough algorithm empowers multi-label classification systems to adapt to evolving data landscapes, ensuring their accuracy and reliability in the face of dynamic and unpredictable data streams. \n",
    "The long-standing debate surrounding the universality of Cepheid Period-Luminosity (PL) relations has intensified with the recognition of potential metallicity effects on both the intercept and, more recently, the slope of these relations.  This investigation aims to rigorously calibrate the Galactic PL relations across various photometric bands (from B to K) and compare them to the well-established PL relations for the Large Magellanic Cloud (LMC).\n\nOur analysis utilizes a sample of 59 Cepheid variables with precise distances determined through five independent methods: Hubble Space Telescope and revised Hipparcos parallaxes, infrared surface brightness and interferometric Baade-Wesselink techniques, and classical Zero-Age-Main-Sequence fitting for Cepheids associated with open clusters or OB associations. We carefully address the complexities of absorption corrections and the appropriate projection factor for converting radial velocities to pulsational velocities.\n\nOur findings reveal no statistically significant difference in the slopes of the PL relations between the LMC and our own Milky Way galaxy.  This supports the conclusion that Cepheid PL relations exhibit universal slopes across all photometric bands, independent of the host galaxy, at least for the LMC and Milky Way.\n\nWhile a potential zero-point offset related to metallicity is not addressed in this study, our data suggest an upper limit of 18.50 for the LMC distance modulus. This work provides compelling evidence for the universality of Cepheid PL relation slopes, strengthening their reliability as powerful tools for measuring cosmic distances. \n",
    "Imagine a team of experts working together to solve a complex problem.  Each expert brings unique skills and perspectives to the table, and combining their knowledge can lead to a better solution than any individual could achieve alone.  That's the basic idea behind ensemble methods in machine learning.\n\nHowever, traditional ensembling methods treat all experts equally, even though some might be more reliable or relevant for specific tasks.  Our new approach, \"stacking with auxiliary features,\" addresses this limitation, allowing the ensemble to learn which experts to trust and how to combine their insights most effectively.\n\nThink of it like having a super-smart manager who can figure out which team members are best suited for different aspects of the problem. This manager, our \"stacker,\" uses extra information, or \"auxiliary features,\" to understand not only what each expert predicts but also how they arrived at that prediction.\n\nWe put this approach to the test on three really tough problems: filling in missing information in dialogues, identifying and linking entities across three different languages, and detecting objects in images. \n\nThe results were amazing!  Our method achieved state-of-the-art performance on the first two tasks and significantly boosted accuracy on the object detection task. This shows that \"stacking with auxiliary features\" is a powerful and versatile technique that can improve prediction accuracy across diverse domains. It's like having a dream team of experts, each contributing their unique strengths, all orchestrated by a super-smart manager to achieve a common goal. \n",
    "This paper introduces a novel and computationally efficient method for simulating the complex dynamics of magnetization within nano-pillar spin-valve structures driven by spin-torque effects. Our approach combines two powerful tools: a spin transport code based on random matrix theory and a micromagnetics finite-element software package.\n\nThis synergistic coupling allows us to accurately capture the intricate interplay between spin transport and magnetization dynamics, considering both their spatial variations within the device.  Crucially, our method considers the non-uniform current flow through the nanopillar, a key factor influencing spin-torque-driven magnetization dynamics. \n\nTo validate our approach, we rigorously compare our simulation results with experimental data.  We successfully reproduce several key experimental observations:\n\n* **Spin-wave Mode Excitation:**  Our simulations accurately capture the excitation of various spin-wave modes within the nanopillar.\n* **Threshold Current:**  We accurately predict the critical current required to sustain steady-state magnetization precession.\n* **Nonlinear Frequency Shift:**  Our model captures the nonlinear dependence of the precession frequency on the applied current.\n* **Giant Magnetoresistance (GMR) Effect:**  We reproduce the experimentally observed GMR behavior, reflecting the dependence of electrical resistance on the magnetization configuration.\n* **Magnetization Switching:** We accurately simulate current-induced magnetization switching, a crucial phenomenon for magnetic memory applications.\n\nBeyond its success in modeling conventional spin-valves, our method offers insights into emerging spin-caloritronics devices, which exploit the interplay between spin and heat transport.  This versatile and efficient simulation approach provides a powerful tool for understanding and designing next-generation spintronic devices with enhanced functionality and performance.\n\n\n",
    "We've discovered a simple and elegant way to compute Voronoi diagrams in hyperbolic space! This exciting new approach opens up new possibilities for visualizing and analyzing data in this unique geometric setting.\n\nOur key insight is that hyperbolic Voronoi diagrams, which partition space based on proximity to a set of points, can be represented as affine diagrams  a much easier problem to solve.  We prove that in Klein's non-conformal disk model, the boundaries between regions in a hyperbolic Voronoi diagram are actually hyperplanes, which have a straightforward interpretation as \"power bisectors\" of Euclidean balls. \n\nThis means we can compute a hyperbolic Voronoi diagram by first constructing a special type of Euclidean diagram called a \"clipped power diagram\" and then applying a simple mapping based on the chosen representation of hyperbolic space, like the Poincar disk or upper-half plane model.\n\nOur framework is incredibly versatile! It extends seamlessly to handle weighted Voronoi diagrams, where points have varying importance, and k-order diagrams, which consider proximity to multiple points simultaneously.  We also describe how to construct dual triangulations from these diagrams, providing valuable tools for further analysis.\n\nTo showcase the practical potential of our approach, we explore two useful applications within the context of an image catalog browser designed for the hyperbolic disk.  These applications, finding nearest neighbors and computing smallest enclosing balls, demonstrate how our framework can empower users to navigate and explore large image collections in a more intuitive and efficient manner.\n\n\n",
    "This study investigates the process of bond dissociation in a double-well potential under the influence of an applied external force. The focus is on the impact of finite rebinding probabilities on the dynamic force spectrum, particularly during bond rupture under linearly increasing load (extension) and bond reformation after complete separation (relaxation).\n\nWe calculate the probability distribution of rupture forces and analyze its dependence on the loading rate. At high loading rates, both the rupture and rejoining forces exhibit a predictable dependence on the loading rate, determined by the shape of the potential energy landscape.  As the loading rate decreases, the mean rupture force in extension and the mean rejoining force in relaxation converge, reflecting the system's approach to equilibrium. \n\nFurthermore, we examine the influence of external parameters, such as cantilever stiffness and the presence of a soft linker, on the rupture force distribution and mean rupture force.  Our results indicate that the equilibrium rupture force remains unaffected by a rigid linker. However, introducing linker compliance leads to predictable shifts in the equilibrium rupture force.\n\nImportantly, we demonstrate that the equilibrium constant for bond association and dissociation rates can be extracted from measurements of the equilibrium rupture force. This study provides a comprehensive understanding of bond dissociation dynamics under external forces, highlighting the significance of rebinding probabilities and the influence of experimental parameters on the observed force spectrum. \n",
    "Ever tried to tell a swirling vortex from a smooth, laminar flow? It's harder than it looks!  But fear not, fellow fluid dynamics enthusiasts, for we've devised a cunning plan: a machine learning approach that can sniff out those viscous, rotational troublemakers (boundary layers and wakes) lurking within a flow.\n\nOur secret weapon?  The \"invariant feature space,\" a mathematical fortress impervious to the whims of coordinate systems. We feed our unsupervised Gaussian mixture model a feast of principal invariants from the strain and rotational rate tensors  those tell-tale signs of swirling and stretching.\n\nThink of it like giving the model a pair of x-ray goggles that can see through the chaos and pinpoint the regions where viscosity is calling the shots.  And because we're using Galilean invariants, it doesn't matter if the flow is doing the twist or the tango  our model can handle it!\n\nWe put our method to the test on two unsuspecting circular cylinders, one basking in the laminar tranquility of Re=40, the other caught in the turbulent frenzy of Re=3900.  Armed with a high-order DGSEM solver, we simulated these flows and unleashed our Gaussian mixture model upon the results.\n\nAnd the verdict?  Eureka!  Our model flawlessly separated the flow into two distinct camps: the viscous, rotational rabble-rousers (boundary layers and wakes) and the inviscid, irrotational peacekeepers (outer flow).\n\nBut here's the kicker: unlike those old-fashioned sensors that rely on arbitrary thresholds, our clustering method is completely objective. No more fiddling with knobs and dials! It's like having a robotic referee that calls the shots with perfect accuracy, leaving no room for debate. \n\nSo, the next time you're facing a swirling, turbulent mystery, remember our invariant feature space  it's the Sherlock Holmes of fluid dynamics, always ready to solve the case! \n",
    "It's amazing what we can achieve by building tiny quantum systems in the lab!  Superconducting circuits, like miniature LEGO blocks, offer a fantastic platform for creating and connecting artificial atoms, mimicking the behavior of their natural counterparts. \n\nThis research unveils an exciting new creation: an artificial molecule crafted from two strongly coupled fluxonium atoms. This tiny marvel possesses a unique feature  a magnetic moment that we can control with external signals.\n\nImagine a tiny compass needle whose direction we can change at will! By applying a magnetic flux, we can switch this artificial molecule between two distinct states: one with a magnetic dipole moment, and another with only a magnetic quadrupole moment. This fine-tuned control allows us to explore the molecule's quantum behavior in unprecedented detail.\n\nWe found that the coherence of our artificial molecule, a measure of how long it can maintain its quantum properties, is primarily limited by local fluctuations in the magnetic flux.  This is a valuable insight that will guide us in building even better and more stable quantum devices.\n\nThis research is a thrilling step forward in our quest to harness the power of quantum mechanics.  By learning to engineer and control these artificial molecules, we're opening up a world of possibilities for building more complex quantum circuits, creating protected qubits for quantum computers, and simulating complex physical phenomena that are impossible to study in nature.  The future of quantum technology is bright, and it's built one tiny, artificial molecule at a time! \n\n\n",
    "In time-critical situations, making the right decisions quickly is crucial. Imagine a robot navigating a dynamic environment, or a self-driving car responding to changing traffic conditions.  These scenarios demand swift and effective decision-making involving a sequence of tasks, often under unpredictable circumstances.\n\nThis paper introduces a novel framework for tackling such time-sensitive decision-making challenges.  Our approach utilizes a collection of iterative refinement routines, each specializing in solving a particular aspect of the complex decision-making problem.  \n\nHowever, a key challenge arises: how do we effectively manage the computational resources allocated to these routines, especially when time is of the essence? This is where \"deliberation scheduling\" comes into play.  \n\nWe develop a series of optimization models that capture the intricate trade-offs involved in decision-making under time constraints. These models represent different scenarios and computational strategies, allowing us to tailor our approach to specific situations.\n\nOur framework encompasses two distinct paradigms:\n\n**1. Precursor Models:**  All decisions are made upfront, before any actions are taken.  These models prioritize planning and strategizing, optimizing the entire sequence of actions based on anticipated events.\n\n**2. Recurrent Models:** Decision-making happens concurrently with execution, allowing the system to adapt to unexpected events and incorporate new information as it becomes available.  These models excel in dynamic environments where real-time responsiveness is critical.\n\nFor each paradigm, we develop efficient algorithms to solve the deliberation scheduling problem, ensuring that computational resources are allocated effectively to achieve the best possible outcome within the given time constraints.\n\nOur research goes beyond theoretical models. We present empirical results from extensive experiments, demonstrating the effectiveness of our deliberation scheduling algorithms across a range of time-critical decision-making scenarios.  This work lays a strong foundation for building intelligent systems capable of making timely and informed decisions in the face of uncertainty and complexity. \n\n\n",
    "This paper introduces the role model strategy as a novel approach for estimator design. This strategy seeks to approximate the output of a superior estimator that benefits from enhanced input observations. We rigorously demonstrate that, under a Markov condition, this strategy yields the optimal Bayesian estimator.\n\nThe utility of the role model strategy is illustrated through two examples involving simple communication channels. We further extend the strategy by incorporating time averaging, enabling the construction of a statistical model through the numerical solution of a convex optimization problem.\n\nThe genesis of the role model strategy lies in the domain of low-complexity decoder design for iterative decoding schemes in communication systems. However, its potential applications extend beyond this field, offering a versatile framework for estimator design in various disciplines. \n\n\n\n",
    "Imagine teaching a computer to recognize a fluffy cat, not just in a photograph, but also in a whimsical cartoon, a Renaissance painting, or a child's charcoal sketch.  This is the challenge of artistic object recognition, a task that pushes the boundaries of current computer vision systems.\n\nOur research tackles this challenge head-on, introducing a novel method that empowers computers to see beyond the stylistic variations and recognize objects in diverse artistic modalities  all without requiring labeled data from those modalities!\n\nWe acknowledge that artistic styles can vary dramatically, creating a \"domain shift\" that confuses traditional algorithms.  Our approach confronts this challenge by embracing style transfer techniques, using them to create a \"complementary training modality\" that mirrors the artistic flair of the target domain. \n\nThink of it like giving the computer an artistic crash course!  We use style transfer to transform labeled images from a standard dataset into a collection of artistic renderings, capturing the essence of the target style. By training the network on both the original and stylized images, we force it to learn features that are invariant to stylistic variations.\n\nOur method is remarkably efficient, requiring as few as ten unlabeled images from the target domain to guide the style transfer process. This sets it apart from existing approaches, which often demand vast amounts of unlabeled data.\n\nWe rigorously evaluate our method on a variety of object and scene classification tasks, including a newly released dataset specifically designed to challenge artistic object recognition algorithms. Our results demonstrate a significant boost in accuracy compared to existing domain adaptation techniques, highlighting the effectiveness of our approach.\n\nThis research paves the way for a new generation of computer vision systems capable of appreciating and understanding the diverse world of artistic expression, bridging the gap between human creativity and artificial intelligence. \n\n\n",
    "Imagine a vast, ever-changing landscape where solutions to complex mathematical equations roam freely.  These solutions, like explorers charting unknown territory, embark on journeys that unfold over time, their trajectories shaped by the forces of nonlinearity.\n\nOur research focuses on a particular class of equations known as semi-linear Cauchy problems, where the terrain is governed by quadratic terms involving gradients. These equations, with their intricate interplay of variables and derivatives, often arise in models of dynamic systems, from financial markets to biological populations.\n\nWe delve into the long-term fate of these mathematical explorers, seeking to understand their behavior as time stretches towards infinity.  Our investigation reveals two fascinating possibilities:\n\nFirst, some solutions settle down, their values and gradients converging towards fixed points in the landscape, like explorers finding a peaceful oasis after a long journey.\n\nSecond, we uncover a hidden connection between these solutions and their elusive counterparts, the solutions to associated backward stochastic differential equations. These backward equations, like time-reversed echoes of the original problem, offer a unique perspective on the system's evolution.  We find that, under certain conditions, the solutions to these forward and backward equations converge, like two explorers meeting at a predetermined rendezvous point after charting different paths through the mathematical wilderness.\n\nOur findings are not merely abstract mathematical curiosities; they have profound implications for real-world problems.  In the realm of finance, they illuminate the behavior of risk-sensitive control strategies and long-term portfolio optimization.  Our results provide a deeper understanding of how these systems evolve over time, guiding investors towards more informed and stable financial decisions. \n\n\n\n\n",
    "This study presents a rigorous analysis of the decaying vacuum (DV) model, a compelling alternative to the standard CDM paradigm in cosmology. The DV model postulates that dark energy, the mysterious force driving the accelerated expansion of the universe, is not a constant but rather a dynamically decaying vacuum energy, proportional to the Hubble parameter at late times: (t)  H(t). This decay process gives rise to an additional matter component in the universe.\n\nWe constrain the parameters of the DV model utilizing a comprehensive dataset encompassing supernovae, gamma-ray bursts, baryon acoustic oscillations, cosmic microwave background radiation, the Hubble rate, and X-ray observations of galaxy clusters.  Our analysis reveals a significant finding: the best-fit value for the matter density contrast (m) in the DV model is substantially larger than that derived within the CDM framework.\n\nWe further present confidence contours in the m-h plane (where h is the dimensionless Hubble constant) up to the 3 confidence level, providing a robust statistical assessment of the model's parameter space. Additionally, we present normalized likelihoods for both m and h, offering a detailed probabilistic view of their preferred values within the DV model. \n\nThese findings demonstrate the viability of the DV model as a compelling explanation for the observed cosmological data, challenging the dominance of the CDM paradigm and motivating further exploration of alternative dark energy models. \n",
    "Perpendicular Magnetic Tunnel Junctions (MTJs) based on magnesium oxide (MgO) are leading contenders for building ultra-efficient spin-transfer torque (STT) magnetoresistive memories.  While STT alone has faced challenges in achieving switching current densities below 10^6 A/cm^2, a recent experimental breakthrough [Wang et al., Nature Mater., vol. 11, pp 64-68, Jan. 2012] has demonstrated the exciting potential of electric-field-assisted magnetization switching at remarkably low current densities.\n\nBuilding upon this experimental success, we present a comprehensive micromagnetic study of this novel switching mechanism, going beyond the limitations of previous macrospin approaches. Our simulations unveil a fascinating and intricate nucleation process, where magnetic vortexes play a key role in mediating the magnetization reversal.\n\nThis deeper understanding of the underlying physics unlocks new possibilities for optimizing electric-field-assisted STT switching in MgO-based MTJs. It paves the way for developing next-generation memory technologies with significantly reduced power consumption, enabling faster, more energy-efficient data storage and processing. \n\n\n",
    "Picture the perceptron, that plucky little algorithm from the dawn of machine learning. It's like a toddler learning to categorize shapes, using a simple \"yes\" or \"no\" rule. But what if we could teach it new tricks, expanding its vocabulary beyond those binary choices?\n\nThat's where our generalized perceptron comes in, armed with a fancy new tool: proximal activation functions!  These functions, like a sophisticated set of building blocks, allow the perceptron to learn more complex and nuanced relationships in the data.\n\nBut here's the real kicker:  we've discovered a secret code, a novel energy function, that reveals the inner workings of this generalized perceptron. It turns out that our souped-up perceptron is actually performing a graceful energy minimization dance, guided by a generalized Bregman distance.\n\nThink of it like the perceptron is trying to find the most comfortable position in a bouncy castle, minimizing its energy by adjusting its weights and biases. The beauty of this energy function is that it doesn't require us to differentiate the activation function  it's like finding a shortcut through the mathematical jungle!\n\nThis energy minimization perspective opens up a playground of algorithmic possibilities.  We've already explored one exciting new variant, an iterative soft-thresholding algorithm that encourages the perceptron to learn sparse solutions  like a minimalist artist, it strives to use only the essential features to make its decisions.\n\nSo, the next time you encounter a perceptron, don't underestimate its potential.  With a little bit of mathematical magic and a dash of energy minimization, even the simplest of algorithms can learn to perform impressive feats! \n",
    "The power of sound to move objects has fascinated scientists since the pioneering work of Rayleigh, Langevin, and Brillouin. This \"acoustic radiation force,\" the gentle push exerted by sound waves, has recently fueled remarkable advancements in acoustic micromanipulation, enabling the precise control of tiny objects using sound.\n\nHowever, a fundamental assumption has long constrained our understanding of this phenomenon: the object being manipulated is assumed to be stationary.  This research breaks free from this static paradigm, unveiling a new dimension in acoustic radiation force.\n\nWe delve into the intricate interplay between sound and motion, considering a monopolar source  a point emitting sound waves  moving at a constant velocity.  Our analysis reveals a striking discovery:  the Doppler effect, that familiar shift in pitch as an object approaches or recedes, imparts an unexpected twist to the acoustic radiation force.\n\nThe asymmetry in the emitted sound field, a consequence of the Doppler shift, generates a radiation force that acts against the source's motion.  This \"acoustic drag,\" a consequence of the interplay between sound and motion, challenges our conventional understanding of acoustic radiation force and opens up exciting new possibilities for manipulating objects with sound. \n\n\n\n\n",
    "Accurately modeling the base of the Sun's convective envelope, a thin layer known as the tachocline, poses a significant challenge for solar physicists. This region, where the Sun's rotation transitions from differential to solid-body, plays a crucial role in solar dynamics and is thought to be the birthplace of the Sun's magnetic field.\n\nCurrent solar models struggle to match helioseismic observations of the sound speed profile within the tachocline, highlighting our incomplete understanding of this critical region. This paper presents a novel approach to constrain the tachocline's properties using helioseismology, a technique that probes the Sun's interior using sound waves.\n\nWe perform inversions of the Ledoux discriminant, a measure of convective stability, derived from helioseismic data. By comparing these inversions with predictions from various standard solar models, constructed using different opacity tables and chemical compositions, we gain insights into the sources of discrepancy between models and observations.\n\nThis research utilizes the power of helioseismology to refine our understanding of the tachocline, a region crucial for understanding the Sun's internal dynamics, magnetic field generation, and overall evolution. \n\n\n",
    "Modeling and understanding human behavior is a significant objective within numerous scientific disciplines.  Current research trends reveal a dominant paradigm: human reasoning serves as the implicit benchmark for artificial intelligence.  This is evident in fields like game theory, theory of mind, and machine learning, which incorporate presumed components of human reasoning as techniques to both emulate and comprehend human actions. \n\nFurthermore, the development of next-generation autonomous and adaptive systems envisions human-AI collaboration, necessitating the integration of practical models of human behavior within autonomous agents. These models should facilitate not only the replication of human behavior as a learning mechanism for AI but also the ability to predict and anticipate human actions, enabling true symbiotic interaction.\n\nThis paper presents a concise yet comprehensive review of prominent approaches for quantitatively modeling human behavior.  We focus on two distinct methodologies:\n\n1. **Behavior Learning through Exploration and Feedback:**  This category encompasses techniques such as reinforcement learning, which learn behavioral models or policies through iterative interactions with an environment and the reception of feedback.\n\n2. **Direct Modeling of Human Reasoning Mechanisms:** This approach focuses on explicitly representing cognitive processes such as beliefs, biases, and reasoning patterns without necessarily relying on trial-and-error learning. This category includes probabilistic graphical models, Bayesian inference frameworks, and cognitive architectures. \n\nBy examining these two complementary perspectives, we aim to provide a structured overview of the current landscape in modeling human behavior, highlighting their respective strengths, limitations, and potential applications in developing intelligent systems capable of effective human-AI collaboration. \n",
    "Dismantling botnets, particularly the resilient peer-to-peer (P2P) variants with their decentralized command-and-control (C&C) structures, presents a formidable challenge in cybersecurity.  This research introduces a novel probabilistic method for reconstructing the C&C topology of P2P botnets, addressing the limitations of existing techniques that require complete network visibility or extensive data collection.\n\nOur approach leverages the inherent geographic dispersion of P2P botnet members, recognizing the impracticality of monitoring every individual bot. Instead, we focus on analyzing the propagation of commands through the network, exploiting the subtle timing variations in how bots respond to these commands.\n\nBy combining these inaccurate receiving times with network model parameters and internet delay distributions, our method estimates the probability of connections between bots.  Simulations demonstrate the effectiveness of our approach, achieving over 90% accuracy in reconstructing the edges of a 1000-node botnet with an average node degree of 50, using only 22 command cascades.  Even with limited observations, capturing timing data from only half the bots, our method achieves comparable accuracy using 95 cascades.\n\nThis research provides a powerful new tool for analyzing and disrupting P2P botnets, enabling security researchers to gain valuable insights into their C&C infrastructure and develop more effective countermeasures.  \n\n\n",
    "Within the framework of Grand Unified Theories (GUTs), the possibility of non-universal boundary conditions for gaugino masses at the unification scale has profound implications for the phenomenology of the Minimal Supersymmetric Standard Model (MSSM). In particular, such non-universality can significantly impact the detectability of neutral MSSM Higgs bosons (h/H/A) at the Large Hadron Collider (LHC).\n\nThis study investigates the consequences of non-universal gaugino masses for Higgs boson production arising from the supersymmetric cascade decay chain initiated by gluino production in proton-proton collisions.  The specific decay chain under consideration proceeds as follows: gluino  squark + quark, squark  neutralino_2 + quark, neutralino_2  neutralino_1 + h/H/A, and finally, h/H/A  b + b-bar.\n\nWe demonstrate that in scenarios with universal gaugino masses and a singlet representation, only the light Higgs boson (h) can be produced in this cascade within the parameter region of interest. However, the introduction of non-universal gaugino masses opens up the possibility for the heavy neutral MSSM Higgs bosons (H/A) to dominate production.\n\nFurthermore, we carefully examine the parameter space allowed by the constraints imposed by the Wilkinson Microwave Anisotropy Probe (WMAP) on the relic density of cold dark matter.  Our analysis reveals that in cases with non-universal gaugino masses, the detection of heavy Higgs bosons in the studied cascade is feasible within parameter regions consistent with the preferred neutralino relic density from WMAP observations. \n\nWe also demonstrate that specific combinations of representations can yield the required dark matter abundance across the entire parameter space. These findings underscore the importance of considering non-universal gaugino masses in phenomenological studies of the MSSM, highlighting their potential impact on Higgs boson searches at the LHC and our understanding of dark matter. \n",
    "This paper presents a novel ultracompact electroabsorption modulator for integrated photonic circuits based on a vanadium dioxide (VO2) dual-mode plasmonic waveguide.  The modulator leverages the metal-insulator transition in VO2 to achieve low insertion loss and high modulation depth within a nanoscale footprint.\n\nBy switching the refractive index of VO2, the modulator routes plasmonic waves through either a low-loss dielectric layer (on-state) or a high-loss VO2 layer (off-state). This design achieves a modulation depth of ~10dB with an active volume of only 200x50x220 nm^3 (^3/1700), requiring a mere 4.6V drive voltage.\n\nThis high-performance plasmonic modulator offers a promising solution for realizing fully integrated plasmonic nanocircuits in next-generation chip technologies. \n",
    "With cars becoming increasingly connected, protecting them from theft has become a top priority!  Data mining, biometrics, and enhanced authentication methods are all being explored to combat this growing threat.\n\nHere's where our research comes in! We're leveraging the power of Generative Adversarial Networks (GANs), a cutting-edge machine learning technique, to create a driver identification system that's truly innovative. \n\nTraditional data mining methods rely on supervised learning, requiring labeled data from both legitimate drivers and thieves  but obtaining that thief data is practically impossible!  GANs, however, offer a brilliant solution: they can learn to identify legitimate drivers by training only on their data, without ever seeing a thief's driving pattern!\n\nWe've put this to the test, training a GAN model using real-world driving data from legitimate drivers.  The results are fantastic! Our trained discriminator can accurately distinguish between legitimate drivers and potential thieves. \n\nImagine a system that can seamlessly and securely verify your identity as the authorized driver, all without any extra effort on your part.  By combining our GAN-based approach with other driver authentication methods, we can create a robust and practical anti-theft system for the real world!  This research paves the way for a future where car theft becomes a relic of the past, and drivers can enjoy peace of mind knowing their vehicles are protected by cutting-edge technology.\n\n\n",
    "Slow oscillations (SlO) in magnetoresistance have proven to be incredibly useful for measuring the electronic properties of materials that are nearly two-dimensional.  Think of it like using sound waves to map out the hidden structures within a material.  \n\nOur research explores the potential of extending this technique to more complex materials with multiple electronic bands, like the exciting iron-based high-temperature superconductors.\n\nWe demonstrate that SlO can be effectively used to measure the \"interlayer transfer integral\" in these multi-band systems. This parameter, which describes how easily electrons hop between layers, is crucial for understanding the material's electronic behavior.\n\nFurthermore, SlO allows us to compare the \"effective masses\" and \"scattering rates\" of electrons in different bands. The effective mass tells us how easily an electron accelerates in response to an electric field, while the scattering rate describes how often it bumps into obstacles within the material. \n\nThis research opens up exciting new avenues for investigating the complex electronic properties of multi-band materials, providing valuable insights into their unique behavior and potential applications. \n\n\n",
    "This review summarizes recent advancements in precision calculations for Standard Model processes relevant to Large Hadron Collider (LHC) physics, focusing on weak gauge boson and Higgs boson production.  The content reflects discussions and presentations given at the 27th Rencontres de Blois conference in 2015, highlighting cutting-edge theoretical developments in this active area of particle physics research. \n",
    "Researchers have developed a groundbreaking new method for recognizing emotions in speech, achieving remarkable accuracy by combining the power of audio analysis and text processing. This innovative approach utilizes both acoustic features, like spectrograms and Mel-frequency Cepstral Coefficients (MFCCs), and speech transcriptions to capture the subtle cues that reveal human emotions.\n\n\"By analyzing both the low-level characteristics of speech and the semantic meaning conveyed by the words, our system gains a more comprehensive understanding of the speaker's emotional state,\" explained the lead researcher.\n\nThe team experimented with various Deep Neural Network (DNN) architectures, feeding them different combinations of speech features and text as inputs. Their results, tested on a benchmark dataset, surpassed the accuracy of existing state-of-the-art methods.\n\n\"Our most successful model, a combined MFCC-Text Convolutional Neural Network, achieved exceptional performance in recognizing emotions in the IEMOCAP dataset,\" announced a member of the research team.\n\nThis breakthrough has far-reaching implications for applications ranging from virtual assistants and customer service bots to mental health monitoring and lie detection technologies. With the ability to accurately discern human emotions, these systems can become more responsive, empathetic, and ultimately, more human-like in their interactions. \n",
    "This work presents a novel approach to few-shot learning by incorporating variational semantic memory into the meta-learning framework. We introduce a hierarchical Bayesian model in which a variational semantic memory module accrues and stores semantic information, enabling the probabilistic inference of class prototypes.\n\nThis semantic memory is constructed incrementally, starting from a nascent state and progressively consolidating its knowledge by assimilating information from encountered tasks.  This continual learning process allows the memory to amass generalized knowledge, facilitating the acquisition of novel object concepts.\n\nMemory recall is formalized as a variational inference procedure, inferring a latent memory variable from addressed content. This probabilistic framework provides a principled mechanism for adapting acquired knowledge to individual tasks.\n\nOur variational semantic memory module offers distinct advantages over traditional long-term memory components. Its principled recall and update mechanisms enable efficient accumulation and adaptation of semantic information, specifically tailored for few-shot learning scenarios.\n\nEmpirical evaluations demonstrate that the probabilistic representation of class prototypes, facilitated by our approach, yields a more informative representation compared to deterministic vector embeddings.  Furthermore, our method achieves state-of-the-art performance on four benchmark datasets, consistently surpassing existing few-shot learning techniques.  These results underscore the efficacy of variational semantic memory in significantly enhancing few-shot recognition capabilities. \n",
    "This study investigates the existence and properties of tetraquarks, exotic particles composed of four quarks, using a coupled-channel formalism within a relativistic framework. \n\nHere are the key findings:\n\n* **Four-Quark Equations:** We derive relativistic four-quark equations for systems containing open charm (c) and open strange (s) quarks.\n* **Dynamical Mixing:**  We consider the mixing between meson-meson states and four-quark states, capturing the dynamic interplay between these configurations.\n* **Four-Flavor Amplitudes:**  We construct four-quark amplitudes encompassing up (u), down (d), strange (s), and charm (c) quarks.\n* **Tetraquark Masses:**  The poles of these amplitudes, representing resonances, determine the masses of the tetraquarks.\n* **Mass Calculations:**  We calculate the masses of tetraquarks with spin-parity J^P = 1^- and 2^-.\n\nThese results contribute to our understanding of the complex landscape of multi-quark states and provide theoretical predictions for experimental searches at high-energy particle accelerators. \n",
    "Imagine trying to predict the future of the universe, mapping out the vast cosmic landscape billions of years from now.  That's the ambitious goal of cosmological forecasting, and the Fisher Matrix is its trusty compass.\n\nThis paper introduces Fisher4Cast, a user-friendly software package that makes exploring the Fisher Matrix framework a breeze.  Think of it as a powerful telescope for peering into the future of cosmology.  \n\nFisher4Cast is open-source, rigorously tested, and boasts a slick graphical user interface (GUI) that even generates fancy LaTeX reports and interactive \"Fisher ellipses\" with just a few clicks.  It's designed to be easily customized and extended, and although written in Matlab, it plays nicely with open-source alternatives like Octave and Scilab.\n\nWe showcase Fisher4Cast's capabilities by creating stunning 3D and 4D visualizations of the cosmological forecasting landscape, revealing the interplay between cosmic expansion and the curvature of spacetime. \n\nThis user-friendly software has already made a splash in the cosmology community, with over 750 downloads in its first year!  We're excited to release version 2.2 alongside this paper, complete with a quick start guide and example code.  \n\nWe believe Fisher4Cast will be a valuable tool for researchers across various scientific disciplines, empowering them to explore the mysteries of the universe and beyond.  \n\n\n",
    "Bridging the chasm between the intricate logic of human knowledge and the raw computational power of machine learning is a grand challenge in artificial intelligence.  Symbolic representations, rooted in the elegance of first-order logic, capture the nuances of language and empower us to reason with probabilistic precision. Yet, their rigid structure clashes with the fluid, numerical world of machine learning.\n\nEnter knowledge embedding, a revolutionary approach that transforms symbolic knowledge into a vibrant tapestry of high-dimensional vectors, unlocking the potential for complex reasoning within the realm of machine learning.  These embeddings, like threads of meaning woven into a rich tapestry, preserve semantic relationships and enable quantitative comparisons.\n\nThis work unveils the recursive neural knowledge network (RNKN), a powerful new architecture that seamlessly merges the symbolic richness of first-order logic with the adaptive learning capabilities of recursive neural networks.  Trained on a vast corpus of manually annotated Chinese Electronic Medical Records (CEMRs), RNKN embarks on a quest to master the intricate art of multi-disease diagnosis.\n\nAs RNKN delves into the sea of medical knowledge, it extracts and distills the essence of diagnostic reasoning, forging a powerful alliance between symbolic logic and numerical computation.  Experimental results showcase its prowess, surpassing the diagnostic accuracy of both classical machine learning models and the formidable Markov logic network (MLN).\n\nFurthermore, RNKN reveals a fascinating phenomenon:  as it learns, the knowledge embeddings it constructs become increasingly interpretable, offering a glimpse into the inner workings of its decision-making process. This transparency, a beacon of understanding in the often opaque world of artificial intelligence, paves the way for more trustworthy and explainable diagnostic systems.\n\nThis research is a testament to the transformative power of fusing symbolic reasoning with machine learning, unlocking new frontiers in medical diagnosis and paving the path towards a future where human knowledge and artificial intelligence work in harmonious synergy. \n\n\n",
    "Imagine a high-tech racetrack for particles, where ions zoom around at incredible speeds. This is the world of storage rings, essential tools for studying the building blocks of matter.  \n\nTo keep these speedy ions in check, physicists use electron coolers, devices that act like a soothing \"chill pill\" for the ion beam, reducing its energy spread and keeping it tightly focused.  Solenoids, powerful electromagnets, play a key role in guiding the electron beam within the cooler.\n\nBut here's the catch: those solenoids can also mess with the ions' trajectories! If not perfectly compensated, they can cause the ions to wobble and dance in unexpected ways, making them harder to study.\n\nThis paper dives into this delicate balancing act, investigating the coupled transverse motion of ions in the CSRm storage ring (at the Institute of Modern Physics in Lanzhou, China) caused by those pesky uncompensated solenoids.\n\nThink of it like trying to steer a car with a wonky steering wheel  it's not easy!  To calculate the resulting wobbly ion beam, we've developed a brand new method that captures the complexities of this coupled motion.  \n\nOur research provides valuable insights for optimizing electron cooler design and operation, ensuring that those speedy ions stay on track, allowing physicists to delve deeper into the mysteries of the subatomic world. \n",
    "This research presents compelling evidence for a groundbreaking discovery:  a wide binary system comprised of a white dwarf star and a cool, substellar companion, likely a brown dwarf. \n\nOur investigation began with the detection of a near-infrared excess around the white dwarf PHL5038 in UKIDSS photometry, hinting at the presence of a cooler companion.  To confirm this, we conducted high-resolution spectroscopic and imaging observations using the NIRI instrument on the Gemini North telescope.\n\nOur data unequivocally reveals that PHL5038 is indeed a binary system, spatially and spectrally resolved into two distinct components:  an 8000K DA white dwarf and a likely L8 brown dwarf companion, separated by 0.94 arcseconds. The spectral type of the companion was rigorously determined using established spectral indices for late L and T dwarfs.\n\nWith a projected orbital separation of 55 astronomical units (AU), PHL5038 becomes only the second known wide white dwarf-brown dwarf binary, following the discovery of GD165AB.  \n\nThis remarkable system offers a unique opportunity to test and refine substellar evolutionary models at intermediate to older ages, potentially serving as a benchmark for understanding the evolution of brown dwarfs in wide binary configurations. The identification of this rare and intriguing system highlights the power of combining photometric and spectroscopic observations to unravel the complexities of stellar evolution and the diverse population of celestial objects. \n\n\n",
    "This research provides a valuable new perspective on measuring the mass of our Milky Way galaxy.  We delve into the intricate dynamics of stars in the galactic halo, exploring how the presence of streams and substructures, remnants of past galactic mergers, can impact our understanding of the Milky Way's gravitational pull.\n\nUsing a suite of high-resolution cosmological simulations, we create a detailed map of the halo's phase space, revealing a rich tapestry of structures that vary significantly across different locations within individual galaxies and across our entire simulation suite.  \n\nOur analysis demonstrates that these substructures unevenly populate the high-velocity tail of the halo star distribution, leading to potential discrepancies in mass estimates. We uncover a combination of factors  streams, sample noise, and the inherent limitations of observing stars below the escape velocity  that can lead to underestimates of the true galactic mass.\n\nArmed with these insights, we develop a method to correct for these biases, refining our measurement of the Milky Way's mass. This leads to a revised and more accurate estimate of  1.29 (+0.37/-0.47) x 10^12 solar masses, as presented in Deason et al.\n\nThis study highlights the importance of considering the complex dynamics of the galactic halo when estimating the Milky Way's mass, paving the way for more accurate and robust measurements in the future. \n\n\n",
    "Imagine unlocking the secrets of an object's shape and identity with a mere whisper of light, using just a handful of photons. This research ventures into the realm of quantum sensing, achieving an extraordinary feat: extracting information at a rate exceeding one bit per photon.\n\nOur approach harnesses the intricate dance of photons encoded with high-dimensional orbital angular momentum (OAM) states. These states, like tiny swirling vortexes of light, carry a wealth of information within their complex correlations.\n\nRemarkably, we demonstrate that these OAM correlations remain impervious to the object's orientation. Even as the object pirouettes randomly between measurements, its unique information signature, embedded within its joint OAM coincidence spectrum, remains steadfast.\n\nFurthermore, we unveil the power of OAM correlations to reconstruct complete images of complex, off-axis objects, revealing intricate details hidden from conventional imaging techniques. This quantum lens into the world of light-matter interactions unveils novel object symmetries encoded within the phases of OAM transition amplitudes.\n\nOur exploration delves into the sensitivity of this approach to environmental factors, uncovering a remarkable robustness.  Object symmetry signatures and information extraction rates remain unwavering even as the object shifts within the beam's embrace, as long as it remains sufficiently far from the beam's center.\n\nThis breakthrough paves the way for revolutionary sensing applications, particularly in scenarios where non-invasive measurements and the preservation of delicate quantum states are paramount.  Imagine medical imaging techniques that probe the intricacies of biological structures without causing harm, or remote sensing applications that unveil the hidden symmetries of distant objects with unparalleled precision. This is the promise of quantum sensing, a realm where information flows on the wings of light, revealing the universe's secrets one photon at a time. \n",
    "Get this  Parker Solar Probe, that fearless explorer diving into the Sun's atmosphere, has stumbled upon a wild phenomenon:  magnetic switchbacks! These rapid magnetic field reversals are popping up everywhere in the near-Sun solar wind, like little magnetic tornadoes.\n\nScientists have been buzzing with excitement about these switchbacks, trying to figure out what causes them and how they impact the solar wind.  One big question:  are these switchbacks hotter than the surrounding plasma?\n\nWell, we dug into the data from Parker's Solar Probe Cup instrument, focusing on periods where the solar wind was doing some crazy angular deflections.  And guess what?  The temperature inside those switchbacks is basically the same as outside!\n\nThis means that the usual relationship between temperature and velocity doesn't hold inside these magnetic whirlwinds.  It's like finding a cool oasis in the middle of a scorching desert!\n\nOur findings strongly suggest that switchbacks are like Alfvnic pulses   imagine ripples traveling along magnetic field lines. But where do these pulses come from? That's still a mystery!\n\nWe also found that the radial Poynting flux, a measure of electromagnetic energy flow, doesn't seem to be driving the switchback party. It's like realizing the DJ isn't actually controlling the music!\n\nThis research gives us a tantalizing glimpse into the complex and dynamic nature of the solar wind, and those mysterious switchbacks are keeping us on the edge of our seats, eager for more clues from Parker's future daring dives! \n",
    "Imagine stars, not as static points of light, but as pulsating celestial bodies, their brightness subtly changing over time.  The Nainital-Cape Survey, a dedicated search for these stellar \"heartbeats,\" has uncovered eight intriguing stars known as Delta Scuti variables.\n\nThese stars exhibit rapid pulsations, their brightness fluctuating over periods ranging from minutes to hours. To understand the driving force behind these pulsations, we created detailed computer models of these stars, simulating their internal structure and dynamics.\n\nOur models, encompassing stars with masses between 1 and 3 times that of our Sun, revealed a fascinating insight: several low-order \"p-modes\"  specific patterns of stellar oscillations  are unstable in these stars.  This means that these modes are prone to growing in amplitude, leading to the observed pulsations.\n\nThe pulsation periods predicted by our models match beautifully with the observed periods for these Delta Scuti stars.  We're particularly excited about five stars  HD 118660, HD 113878, HD 102480, HD 98851, and HD 25515  where we can confidently explain their observed variability as arising from the pulsations of these fundamental p-modes.\n\nThis research provides a valuable window into the inner workings of stars, allowing us to understand the physical processes that drive their rhythmic pulsations. It's like listening to the heartbeat of a star, revealing its secrets through the subtle changes in its brightness. \n\n\n",
    "Imagine a world where the laws of physics, those unwavering rules governing the cosmos, are subtly skewed, where the bedrock principle of Lorentz invariance crumbles.  In this realm of \"Lorentz-violating theories,\" particles dance to a different tune, their trajectories defying the expectations of classical physics.\n\nThis research embarks on a bold expedition into this uncharted territory, seeking to unravel the mysteries of particle motion in these exotic landscapes.  We wield the powerful tools of the extended Hamiltonian formalism, forging a new path through the mathematical wilderness.\n\nOur journey begins with a \"Legendre transformation,\" a mathematical bridge connecting the realms of Lagrangian and Hamiltonian mechanics.  This transformation unveils a hidden duality, allowing us to calculate particle trajectories in both momentum space, guided by Hamilton's equations, and velocity space, governed by the Euler-Lagrange equations.\n\nBut this terrain is not without its perils.  Singular points, like treacherous chasms in the mathematical landscape, threaten to derail our exploration.  Undeterred, we impose the elegant constraint of smoothness, demanding that particle trajectories remain continuous and well-behaved in both velocity and momentum.  This simple yet profound requirement tames the wild singularities, allowing us to chart a safe passage through this treacherous terrain.\n\nOur journey leads us to a remarkable discovery:  a hidden correspondence between the specific \"sheets\" of the dispersion relations, those mathematical surfaces that dictate a particle's energy and momentum, and the distinct solutions of the Lagrangian equations.  This revelation provides a deeper understanding of the intricate interplay between a particle's energy, momentum, and its path through spacetime.\n\nWe illuminate our findings by exploring the captivating world of bipartite Finsler functions, exotic mathematical objects that govern particle motion in these unconventional settings.  Our exploration culminates in a breathtaking revelation:  a direct connection between the Lagrangians describing particle motion and the solutions to the Dirac equation, a cornerstone of quantum field theory, in a special case.\n\nThis research is more than just a mathematical odyssey; it's a testament to the boundless power of theoretical physics to illuminate the hidden depths of the universe, even in realms where the fundamental laws of physics take on a new and unexpected form. \n",
    "Efficient spectrum management is crucial for unlocking the full potential of cognitive radio networks (CRNs), enabling them to dynamically adapt and share available radio spectrum. While much research has focused on individual aspects of spectrum management, such as sensing, decision-making, sharing, and mobility, this two-part paper advocates for a more holistic approach.\n\nWe posit that, in certain network configurations, addressing multiple spectrum management tasks concurrently can significantly enhance spectrum utilization. Specifically, we delve into the challenge of uplink resource allocation in a CRN comprised of multiple cognitive users (CUs) and access points (APs). \n\nTo maximize their data transmission rates, CUs must not only choose the most suitable AP (spectrum decision) but also efficiently share the available channels with other CUs associated with the same AP (spectrum sharing). These tasks are inherently interconnected, yet the problem of how to optimally and distributively coordinate them remains an open question in the field. \n",
    "This work introduces an analytically solvable statistical model designed to describe the thermodynamic behavior of baryonic matter under the extreme conditions prevalent in core-collapse supernovae. The model incorporates key features of nuclear interactions, including attractive short-range strong forces and repulsive long-range Coulomb interactions, with the latter partially screened by a background electron gas.\n\nOur analysis reveals a first-order phase transition within the grand canonical ensemble, characterized by a discontinuous jump in the order parameter, which represents the average baryon density. Intriguingly, this phase transition is absent in the canonical ensemble, demonstrating a clear instance of ensemble inequivalence.\n\nThis phenomenon, well-documented in condensed matter physics, is accompanied by several hallmark features: negative susceptibility (a measure of the system's response to external perturbations) and discontinuities in the intensive variables conjugate to the order parameter (e.g., chemical potential and pressure).\n\nThe observed ensemble inequivalence arises from the competition between attractive and repulsive forces within the system, a characteristic inherent to nuclear matter under astrophysical conditions.  This finding has significant implications for understanding the dynamics of core-collapse supernovae.\n\nThe absence of a phase transition in the canonical ensemble, which more closely resembles the conditions within a collapsing stellar core, suggests a smoother evolution of baryon density.  However, the presence of a first-order transition in the grand canonical ensemble highlights the potential for metastable states and non-equilibrium effects, potentially influencing the dynamics of shock wave propagation and neutrino transport within the collapsing core. \n\n\n",
    "Imagine a miniature particle accelerator, a symphony of electric and magnetic fields, coaxing electrons into a perfectly choreographed dance. This is the realm of terahertz free-electron lasers (THz-FELs), powerful beams of light capable of probing the secrets of matter at the atomic scale.\n\nOur quest for a high-performance, compact THz-FEL injector led us to a design that defies convention, eschewing the complexities of a photocathode for the humble yet reliable glow of a thermionic cathode. This electron source, like a tiny incandescent bulb, emits a steady stream of electrons, ready to be sculpted into a beam of unparalleled brilliance.\n\nTo amplify the power of our electron beam, we harnessed the magic of an enhanced EC-ITC RF gun, a device that extracts micro-bunches of electrons with exquisite precision, boosting the effective charge to a remarkable ~200 pC.  This ingenious design also banishes the troublesome \"back bombardment\" effect, ensuring a clean and stable electron source.\n\nOur electrons, like tiny surfers riding waves of energy, are accelerated to a brisk 14 MeV by a series of constant gradient accelerator structures. A carefully crafted focusing system acts as a shepherd, gently guiding the electrons and preserving their tightly packed formation, ensuring a beam of exceptional quality.\n\nThrough meticulous simulations, weaving together the languages of MATLAB and Parmela, we orchestrated the intricate ballet of electrons within our injector.  The results are breathtaking: a continuous stream of high-brightness electron bunches, each a tiny packet of energy, with minimal energy spread and a laser-like focus.\n\nThis compact and efficient injector design paves the way for a new generation of THz-FELs, unlocking unprecedented opportunities for scientific exploration and technological innovation.\n\n\n",
    "Picture the cosmic microwave background, a faint afterglow of the Big Bang, like a sprawling canvas painted across the entire sky.  Within this cosmic tapestry, scientists have been meticulously searching for patterns, clues to the universe's earliest moments.\n\nBut whispers of strange anomalies have begun to circulate.  Rumors of missing power at large scales, a mysterious north-south asymmetry, and an eerie alignment of multipole moments   like hidden messages etched into the cosmic canvas.  \n\nThe challenge lies in separating these whispers from the noise, in determining whether these anomalies are genuine signals of new physics or merely statistical flukes.  The very act of searching for these specific features after the data has been collected introduces a bias, making it incredibly difficult to assess their true significance.\n\nThis is a detective story on a cosmic scale, where we must tread carefully to avoid being misled by false clues.  One promising strategy is to seek out independent witnesses, new datasets that probe similar physical scales as the large-angle CMB.  \n\nFinding these independent witnesses is no easy feat, but the potential reward is immense.  If these anomalies are confirmed, they could revolutionize our understanding of the universe, pointing towards new physics beyond our current theories. The search for truth in the cosmic microwave background is a thrilling quest, where every clue, every anomaly, could lead us to a profound discovery. \n",
    "Multi-photon states, generated through multiple parametric down-conversion (PDC) processes driven by high-power pumping of nonlinear crystals, offer a platform for exploring the foundations of quantum mechanics.  The degree of conflict between these states and local realistic descriptions increases with the population of multi-photon states.  However, experimental realizations often face limitations due to low interference contrast, particularly at high pumping powers.\n\nThis work proposes a method for enhancing interference contrast in multi-photon PDC experiments. The approach utilizes readily available optical components known as multiport beam splitters, which can split an incoming light beam into multiple output modes. This scheme functions as a Positive Operator-Valued Measure (POVM) filter, effectively improving the signal-to-noise ratio in the measurement.\n\nThe enhanced contrast facilitated by our method could enable feasible tests of the CHSH-Bell inequality, a cornerstone for demonstrating the non-locality of quantum mechanics.  This improved experimental capability has potential applications in various quantum information protocols, including those aimed at reducing communication complexity. \n",
    "This work unveils a profound and unexpected connection between the seemingly disparate realms of Anderson localization and non-Hermitian quantum mechanics. Anderson localization, a fundamental phenomenon in condensed matter physics, describes how disorder can trap electrons, preventing them from propagating through a material. \n\nWe focus on the iconic Anderson model, a theoretical cornerstone for understanding this phenomenon. The localization lengths, quantifying the spatial extent of electron wavefunctions, are encoded within the spectrum of exponents of the transfer matrix, a mathematical object describing the propagation of electrons through the disordered lattice.\n\nOur research unveils a surprising new formula for this spectrum, derived using a powerful combination of mathematical tools: a duality identity for determinants and Jensen's identity for subharmonic functions. This remarkable result expresses the localization length spectrum in terms of the eigenvalues of the Anderson Hamiltonian subject to unconventional, non-Hermitian boundary conditions.\n\nThis connection is profound because it transcends the traditional reliance on disorder averaging. Our formula is exact and instead involves an average over a Bloch phase, a parameter associated with the periodicity of the underlying lattice. \n\nWe present a preliminary investigation of the non-Hermitian spectra for the Anderson model in one and two dimensions, focusing on the behavior of the smallest exponent, which corresponds to the longest localization length.  This research opens a new avenue for understanding Anderson localization, bridging the gap between this fundamental phenomenon and the burgeoning field of non-Hermitian quantum mechanics. \n",
    "Imagine you're trying to predict something, like the price of a stock or the temperature tomorrow. You have some data, but it's noisy and there's not a lot of it.  That's where extreme learning machines (ELMs) come in  they're like super-fast learners that can make predictions even with messy data.\n\nBut here's the cool part: we've found a way to make ELMs even better using something called \"graph signal processing.\"  Think of it like giving the ELM a map that connects all the data points.\n\nThis map, or \"graph,\" tells the ELM how the data points are related to each other.  We then use a special trick called \"regularization\" to make sure that the ELM's predictions are smooth and consistent with the map.\n\nIt's like telling the ELM, \"Hey, don't just focus on individual data points, look at the bigger picture and make sure your predictions make sense in the context of the map!\"\n\nWe tested this approach on real-world data and it worked like a charm!  When the training data was limited or noisy, our \"graph-savvy\" ELM made significantly better predictions than a regular ELM.  \n\nSo, next time you're trying to predict something with limited or messy data, remember the power of graphs!  They can help your ELM see the forest for the trees and make more accurate predictions. \n\n\n",
    "Imagine a pot of soup simmering on a stove. The heat from the stove causes the soup to bubble and swirl, creating complex patterns of motion. This is similar to what happens in turbulent plasmas, like the solar wind, where particles are constantly moving and interacting.\n\nIn these plasmas, collisions between particles play a crucial role in heating the system.  Think of it like the collisions between soup molecules transferring heat throughout the pot.  \n\nRecently, scientists discovered that these collisions are even more important than we thought, especially when the plasma has fine-scale structures in velocity space.  Imagine tiny whirlpools within the soup, increasing the rate at which heat is transferred.\n\nThis study digs deeper into this phenomenon, comparing two different ways of describing these collisions: a full, nonlinear model and a simplified, linearized model.  \n\nWe found that using the full, nonlinear model is crucial for accurately capturing the heating process.  While both models reveal the presence of multiple timescales associated with different types of motion, the nonlinear model shows that these motions dissipate much faster.\n\nIt's like realizing that those tiny whirlpools in the soup actually disappear much quicker than we initially thought, leading to faster heating.  \n\nThis research highlights the importance of considering the full complexity of particle collisions when studying turbulent plasmas, especially in astrophysical environments like the solar wind.\n\n\n",
    "As semiconductor manufacturing pushes towards ever-smaller feature sizes (below 32nm), detecting and classifying microscopic defects becomes increasingly challenging. Traditional rule-based methods employed by optical and e-beam inspection tools often struggle with accurate classification, leading to costly misclassifications and requiring time-consuming human intervention.\n\nThis research presents a significant advancement in automated defect inspection by leveraging the power of Mask R-CNN, a deep learning algorithm renowned for its object detection and instance segmentation capabilities. Building upon our previous work in deep learning-based defect analysis, we extend the approach to achieve precise defect instance segmentation in Scanning Electron Microscope (SEM) images.\n\nOur method not only identifies and classifies defects but also generates precise masks outlining the extent of each defect instance. This enables:\n\n* **Accurate Quantification:**  We can extract and calibrate each segmented mask, quantifying the number of pixels associated with specific defect categories. \n* **Defect Counting and Area Measurement:**  We can accurately count individual defect instances for each category and calculate their surface area in terms of pixels.\n\nOur focus is on detecting and segmenting a diverse range of challenging stochastic defect patterns, including bridges, breaks, and line collapses. Crucially, our method can differentiate between subtle variations within defect categories, such as distinguishing between thin, single, multi-line, horizontal, and non-horizontal bridge defects. This level of granularity is essential for advanced semiconductor processes involving aggressive pitches and thin resist layers (high numerical aperture applications).\n\nThrough rigorous quantitative and qualitative evaluation, we demonstrate the effectiveness of our proposed approach. It delivers superior accuracy compared to traditional methods, paving the way for fully automated, highly accurate defect inspection in advanced semiconductor manufacturing. \n\n\n",
    "Imagine a vast network of interconnected points, a complex graph where every connection follows strict rules.  This is the realm of distance-regular graphs, mathematical structures with intricate symmetries and hidden patterns. \n\nOur research focuses on a key parameter, , which counts the number of common neighbors shared by any two connected points in this graph.  Think of it as a measure of local interconnectedness.\n\nBuilding on the work of Spielman and Pyber, who established bounds for a specific type of graph called strongly regular graphs, we unveil a tighter and more general bound for .  This discovery is not merely a mathematical curiosity; it's a key ingredient in recent breakthroughs concerning the complexity of determining whether two strongly regular graphs are essentially the same (isomorphic).\n\nOur proof relies on a remarkable geometric structure within these graphs, unearthed by Metsch under certain conditions.  Imagine a constellation of cliques, tightly knit groups of points where everyone knows everyone else. Metsch showed that, when certain relationships between the graph's parameters hold, these cliques emerge with a predictable size and arrangement.\n\nWe provide a streamlined proof of this geometric insight, demonstrating that when the product of the graph's degree (k) and a parameter related to connections at distance 2 () is much smaller than  squared (k = o(^2)), something magical happens.  Each connection in the graph belongs to a unique, maximal clique of size roughly equal to , while all other cliques fade into insignificance.\n\nThese special cliques, we reveal, are \"asymptotically Delsarte,\" echoing the properties of a well-known construction in coding theory.  This connection hints at a deep and unexpected link between these seemingly disparate areas of mathematics.\n\nOur work provides a powerful new lens for understanding the structure of distance-regular graphs, illuminating their hidden symmetries and revealing the surprising role of cliques in shaping their properties. \n\n\n",
    "Astronomers have noticed that galaxies form stars at different rates depending on their surroundings.  To understand this diversity, it's crucial to study how giant molecular clouds, the birthplaces of stars, form and evolve in different galactic environments.\n\nOne puzzling observation is that bars in strongly barred galaxies seem to be missing massive stars, even though there's plenty of gas available for star formation.  It's like having all the ingredients for a cake but finding no oven to bake it!\n\nTo investigate this mystery, we created a computer simulation of a strongly barred galaxy, using the structure of NGC 1300 as a template.  We compared the properties of molecular clouds in three different environments: the bar, the bar-ends, and the spiral arms.\n\nWe found that clouds in all these regions are gravitationally bound  meaning they're held together by their own gravity  so that's not the reason for the lack of massive stars in the bar.  Instead, we focused on cloud-cloud collisions, which are thought to be essential for triggering the formation of massive stars. \n\nOur simulation revealed that clouds in the bar collide much faster than in other regions.  It's like a cosmic demolition derby! We discovered that this frantic motion is caused by the bar's gravity, which forces the gas into elongated orbits, leading to more frequent and violent collisions.\n\nThese findings suggest that the lack of star formation in the bars of strongly barred galaxies is caused by these speedy collisions.  When clouds collide too fast, they don't have enough time to collapse and form massive stars. \n\nSo, it seems that the bar's strong gravitational influence, while stirring up the gas, also hinders the birth of massive stars, creating a cosmic paradox of plenty of ingredients but a lack of stellar ovens. \n",
    "This investigation presents a comprehensive analysis of the mass-metallicity relationship (MMR) for star-forming galaxies within the Galaxy And Mass Assembly (GAMA) survey.  The MMR, a fundamental correlation in astrophysics, describes the observed trend of increasing metallicity (heavy element abundance) with increasing galaxy mass.\n\nWe rigorously derive oxygen abundances using robust strong emission line ratio diagnostics. Subsequently, we apply a systematic range of selection criteria based on signal-to-noise ratios of various emission lines, as well as apparent and absolute magnitudes.  This meticulous approach allows us to identify potential sources of discrepancies in the MMR reported in previous studies.\n\nOur analysis demonstrates that the shape and position of the MMR can vary significantly depending on the adopted metallicity calibration and sample selection criteria.  Employing a robust metallicity calibration, we find that the MMR for redshifts 0.061 < z < 0.35 in the GAMA survey is consistent with that derived from the Sloan Digital Sky Survey (SDSS), despite probing a different luminosity range.\n\nThese findings underscore the importance of carefully considering methodological choices when comparing MMRs across different surveys and studies.  Our results caution against direct comparisons without accounting for potential variations arising from sample selection and analysis techniques.  \n\nFurthermore, our analysis suggests a modest level of evolution in the MMR within the GAMA sample over the redshift range 0.06 < z < 0.35. This work provides a robust and detailed characterization of the MMR in the GAMA survey, offering valuable insights for understanding the interplay between galaxy mass, metallicity, and cosmic evolution. \n",
    "Imagine two fluids, like oil and water, flowing through a sponge. Understanding how these fluids move and interact within the sponge's intricate network of pores is a challenge in fluid dynamics. \n\nOur research presents a new way to describe this complex two-phase flow, based on the fundamental principles of thermodynamics. We've derived a set of equations that relate the speeds of each fluid to the forces driving their motion, like pressure differences.\n\nOur secret ingredient?  A new concept called the \"co-moving velocity,\" which captures how the structure of the sponge itself influences the flow. Think of it like the sponge's own \"personality\" that shapes how the fluids move through it.\n\nWe put our theory to the test, first by solving four different versions of the classic \"capillary tube\" model  a simplified representation of a porous medium.  Then, we took it a step further, using a computer simulation of a complex network of interconnected tubes to mimic a real-world sponge.\n\nThe results were spot on!  Our equations accurately predicted the flow behavior in both the simple and complex models. This research provides a powerful new tool for understanding multi-phase flow in porous media, with applications in fields like oil recovery, groundwater management, and even designing better filters! \n",
    "Life's boundless creativity, evident in the staggering diversity of forms and functions across the biosphere, stands as a hallmark distinguishing the living from the nonliving. It is this very essence of open-ended evolution (OEE) that captivates the field of Artificial Life (ALife), inspiring researchers to replicate the dynamic and unpredictable emergence of novelty observed in nature.\n\nThis article serves as an introduction to the second of two special issues dedicated to cutting-edge research in OEE, offering a comprehensive overview of the contributions within both volumes.  The majority of the featured work was initially presented at a dedicated workshop on OEE during the 2018 Conference on Artificial Life in Tokyo, building upon the foundation laid by two previous ALife workshops in Cancun and York.\n\nWe present a streamlined categorization of OEE research, highlighting the key themes and approaches employed in this vibrant field.  Through a concise summary of the articles contained within this special issue, we illuminate the significant progress made towards understanding and replicating the principles of OEE in artificial systems.  This collection of research represents a crucial step towards unraveling the mysteries of life's boundless creativity and harnessing its power for technological innovation.\n",
    "Imagine a microscopic layer cake, where ultrathin films of magnesium oxide (MgO) are delicately layered atop a silver (Ag) surface.  These atomically thin structures hold immense potential for tailoring material properties and designing next-generation electronic devices.\n\nThis research delves into the fascinating world of these MgO/Ag interfaces, exploring how strategically placing magnesium atoms within the silver layer can dramatically alter the electronic landscape.  We employ a powerful trio of experimental and theoretical techniques  Auger electron diffraction, ultraviolet photoemission spectroscopy, and density functional theory (DFT) calculations  to unravel the secrets of these modified interfaces.\n\nLike skilled detectives, we use the layer-by-layer resolution of Auger electron spectra to map out the atomic arrangement, revealing a striking distortion of the interface layers caused by the presence of those extra magnesium atoms.  This distortion, a subtle shift in the atomic dance at the interface, has a profound impact on the material's electronic properties, leading to a significant reduction in its work function  a measure of how easily electrons can escape the surface.\n\nOur DFT calculations, like a virtual microscope peering into the quantum world, confirm and explain these experimental observations.  They not only reproduce the observed lattice distortion but also reveal a fascinating electron transfer from magnesium to silver atoms at the interface.  This redistribution of charge, like a microscopic tug-of-war, is responsible for the observed work function reduction.\n\nOur investigation delves deeper, dissecting the contributions of various factors, including charge transfer, atomic \"rumpling,\" and electrostatic compression, to the overall work function change.  The verdict?  The dominant force behind this electronic transformation is the enhanced electrostatic compression effect, a consequence of the subtle interplay between charged atoms at the interface.\n\nThis research illuminates the power of atomic-scale engineering to manipulate the electronic properties of materials, opening exciting new avenues for designing high-performance electronic devices with tailored functionalities. \n\n\n\n",
    "Industrial process monitoring is plagued by the persistent problem of incomplete observations.  Critical process parameters often fall below the detection limits of measuring systems, resulting in censored data that obscures the true state of the process.  This issue is particularly acute in scenarios with high levels of censorship, exceeding 70%, rendering traditional monitoring methods unreliable and potentially dangerous.\n\nThe inadequacy of conventional approaches stems from their inability to handle the complexities of censored data.  They fail to provide accurate assessments of the process state, leaving operators blind to potential problems and hindering timely corrective actions.\n\nThis paper attempts to address this deficiency by proposing a new method for estimating process parameters in the presence of censored data.  However, the proposed algorithm and its corresponding control chart offer only a limited solution.  The complexities of high censorship levels and the inherent uncertainties associated with censored data continue to pose significant challenges for effective process monitoring, leaving room for potentially catastrophic failures. \n\n\n",
    "Clustering, a cornerstone of data analysis, has traditionally focused on distance metrics and grouping algorithms.  This work breaks new ground by introducing Deep Embedded Clustering (DEC), a powerful method that fundamentally reimagines clustering through the lens of deep learning.\n\nDEC harnesses the representation learning capabilities of deep neural networks to simultaneously learn optimal feature representations and cluster assignments.  It achieves this by mapping the input data to a lower-dimensional feature space, where a clustering objective is iteratively optimized.\n\nOur comprehensive experimental evaluations, spanning both image and text datasets, demonstrate DEC's undeniable superiority.  It consistently surpasses existing state-of-the-art clustering methods, achieving significant improvements in clustering accuracy and revealing more meaningful data groupings.\n\nThis research establishes DEC as a leading approach for clustering, harnessing the power of deep learning to unlock new insights from complex datasets and revolutionize data-driven applications across diverse domains. \n",
    "This study builds upon the work of Sarvotham et al. [2005], which investigated the relationship between peak transmission rate and network burstiness.  We analyze TCP packet headers, grouping packets into sessions characterized by a 5-tuple: total payload (S), duration (D), average transmission rate (R), peak transmission rate (Peak R), and initiation time (Initiation T).\n\nOur analysis necessitates a revised definition of peak rate.  Departing from the two-group (alpha and beta) segmentation used by Sarvotham et al., we demonstrate the heterogeneity of the beta group by segmenting sessions into 10 groups based on empirical peak rate quantiles. This finer segmentation reveals nuanced structural characteristics not captured by a simple two-group division.\n\nExamining the dependence structure of (S, D, R) within each segment, we observe variations across groups.  Furthermore, while session initiation times within each segment closely resemble a Poisson process, this property does not hold for the entire dataset. \n\nThese findings highlight the importance of peak rate as a key factor influencing network structure and behavior.  We propose that incorporating peak rate information is essential for constructing accurate simulations of real-world network traffic. A basic methodology for such traffic simulation, based on our observations, is outlined. \n\n\n",
    "The Brouwer fixed-point theorem, a cornerstone of topology, offers a fascinating lens through which to explore the potential existence of traversable wormholes.  This theorem guarantees that for any continuous function mapping a compact convex set onto itself, there must exist at least one point that remains unchanged by the function.\n\nIn the context of wormhole geometry, we can relate this fixed point to the \"throat\" of a wormhole, a crucial point where spacetime curves dramatically, potentially connecting distant regions of the universe.  \n\nRemarkably, we can leverage the Brouwer fixed-point theorem to demonstrate that, under certain conditions, the existence of this wormhole throat is a direct consequence of the mathematical structure of spacetime.  This suggests that the possibility of traversable wormholes isn't just a wild speculation; it emerges naturally from established mathematical principles without requiring us to venture beyond currently accepted physical laws. \n\nThis connection between abstract mathematics and the physical world offers a tantalizing glimpse into the potential for wormholes to exist, inspiring further investigation into their properties and the conditions necessary for their formation. \n",
    "Within the intricate labyrinth of the human body, medical images reveal a hidden world of anatomical structures, a delicate dance of tissues and organs.  To decipher this visual symphony, researchers have turned to the power of convolutional neural networks (CNNs), those digital virtuosos capable of recognizing patterns and extracting meaning from vast amounts of data.\n\nYet, traditional CNNs, with their two-dimensional gaze, struggle to fully grasp the three-dimensional complexity of medical scans. This work unveils a novel approach, a volumetric CNN that embraces the full depth and richness of medical volumes.\n\nTrained on a collection of MRI scans depicting the prostate, our CNN learns to paint a precise segmentation map, outlining the organ's boundaries with a single, graceful stroke. We introduce a new objective function, the Dice coefficient, as our guiding star, ensuring that even in the face of imbalanced data, where foreground and background voxels compete for attention, our network remains steadfast in its pursuit of accuracy.\n\nTo overcome the scarcity of annotated training data, we employ a symphony of random non-linear transformations and histogram matching, creating a kaleidoscope of augmented data that expands the network's visual vocabulary.\n\nOur experimental evaluation reveals the brilliance of this approach.  Our volumetric CNN achieves remarkable accuracy on challenging test data, all while performing its intricate calculations with a fraction of the time required by its predecessors.\n\nThis research marks a pivotal step towards a future where intelligent algorithms, guided by the artistry of deep learning, illuminate the hidden depths of medical imaging, empowering clinicians with tools of unprecedented precision and insight.\n\n\n",
    "The energy levels of a two-body system interacting through the Coulomb force are described by the famous Balmer series, derived from the non-relativistic Schrdinger equation.  However, in 1954, Wick and Cutkosky, using the relativistic Bethe-Salpeter equation, discovered the existence of additional energy levels when the interaction strength () exceeds /4. \n\nThe physical nature of these extra states remained a puzzle, leading to doubts about their existence. Our recent work resolves this mystery, demonstrating that these states are primarily governed by the exchange of massless particles traveling at the speed of light. \n\nBecause these massless particles are inherently relativistic, they are not captured by the non-relativistic Schrdinger equation, explaining why the extra states were absent in previous calculations. \n",
    "This research delves into the heart of quantum information theory, exploring the fundamental properties of a powerful new concept: the quantum f-relative entropy. This generalized entropy measure, defined using an operator convex function f(.), unlocks a deeper understanding of quantum information processing tasks, from channel capacity to entanglement manipulation.\n\nWe rigorously establish the equality conditions for two crucial properties: monotonicity and joint convexity. These conditions, essential for a well-behaved entropy measure, are remarkably general, holding for a broader class of operator convex functions than previously known.  Intriguingly, for the specific case of f(t) = -ln(t), our conditions reveal new and unexpected nuances.\n\nBuilding upon this foundation, we define the quantum f-entropy and unveil its key properties, deriving precise equality conditions in several cases. This exploration illuminates the rich mathematical structure of this generalized entropy framework.\n\nWe then extend our analysis to crucial information-theoretic quantities, demonstrating that the f-generalizations of Holevo information, entanglement-assisted capacity, and coherent information all obey the data processing inequality. This fundamental inequality ensures that information cannot be increased by processing alone, a cornerstone of both classical and quantum information theory.  \n\nFurthermore, we derive the equality conditions for the f-coherent information, providing a deeper understanding of the limits of information transmission through quantum channels.  This work lays the foundation for a powerful new framework for understanding quantum information, paving the way for advances in quantum communication, cryptography, and computation.  \n\n\n",
    "A fundamental principle in computer vision is that simple transformations like translations or rotations of an image should not alter the outcome of tasks such as object recognition.  While convolutional neural networks (CNNs) naturally exhibit translation equivariance (shifting the input image shifts the feature maps accordingly), achieving rotation equivariance is more challenging.\n\nCurrent approaches rely on data augmentation to achieve global rotation equivariance, but attaining equivariance to local, patch-wise rotations remains elusive.  This work introduces Harmonic Networks (H-Nets), a novel CNN architecture that exhibits equivariance to both patch-wise translations and 360-degree rotations.\n\nWe achieve this by replacing standard CNN filters with circular harmonics, which provide a maximal response and orientation for every receptive field patch, regardless of its rotation. H-Nets utilize a rich, yet parameter-efficient and computationally frugal representation. Analysis reveals that deep feature maps within the network encode complex rotational invariants, capturing the essence of the input image regardless of its orientation.\n\nOur H-Net layers are versatile and can be seamlessly integrated into existing architectures, complementing techniques like deep supervision and batch normalization.  We demonstrate state-of-the-art classification accuracy on rotated-MNIST and achieve competitive results on other benchmark challenges.  These findings highlight the potential of H-Nets to advance rotation-invariant computer vision applications. \n",
    "This study investigates the reflection spectra of directly coupled waveguide-cavity systems, utilizing Fano resonance analysis to elucidate the underlying reflection and coupling mechanisms. Unlike side-coupled systems, where Fano resonances typically arise from interference at waveguide termini, our investigation reveals a distinct origin for the observed Fano line shapes: the coupling between the measurement apparatus (optical fiber) and the waveguide.\n\nWe conduct meticulous experimental measurements of the reflection spectra, complemented by a rigorous analytical model.  Our analysis demonstrates a strong dependence of the Fano parameter, which quantifies the asymmetry of the resonance line shape, on the precise coupling conditions between the fiber and the waveguide.  \n\nSpecifically, we observe that even minute displacements of the fiber tip, well within the Rayleigh range, induce dramatic alterations in the Fano line shape.  This heightened sensitivity highlights the crucial role of the measurement apparatus in shaping the observed resonant behavior within directly coupled waveguide-cavity systems.  \n\nOur findings provide a deeper understanding of the interplay between coupling, interference, and Fano resonances in integrated optical systems, with implications for the design and characterization of photonic devices. \n\n\n",
    "Atmospheric turbulence, the shimmering of air that makes stars twinkle, is a major headache for astronomers using ground-based telescopes.  It blurs images and limits the sharpness of observations, even with advanced adaptive optics systems. \n\nBut measuring the strength and distribution of this turbulence throughout the atmosphere is a tricky task.  This paper introduces a clever new technique for tackling this challenge using a simple setup: a small telescope and a series of short-exposure images of a star field.\n\nOur method analyzes the tiny, jittery movements of stars in these images, comparing them in pairs to calculate something called \"structure functions.\" These functions describe how the turbulence distorts light waves coming from different directions in the sky.\n\nWe then match these observed structure functions to theoretical predictions from simple turbulence models using a powerful statistical technique called Markov-Chain Monte-Carlo optimization.  This allows us to estimate key parameters like:\n\n* **Turbulence profile:** How the strength of turbulence varies with altitude in the lower atmosphere.\n* **Total seeing:** The overall blurring caused by turbulence.\n* **Free-atmosphere seeing:** The blurring caused by turbulence above the telescope's location.\n* **Outer scale:**  The largest size scale of turbulent eddies.\n\nWe've tested our method thoroughly using computer simulations and demonstrated its effectiveness on real data from the AST3 telescope in Antarctica.  This new technique provides astronomers with a valuable tool for characterizing atmospheric turbulence, helping them optimize telescope performance and capture sharper images of the cosmos. \n",
    "This work introduces a novel mathematical framework for understanding n-plectic structures, a generalization of symplectic geometry that plays a crucial role in theoretical physics and differential geometry.  We define an n-plectic structure as a specific type of algebraic object called a \"commutative and torsionless Lie Rinehart pair,\" endowed with a distinguished cocycle from its Chevalley-Eilenberg complex.\n\nThis \"n-plectic cocycle\" acts as a key ingredient, giving rise to an extension of the Chevalley-Eilenberg complex, a powerful tool for studying Lie algebras and their representations. This extended complex, populated by \"symplectic tensors,\" provides a natural framework for generalizing the concepts of Hamiltonian functions and vector fields, which are central to symplectic geometry.\n\nOur construction elevates these familiar notions to a richer world of tensors and cotensors, encompassing a range of degrees.  These generalized Hamiltonian objects form a remarkable algebraic structure known as a Lie -algebra, capturing the intricate relationships and symmetries within the n-plectic setting.\n\nFinally, we demonstrate that \"momentum maps,\" crucial for connecting symmetries to conserved quantities in physics, emerge naturally within our framework.  We identify them as \"weak Lie -morphisms,\" mappings that preserve the algebraic structure, from an arbitrary Lie -algebra into the Lie -algebra of Hamiltonian tensors and cotensors.\n\nThis research provides a powerful new lens for exploring the geometry and algebra of n-plectic structures, paving the way for deeper insights into their role in theoretical physics, particularly in areas like classical and quantum field theory, as well as in the study of geometric mechanics and quantization. \n\n\n",
    "This study reveals the underlying mechanism behind the stretched-exponential relaxation observed in various macroscopic properties of amorphous solids (glasses), particularly near the glass transition temperature. \n\nHere are the key findings:\n\n* **Stretched-exponential relaxation:** This behavior is prevalent in several properties of glasses, such as the intermediate scattering function, dielectric relaxation modulus, and time-elastic modulus.\n* **Connection to lattice dynamics:** We demonstrate, using dielectric relaxation as an example, that this peculiar relaxation pattern is intimately linked to the unique vibrational characteristics of glasses.\n* **Reformulated Lorentz model:** We generalize the classical Lorentz model for dielectric materials, expressing the dielectric response in terms of the vibrational density of states (DOS).\n* **Glass transition and stretched-exponential behavior:** Our analysis reveals that near the glass transition (coinciding with the Maxwell rigidity transition), the dielectric relaxation precisely exhibits stretched-exponential behavior with Kohlrausch exponents () ranging from 0.56 to 0.65. This range aligns remarkably well with experimental observations across various glassy systems.\n* **Origin in soft modes:** We identify the root cause of this stretched-exponential relaxation as the presence of \"soft modes\" (boson peak) in the vibrational density of states.\n\nThis work establishes a clear connection between the microscopic vibrational properties of glasses and their macroscopic relaxation behavior, providing a fundamental understanding of stretched-exponential relaxation in amorphous materials. \n\n\n",
    "This paper addresses the challenging problem of achieving unsupervised representation disentanglement in the text domain. Despite its success in computer vision, disentanglement remains largely unexplored for text.  \n\nTo illuminate the challenges and opportunities, we carefully select a representative set of disentanglement models that have proven effective for images. We rigorously evaluate these models using six established disentanglement metrics, as well as downstream classification tasks and homotopy analysis.\n\nTo facilitate this evaluation, we introduce two novel synthetic text datasets with known generative factors, providing a controlled environment for assessing disentanglement performance. Our experiments reveal a significant gap between current capabilities and desired outcomes for text disentanglement.  \n\nFurthermore, our findings highlight the potential impact of factors like representation sparsity and the coupling between representation and decoder on disentanglement performance.  These insights provide valuable guidance for future research in this domain.\n\nThis work represents the first systematic exploration of unsupervised representation disentanglement for text.  By establishing a comprehensive evaluation framework and providing specifically designed datasets, we aim to catalyze future advancements in this crucial area of natural language processing. \n",
    "This paper presents a novel hybrid quantum-classical algorithm for solving the unit commitment (UC) problem, a fundamental optimization task in power systems. We decompose the UC problem into three distinct subproblems: a quadratic program, a quadratic unconstrained binary optimization (QUBO) problem, and an unconstrained quadratic program.\n\nLeveraging the strengths of both classical and quantum computing, we employ a classical optimization solver for the first and third subproblems, while the QUBO subproblem is tackled using the quantum approximate optimization algorithm (QAOA).  Coordination between these subproblems is achieved through an iterative three-block alternating direction method of multipliers (ADMM) algorithm.\n\nWe validate the efficacy of our proposed algorithm through simulations conducted using Qiskit on the IBM Q quantum computing platform. Our results demonstrate the feasibility and potential of this hybrid approach for solving the UC problem, showcasing the synergistic interplay between classical and quantum computation in addressing complex optimization challenges in the energy domain. \n",
    "Scientists have long suspected that Delta Scuti stars, a type of pulsating star, harbor a rich tapestry of hidden oscillations, too faint to be detected from Earth.  The CoRoT space mission, with its unparalleled sensitivity, offered a unique opportunity to unveil these subtle stellar tremors.\n\nThis study focused on HD 50844, a Delta Scuti star observed by CoRoT with exquisite precision.  Analyzing over 140,000 data points, the researchers achieved an unprecedented noise level, allowing them to detect incredibly faint oscillations.\n\nThe results were astonishing.  Hundreds of pulsation frequencies were identified, confirming the long-held belief that Delta Scuti stars possess a remarkably rich spectrum of oscillations.  Further analysis using spectroscopic data revealed that these oscillations correspond to modes with very high degrees, reaching up to l=14, indicating complex patterns of stellar vibrations.\n\nGround-based observations suggest that HD 50844 is an evolved star nearing the end of its main sequence lifetime, with a slightly lower abundance of heavy elements. This evolutionary stage might explain the lack of a clear, regular pattern in the observed frequencies.\n\nBy combining data from both space-based and ground-based telescopes, the researchers confidently identified the most prominent oscillation frequency (f1 = 6.92 d^-1) as the star's fundamental radial mode, the basic \"heartbeat\" of its pulsations.\n\nThis research highlights the power of space-based observations to uncover subtle astrophysical phenomena, revealing the hidden complexities of stellar pulsations and advancing our understanding of stellar evolution. \n",
    "This study investigates star formation within the S231-S235 region, nestled inside the giant molecular cloud G174+2.5. Utilizing archival carbon monoxide (CO) data, we identified all massive molecular clumps within G174+2.5 and characterized their mass, size, and CO column density.\n\nSubsequent observations targeted these clumps, focusing on \"quasi-thermal\" emission lines of ammonia (NH3) and cyanoacetylene (HC3N), as well as maser lines of methanol (CH3OH) and water vapor (H2O).  Our observations yielded the first detections of NH3 and HC3N lines towards two specific clumps, WB89 673 and WB89 668, providing compelling evidence for the presence of dense gas within these regions.\n\nUsing the ammonia emission data, we estimated the physical properties of the molecular gas in these clumps.  Our analysis indicates gas temperatures ranging from 16 to 30 Kelvin and hydrogen number densities between 2.8 and 7.2 x 10^3 cm^-3.\n\nFurthermore, we report a new detection of the 36.2 GHz methanol maser line towards WB89 673, a spectral signature often associated with shocks within molecular clouds. These findings shed light on the physical conditions and processes driving star formation within the S231-S235 region. \n",
    "This study presents an extensive analysis of the radio and X-ray afterglow of GRB 171205A, utilizing low-frequency observations from the upgraded Giant Metrewave Radio Telescope (uGMRT) and archival Chandra X-ray data. Our uGMRT observations, spanning 250-1450 MHz and 4-937 days post-burst, represent the first detection of a GRB afterglow in the 250-500 MHz range and the second brightest GRB observed by the uGMRT. \n\nDespite extensive temporal coverage, our analysis reveals no evidence of a transition to the non-relativistic phase or a jet break in either the radio or X-ray light curves.  We model the synchrotron afterglow emission using two scenarios:  a relativistic, isotropic, self-similar deceleration model and a shock-breakout cocoon model.  \n\nThe density profile inferred from our data deviates from a standard constant density medium, suggesting that GRB 171205A exploded within a stratified, wind-like environment. Notably, the low-frequency radio data, encompassing the absorbed portion of the light curves, proves crucial for determining the circumburst medium's properties.\n\nCombining our data with previously published measurements, we conclude that the radio afterglow consists of two components: a weak, potentially slightly off-axis jet and a surrounding cocoon. Our analysis supports the findings of Izzo et al. (2019), suggesting that cocoon emission dominates at early times, while the jet contribution becomes dominant at later epochs, resulting in flatter radio light curves.  These findings underscore the importance of low-frequency radio observations for probing GRB environments and afterglow evolution. \n",
    "This paper utilizes the novel theory of quasi-Lie schemes to analyze Emden-type equations, a class of nonlinear ordinary differential equations with applications in astrophysics and other fields.  We present a systematic approach for deriving time-dependent constants of motion for these equations, leveraging particular solutions and specific conditions on the equation parameters.  Our method recovers previously known results while offering a fresh perspective and extending the analysis to a broader class of Emden-type equations. \n\n\n",
    "This investigation explores the phenomenology of charged Higgs bosons within the framework of a model characterized by the gauge symmetry SU(3)_c  SU(3)_L  U(1)_X, commonly referred to as the 3-3-1 model.  We demonstrate that by incorporating Yukawa mixing couplings spanning both small (GeV) and large (TeV) energy scales, the model predicts the simultaneous production of two distinct types of charged Higgs bosons: the hypercharge-one (H_1^{\\pm}) and hypercharge-two (H_2^{\\pm}) bosons.\n\nAt low energies, the H_1^{\\pm} bosons exhibit phenomenological characteristics consistent with those predicted by two-Higgs-doublet models (2HDMs).  The H_2^{\\pm} bosons, however, represent additional charged Higgs states unique to the 3-3-1 model.  Therefore, the observation of multiple charged Higgs boson resonances at collider experiments could provide a discriminating test for these models.\n\nWe perform a detailed analysis of H_{1,2}^{\\pm} pair production and associated tbH_{1,2}^{\\pm} production at the CERN Large Hadron Collider (LHC).  Notably, we find that the pair production cross-section, mediated by the exchange of a heavy neutral Z' gauge boson predicted by the 3-3-1 model, can be comparable in magnitude to the single production cross-section in gluon-gluon collisions.\n\nConsidering leptonic decay channels (H_{1,2}^{\\pm}   _), we identify scenarios where distinct peaks corresponding to H_2^{\\pm} events emerge in the transverse mass distributions, discernable above the H_1^{\\pm} background. These findings underscore the potential of the LHC to probe the rich phenomenology of extended Higgs sectors and differentiate between various theoretical models. \n\n\n\n\n",
    "This study investigates isospin breaking effects in the $K_{\\ell 4}$ form factors, specifically those induced by the mass difference between charged and neutral pions.  The analysis employs a framework based on suitably subtracted dispersion representations. \n\nWe construct the $K_{\\ell 4}$ form factors iteratively up to two loops in the low-energy expansion, incorporating constraints from analyticity, crossing symmetry, and unitarity arising from two-meson intermediate states. Analytical expressions for the phases of the two-loop form factors in the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ channel are derived.  These expressions enable a direct connection between experimentally measured phase shift differences in the form factors (outside the isospin limit) and theoretically calculated phase shift differences in the $S$- and $P$-wave $\\pi\\pi$ scattering amplitudes (within the isospin limit).\n\nOur analysis provides a general framework for studying the dependence on the $S$-wave scattering lengths, $a_0^0$ and $a_0^2$, in the isospin limit, surpassing previous limitations of one-loop chiral perturbation theory analyses. \n\nWe reanalyze experimental data from the NA48/2 collaboration at CERN, incorporating isospin-breaking corrections to extract values for the scattering lengths $a_0^0$ and $a_0^2$.  This comprehensive study provides a refined understanding of isospin breaking effects in $K_{\\ell 4}$ decays and their implications for low-energy pion-pion scattering parameters. \n",
    "This paper presents a compelling statistical description of the cosmological constant within a de Sitter universe, demonstrating its emergence from massless excitations incorporating Planckian effects. We build upon previous work, specifically extending the results presented in references [1, 2, 3, 4].\n\nWe unequivocally establish that at the classical level, a positive cosmological constant ( > 0) can only be obtained in the limit of zero temperature (T  0). Analogous to the case of black holes, incorporating quantum effects allows for a representation of  in terms of massless excitations, provided quantum corrections to the Misner-Sharp mass are considered.\n\nCrucially, our analysis reveals that quantum fluctuations give rise to an effective cosmological constant that varies with the physical scale under consideration. This provides a compelling resolution to the cosmological constant problem without invoking a quintessence field. \n\nFurthermore, the remarkably small observed value of  can be attributed to the existence of a quantum decoherence scale exceeding the Planck length. This scale dictates a transition to a pure de Sitter universe characterized by a small, averaged cosmological constant frozen in its lowest energy state.  These findings provide a robust and elegant explanation for the origin and magnitude of the cosmological constant within a quantum gravitational framework. \n",
    "This study examines the one-dimensional spin-glass model with vector spins in the limit of infinite spin components (m  ) and power-law decaying interactions (). We investigate both the fully connected and diluted versions of the model, observing significant differences between them.\n\nAt zero temperature, we determine the defect energy exponent () by analyzing the ground-state energy differences between systems with periodic and antiperiodic boundary conditions. Our results suggest a relationship  = 3/4 - , implying an upper critical value of  = 3/4, corresponding to the lower critical dimension of the short-range model.\n\nFor finite temperatures, we solve the large-m saddle-point equations self-consistently, extracting the correlation function, order parameter, and spin-glass susceptibility.  We carefully analyze finite-size scaling effects, highlighting distinct behaviors below and above the lower critical value of  = 5/8, which corresponds to the upper critical dimension (8) of the hypercubic short-range model. \n",
    "The enigmatic Of^+ supergiants, a rare class of stars, exhibit properties that bridge the gap between typical O-type stars and the enigmatic Wolf-Rayet (WR) stars.  Recent studies have illuminated striking similarities between these transitional objects and WN-type WR stars, particularly in the visible and near-infrared spectral domains, suggesting commonalities in their stellar wind characteristics.\n\nIn this presentation, we report on the first dedicated X-ray observations of two Of^+ supergiants, HD 16691 (O4If^+) and HD 14947 (O5f^+). Our analysis reveals soft thermal X-ray spectra consistent with expectations for single O-type stars.  However, the measured X-ray luminosities are notably lower than anticipated for isolated O-type stars.\n\nThis intriguing under-luminosity suggests that the unique properties of their stellar winds, a hallmark of their transitional nature, significantly influence their X-ray emission. We propose that the observed X-ray deficiency in HD 16691 and HD 14947 serves as an X-ray signature of their intermediary evolutionary stage between O and WR stars, likely attributable to enhanced wind densities.  \n\n\n\n",
    "The AARTFAAC project is an ambitious endeavor to create an All-Sky Monitor (ASM) using the powerful Low Frequency Array (LOFAR) radio telescope. The goal is to continuously scan the vast expanse of the sky visible to LOFAR, searching for fleeting bursts of radio waves known as transients. \n\nThis real-time monitoring system will be capable of detecting transients on timescales ranging from milliseconds to days, enabling rapid follow-up observations with the full power of LOFAR whenever a potential transient candidate is discovered.\n\nCreating such a system presents formidable challenges:\n\n* **Imaging a Vast Field of View:** The ASM must capture images of the entire sky visible to LOFAR, requiring a vast amount of data processing.\n* **Low Latency Processing:** To enable rapid response to transient events, data processing must occur with minimal delay.\n* **Continuous Availability and Autonomous Operation:**  The ASM must operate continuously and autonomously, without requiring constant human intervention.\n\nTo address these challenges, the AARTFAAC project has developed a cutting-edge correlator, the heart of the signal processing system.  This correlator, the largest of its kind in the world, can handle an enormous number of input channels, enabling it to process the vast amounts of data necessary for all-sky imaging. When fully operational, it will generate an astounding 150,000 correlations per second for each spectral channel.\n\nTo refine the design and quantify the instrument's capabilities, the team conducted test observations using existing LOFAR infrastructure.  This paper provides a detailed overview of the AARTFAAC data processing pipeline, showcasing the complexity of handling such massive data streams.\n\nWe present stunning all-sky images generated from one of these test observations, illustrating the system's ability to capture the dynamic radio sky.  These results offer quantitative estimates of the instrument's sensitivity and resolution, highlighting the immense potential of AARTFAAC to revolutionize our understanding of transient phenomena in the universe. \n\n\n",
    "You know those massive stars, the ones that live fast and die young? Well, Wolf-Rayet (WR) stars are like the rock stars of the stellar world  they're the evolved versions of those super-bright O-type stars and they're thought to be the culprits behind those awesome Type Ib/c supernova explosions.\n\nWe've been using the Hubble Space Telescope to check out WR stars in the galaxy M101, and we've found something really interesting. Turns out, those narrow-band filters are like a secret weapon for spotting these elusive stars.  We're finding way more WR stars with these filters compared to the usual broad-band methods.\n\nOn average, we're catching an extra 42% of WR stars with narrow-band imaging, and that number jumps to a whopping 85% in the crowded center of the galaxy!  This means that just because we don't see a WR star in a broad-band image of a supernova doesn't mean it wasn't there all along. \n\nSo, those Type Ib/c supernovae that seemed to have no WR star parent?  Well, they might have had a sneaky WR star hiding in plain sight!  This changes the game for understanding how these massive stars explode and leave behind those incredible supernova remnants. \n\n\n"
  ]
}