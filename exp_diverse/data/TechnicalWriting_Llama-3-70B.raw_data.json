{
  "original": [
    "Process calculi based on logic, such as $\\pi$DILL and CP, provide a foundation for deadlock-free concurrent programming. However, in previous work, there is a mismatch between the rules for constructing proofs and the term constructors of the $\\pi$-calculus: the fundamental operator for parallel composition does not correspond to any rule of linear logic. Kokke et al. (2019) introduced Hypersequent Classical Processes (HCP), which addresses this mismatch using hypersequents (collections of sequents) to register parallelism in the typing judgements. However, the step from CP to HCP is a big one. As of yet, HCP does not have reduction semantics, and the addition of delayed actions means that CP processes interpreted as HCP processes do not behave as they would in CP. We introduce HCP-, a variant of HCP with reduction semantics and without delayed actions. We prove progress, preservation, and termination, and show that HCP- supports the same communication protocols as CP.",
    "A simple variant of the BDDC preconditioner in which constraints are imposed on a selected set of subobjects (subdomain subedges, subfaces and vertices between pairs of subedges) is presented. We are able to show that the condition number of the preconditioner is bounded by $C \\big(1+\\log (L/h)\\big)^2$, where $C$ is a constant, and $h$ and $L$ are the characteristic sizes of the mesh and the subobjects, respectively. As $L$ can be chosen almost freely, the condition number can theoretically be as small as $O(1)$. We will discuss the pros and cons of the preconditioner and its application to heterogeneous problems. Numerical results on supercomputers are provided.",
    "We give examples of where the Heun function exists as solutions of wave equations encountered in general relativity. While the Dirac equation written in the background of Nutku helicoid metric yields Mathieu functions as its solutions in four spacetime dimensions, the trivial generalization to five dimensions results in the double confluent Heun function. We reduce this solution to the Mathieu function with some transformations. We must apply Atiyah-Patodi-Singer spectral boundary conditions to this system since the metric has a singularity at the origin.",
    "As it was shown by many authors, a slow decrease in X-rays observed during the decay phase of long duration flares (LDE) can be explained only by a magnetic reconnection and energy release ceaselessly ongoing in the coronal part of a flare. Using RHESSI data we try to answer two following questions. How effective are these processes at the LDEs decay phase and how can precisely the energy release rate be calculated based on these data? To answer the questions images of the selected LDEs during their decay phase were reconstructed. Physical parameters of flare coronal sources obtained from image spectral analysis allowed us to study the efficiency of the energy release process. We also examined terms included in the energy equation to find out what is the accuracy of determination of each term.",
    "By means of a multi-scale analysis we describe the typical geometrical structure of the clusters under the FK measure in random media. Our result holds in any dimension greater or equal to 2 provided that slab percolation occurs under the averaged measure, which should be the case in the whole supercritical phase. This work extends the one of Pisztora and provides an essential tool for the analysis of the supercritical regime in disordered FK models and in the corresponding disordered Ising and Potts models.",
    "Photospheric absorption lines in classical T Tauri stars (CTTS) are weak compared to normal stars. This so-called veiling is normally identified with an excess continuous emission formed in shock-heated gas at the stellar surface below the accretion streams. We have selected four stars (RW Aur A, RU Lup, S CrA NW and S CrA SE) with unusually strong veiling to make a detailed investigation of veiling versus stellar brightness and emission line strengths for comparisons to standard accretion models. We have monitored the stars photometrically and spectroscopically at several epochs. In standard accretion models a variable accretion rate will lead to a variable excess emission. Consequently, the stellar brightness should vary accordingly. We find that the veiling of absorption lines in these stars is strongly variable and usually so large that it would require the release of several stellar luminosities of potential energy. At states of very large line dilution, the correspondingly large veiling factors derived correlate only weakly with brightness. Moreover, the emission line strengths violate the expected trend of veiling versus line strength. The veiling can change dramatically in one night, and is not correlated with the phase of the rotation periods found for two stars. We show that in at least three of the stars, when the veiling becomes high, the photospheric lines become filled-in by line emission, which produces large veiling factors unrelated to changes in any continuous emission from shocked regions. We also consider to what extent extinction by dust and electron scattering in the accretion stream may affect veiling measures in CTTS. We conclude that the degree of veiling cannot be used as a measure of accretion rates in CTTS with rich emission line spectra.",
    "Giant low surface brightness (GLSB) galaxies are commonly thought to be massive, dark matter dominated systems. However, this conclusion is based on highly uncertain rotation curves. We present here a new study of two prototypical GLSB galaxies: Malin 1 and NGC 7589. We re-analysed existing HI observations and derived new rotation curves, which were used to investigate the distributions of luminous and dark matter in these galaxies. In contrast to previous findings, the rotation curves of both galaxies show a steep rise in the central parts, typical of high surface brightness (HSB) systems. Mass decompositions with a dark matter halo show that baryons may dominate the dynamics of the inner regions. Indeed, a \"maximum disk\" fit gives stellar mass-to-light ratios in the range of values typically found for HSB galaxies. These results, together with other recent studies, suggest that GLSB galaxies are systems with a double structure: an inner HSB early-type spiral galaxy and an outer extended LSB disk. We also tested the predictions of MOND: the rotation curve of NGC 7589 is reproduced well, whereas Malin 1 represents a challenging test for the theory.",
    "The multiplicity distribution, multiplicity moment, scaled variance, entropy and reduced entropy of target evaporated fragment emitted in forward and backward hemispheres in 12 A GeV $^{4}$He, 3.7 A GeV $^{16}$O, 60 A GeV $^{16}$O, 1.7 A GeV $^{84}$Kr and 10.7 A GeV $^{197}$Au induced emulsion heavy targets (AgBr) interactions are investigated. It is found that the multiplicity distribution of target evaporated fragments emitted in forward and backward hemispheres can be fitted by a Gaussian distribution. The multiplicity moments of target evaporated particles emitted in forward and backward hemispheres increase with the order of the moment {\\em q}, and second-order multiplicity moment is energy independent over the entire energy for all the interactions in the forward and backward hemisphere respectively. The scaled variance, a direct measure of multiplicity fluctuations, is close to one for all the interactions which may be said that there is a feeble correlation among the produced particles. The entropy of target evaporated fragments emitted in forward and backward hemispheres are the same within experimental errors, respectively.",
    "We investigate theoretically the temporal behavior of a quantum dot under off-resonant optical excitation targeted at fast acoustic phonon-assisted state preparation. We demonstrate that in a preparation process driven by short laser pulses three processes can be identified: a dressing of the states during the switch on of the laser pulse, a subsequent phonon-induced relaxation and an undressing at the end of the pulse. By analyzing excitation scenarios with different pulse shapes we highlight the decisive impact of an adiabatic undressing on the final state in short pulse protocols. Furthermore, we show that in exciton-biexciton systems the laser characteristics such as the pulse detuning and the pulse length as well as the biexciton binding energy can be used to select the targeted quantum dot state.",
    "In the quantum mechanical Hilbert space formalism, the probabilistic interpretation is a later ad-hoc add-on, more or less enforced by the experimental evidence, but not motivated by the mathematical model itself. A model involving a clear probabilistic interpretation from the very beginning is provided by the quantum logics with unique conditional probabilities. It includes the projection lattices in von Neumann algebras and here probability conditionalization becomes identical with the state transition of the Lueders - von Neumann measurement process. This motivates the definition of a hierarchy of five compatibility and comeasurability levels in the abstract setting of the quantum logics with unique conditional probabilities. Their meanings are: the absence of quantum interference or influence, the existence of a joint distribution, simultaneous measurability, and the independence of the final state after two successive measurements from the sequential order of these two measurements. A further level means that two elements of the quantum logic (events) belong to the same Boolean subalgebra. In the general case, the five compatibility and comeasurability levels appear to differ, but they all coincide in the common Hilbert space formalism of quantum mechanics, in von Neumann algebras, and in some other cases.",
    "An analysis is presented of wave-vector dispersion in elliptically birefringent stratified magneto-optic media having one-dimensional periodicity. It is found that local normal-mode polarization-state differences between adjacent layers lead to mode coupling and impact the wave-vector dispersion and the character of the Bloch states of the system. This coupling produces extra terms in the dispersion relation not present in uniform circularly birefringent magneto-optic stratified media. Normal mode coupling lifts the degeneracy at frequency band cross-over points under certain conditions and induces a magnetization-dependent optical band gap. This study examines the conditions for band gap formation in the system. It shows that such a frequency-split can be characterized by a simple coupling parameter that depends on the relation between polarization states of local normal modes in adjacent layers. The character of the Bloch states and conditions for maximizing the strength of the band splitting in these systems are analyzed.",
    "We study a natural extension of classical empirical risk minimization, where the hypothesis space is a random subspace of a given space. In particular, we consider possibly data dependent subspaces spanned by a random subset of the data, recovering as a special case Nystr\\\"om approaches for kernel methods. Considering random subspaces naturally leads to computational savings, but the question is whether the corresponding learning accuracy is degraded. These statistical-computational tradeoffs have been recently explored for the least squares loss and self-concordant loss functions, such as the logistic loss. Here, we work to extend these results to convex Lipschitz loss functions, that might not be smooth, such as the hinge loss used in support vector machines. This extension requires developing new proofs, that use different technical tools. Our main results show the existence of different settings, depending on how hard the learning problem is, for which computational efficiency can be improved with no loss in performance. Theoretical results are illustrated with simple numerical experiments.",
    "The notion of patient's consent plays a major role in granting access to medical data. In typical healthcare systems, consent is captured by a form that the patient has to fill in and sign. In e-Health systems, the paper-form consent is being replaced by the integration of the notion of consent in the mechanisms that regulate the access to the medical data. This helps in empowering the patient with the capability of granting and revoking consent in a more effective manner. However, the process of granting and revoking consent greatly varies according to the situation in which the patient is. Our main argument is that such a level of detail is very difficult and error-prone to capture as a set of authorisation policies. In this paper, we present ACTORS, a goal-driven approach to manage consent. The main idea behind ACTORS is to leverage the goal-driven approach of Teleo-Reactive (TR) programming for managing consent that takes into account changes regarding the domains and contexts in which the patient is providing her consent.",
    "This paper is concerned with the mathematical analysis of the inverse random source problem for the time fractional diffusion equation, where the source is assumed to be driven by a fractional Brownian motion. Given the random source, the direct problem is to study the stochastic time fractional diffusion equation. The inverse problem is to determine the statistical properties of the source from the expectation and variance of the final time data. For the direct problem, we show that it is well-posed and has a unique mild solution under a certain condition. For the inverse problem, the uniqueness is proved and the instability is characterized. The major ingredients of the analysis are based on the properties of the Mittag--Leffler function and the stochastic integrals associated with the fractional Brownian motion.",
    "Manifold learning methods play a prominent role in nonlinear dimensionality reduction and other tasks involving high-dimensional data sets with low intrinsic dimensionality. Many of these methods are graph-based: they associate a vertex with each data point and a weighted edge with each pair. Existing theory shows that the Laplacian matrix of the graph converges to the Laplace-Beltrami operator of the data manifold, under the assumption that the pairwise affinities are based on the Euclidean norm. In this paper, we determine the limiting differential operator for graph Laplacians constructed using $\\textit{any}$ norm. Our proof involves an interplay between the second fundamental form of the manifold and the convex geometry of the given norm's unit ball. To demonstrate the potential benefits of non-Euclidean norms in manifold learning, we consider the task of mapping the motion of large molecules with continuous variability. In a numerical simulation we show that a modified Laplacian eigenmaps algorithm, based on the Earthmover's distance, outperforms the classic Euclidean Laplacian eigenmaps, both in terms of computational cost and the sample size needed to recover the intrinsic geometry.",
    "We present an efficient integral equation approach to solve the heat equation, $u_t (\\x) - \\Delta u(\\x) = F(\\x,t)$, in a two-dimensional, multiply connected domain, and with Dirichlet boundary conditions. Instead of using integral equations based on the heat kernel, we take the approach of discretizing in time, first. This leads to a non-homogeneous modified Helmholtz equation that is solved at each time step. The solution to this equation is formulated as a volume potential plus a double layer potential.The volume potential is evaluated using a fast multipole-accelerated solver. The boundary conditions are then satisfied by solving an integral equation for the homogeneous modified Helmholtz equation. The integral equation solver is also accelerated by the fast multipole method (FMM). For a total of $N$ points in the discretization of the boundary and the domain, the total computational cost per time step is $O(N)$ or $O(N\\log N)$.",
    "We discuss a scheme in which sequential state-discrimination measurements are performed on qudits to determine the quantum state in which they were initially prepared. The qudits belong to a set of nonorthogonal quantum states and hence cannot be distinguished with certainty. Unambiguous state discrimination allows error-free measurements at the expense of occasionally failing to give a conclusive answer about the state of the qudit. Qudits have the potential to carry more information per transmission than qubits. We considered the situation in which Alice sends one of N qudits, where the dimension of the qudits is also N. We look at two cases, one in which the states all have the same overlap and one in which the qudits are divided into two sets, with qudits in different sets having different overlaps. We also study the robustness of our scheme against a simple eavesdropping attack and found that by using qudits rather than qubits, there is a greater probability that an eavesdropper will introduce errors and be detected.",
    "This work aims to provide a more secure access control in Hyperledger Fabric blockchain by combining multiple ID's, attributes, and policies with the components that regulate access control. The access control system currently used by Hyperledger Fabric is first completely analyzed. Next, a new implementation is proposed that builds upon the existing solution but provides users and developers with easier ways to make access control decisions based on combinations of multiple ID's, attributes, and policies. Our proposed implementation encapsulates the Fabric CA client to facilitate attribute addition and simplify the process of registering and enrolling a newly created certificate (corresponding to a new user). This research, concludes that it is possible to combine multiple ID's, attributes, and policies with the help of Hyperledger Fabric's smart contract technology. Furthermore, it could be seen that the performance impact for real-world applications is negligible compared to the insecure case of always providing access to a resource without performing access control.",
    "This work introduces pyramidal convolution (PyConv), which is capable of processing the input at multiple filter scales. PyConv contains a pyramid of kernels, where each level involves different types of filters with varying size and depth, which are able to capture different levels of details in the scene. On top of these improved recognition capabilities, PyConv is also efficient and, with our formulation, it does not increase the computational cost and parameters compared to standard convolution. Moreover, it is very flexible and extensible, providing a large space of potential network architectures for different applications. PyConv has the potential to impact nearly every computer vision task and, in this work, we present different architectures based on PyConv for four main tasks on visual recognition: image classification, video action classification/recognition, object detection and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baselines. For instance, on image recognition, our 50-layers network outperforms in terms of recognition performance on ImageNet dataset its counterpart baseline ResNet with 152 layers, while having 2.39 times less parameters, 2.52 times lower computational complexity and more than 3 times less layers. On image segmentation, our novel framework sets a new state-of-the-art on the challenging ADE20K benchmark for scene parsing. Code is available at: https://github.com/iduta/pyconv",
    "The status of the solar axion search with the CERN Axion Solar Telescope (CAST) will be discussed. Results from the first part of CAST phase II where the magnet bores were filled with 4He gas at variable pressure in order to scan axion masses up to 0.4 eV will be presented. From the absence of excess X-rays when the magnet was pointing to the Sun, we set a typical upper limit on the axion-photon coupling of g < 2.17 x 10^10 GeV$-1 at 95% CL for axion masses lower than 0.4 eV, the exact result depending on the pressure setting. Our search for axions with masses up to about 1.2 eV using 3He as a buffer gas is in progress in the second part of CAST phase II. Expectations for sensibilities will be given. Near future perspectives as well as more long term options for a new helioscope experiment will be evoked.",
    "Observations indicate that the Arctic sea ice cover is rapidly retreating while the Antarctic sea ice cover is steadily expanding. State-of-the-art climate models, by contrast, typically simulate a moderate decrease in both the Arctic and Antarctic sea ice covers. However, in each hemisphere there is a small subset of model simulations that have sea ice trends similar to the observations. Based on this, a number of recent studies have suggested that the models are consistent with the observations in each hemisphere when simulated internal climate variability is taken into account. Here we examine sea ice changes during 1979-2013 in simulations from the most recent Coupled Model Intercomparison Project (CMIP5) as well as the Community Earth System Model Large Ensemble (CESM-LE), drawing on previous work that found a close relationship in climate models between global-mean surface temperature and sea ice extent. We find that all of the simulations with 1979-2013 Arctic sea ice retreat as fast as observed have considerably more global warming than observations during this time period. Using two separate methods to estimate the sea ice retreat that would occur under the observed level of global warming in each simulation in both ensembles, we find that simulated Arctic sea ice retreat as fast as observed would occur less than 1% of the time. This implies that the models are not consistent with the observations. In the Antarctic, we find that simulated sea ice expansion as fast as observed typically corresponds with too little global warming, although these results are more equivocal. We show that because of this, the simulations do not capture the observed asymmetry between Arctic and Antarctic sea ice trends. This suggests that the models may be getting the right sea ice trends for the wrong reasons in both polar regions.",
    "Bio-features are fast becoming a key tool to authenticate the IoT devices; in this sense, the purpose of this investigation is to summaries the factors that hinder biometrics models' development and deployment on a large scale, including human physiological (e.g., face, eyes, fingerprints-palm, or electrocardiogram) and behavioral features (e.g., signature, voice, gait, or keystroke). The different machine learning and data mining methods used by authentication and authorization schemes for mobile IoT devices are provided. Threat models and countermeasures used by biometrics-based authentication schemes for mobile IoT devices are also presented. More specifically, We analyze the state of the art of the existing biometric-based authentication schemes for IoT devices. Based on the current taxonomy, We conclude our paper with different types of challenges for future research efforts in biometrics-based authentication schemes for IoT devices.",
    "Device fingerprinting over the web has received much attention both by the research community and the commercial market a like. Almost all the fingerprinting features proposed to date depend on software run on the device. All of these features can be changed by the user, thereby thwarting the device's fingerprint. In this position paper we argue that the recent emergence of the HTML5 standard gives rise to a new class of fingerprinting features that are based on the \\emph{hardware} of the device. Such features are much harder to mask or change thus provide a higher degree of confidence in the fingerprint. We propose several possible fingerprint methods that allow a HTML5 web application to identify a device's hardware. We also present an initial experiment to fingerprint a device's GPU.",
    "We present the partition function of Chern-Simons theory with the exceptional gauge group on three-sphere in the form of a partition function of the refined closed topological string with relation $2\\tau=g_s(1-b) $ between single K\\\"ahler parameter $\\tau$, string coupling constant $g_s$ and refinement parameter $b$, where $b=\\frac{5}{3},\\frac{5}{2},3,4,6$ for $G_2, F_4, E_6, E_7, E_8$, respectively. The non-zero BPS invariants $N^d_{J_L,J_R}$ ($d$ - degree) are $N^2_{0,\\frac{1}{2}}=1, N^{11}_{0,1}=1$. Besides these terms, partition function of Chern-Simons theory contains term corresponding to the refined constant maps of string theory. Derivation is based on the universal (in Vogel's sense) form of a Chern-Simons partition function on three-sphere, restricted to exceptional line $Exc$ with Vogel's parameters satisfying $\\gamma=2(\\alpha+\\beta)$. This line contains points, corresponding to the all exceptional groups. The same results are obtained for $F$ line $\\gamma=\\alpha+\\beta$ (containing $SU(4), SO(10)$ and $E_6$ groups), with the non-zero $N^2_{0,\\frac{1}{2}}=1, N^{7}_{0,1}=1$. In both cases refinement parameter $b$ ($=-\\epsilon_2/\\epsilon_1$ in terms of Nekrasov's parameters) is given in terms of universal parameters, restricted to the line, by $b=-\\beta/\\alpha$.",
    "The centerpoint theorem is a well-known and widely used result in discrete geometry. It states that for any point set $P$ of $n$ points in $\\mathbb{R}^d$, there is a point $c$, not necessarily from $P$, such that each halfspace containing $c$ contains at least $\\frac{n}{d+1}$ points of $P$. Such a point $c$ is called a centerpoint, and it can be viewed as a generalization of a median to higher dimensions. In other words, a centerpoint can be interpreted as a good representative for the point set $P$. But what if we allow more than one representative? For example in one-dimensional data sets, often certain quantiles are chosen as representatives instead of the median. We present a possible extension of the concept of quantiles to higher dimensions. The idea is to find a set $Q$ of (few) points such that every halfspace that contains one point of $Q$ contains a large fraction of the points of $P$ and every halfspace that contains more of $Q$ contains an even larger fraction of $P$. This setting is comparable to the well-studied concepts of weak $\\varepsilon$-nets and weak $\\varepsilon$-approximations, where it is stronger than the former but weaker than the latter.",
    "We have produced a new software package for the simulation of pulsar populations, \\textsc{PsrPopPy}, based on the \\textsc{Psrpop} package. The codebase has been re-written in Python (save for some external libraries, which remain in their native Fortran), utilising the object-oriented features of the language, and improving the modularity of the code. Pre-written scripts are provided for running the simulations in `standard' modes of operation, but the code is flexible enough to support the writing of personalised scripts. The modular structure also makes the addition of experimental features (such as new models for period or luminosity distributions) more straightforward than with the previous code. We also discuss potential additions to the modelling capabilities of the software. Finally, we demonstrate some potential applications of the code; first, using results of surveys at different observing frequencies, we find pulsar spectral indices are best fit by a normal distribution with mean $-1.4$ and standard deviation $1.0$. Second, we model pulsar spin evolution to calculate the best-fit for a relationship between a pulsar's luminosity and spin parameters. We used the code to replicate the analysis of Faucher-Gigu\\`ere & Kaspi, and have subsequently optimized their power-law dependence of radio luminosity, $L$, with period, $P$, and period derivative, $\\dot{P}$. We find that the underlying population is best described by $L \\propto P^{-1.39 \\pm 0.09} \\dot{P}^{0.48 \\pm 0.04}$ and is very similar to that found for $\\gamma$-ray pulsars by Perera et al. Using this relationship, we generate a model population and examine the age-luminosity relation for the entire pulsar population, which may be measurable after future large-scale surveys with the Square Kilometer Array.",
    "We study the dynamics of a spin ensemble strongly coupled to a single-mode resonator driven by external pulses. When the mean frequency of the spin ensemble is in resonance with the cavity mode, damped Rabi oscillations are found between the spin ensemble and the cavity mode which we describe very accurately, including the dephasing effect of the inhomogeneous spin broadening. We demonstrate that a precise knowledge of this broadening is crucial both for a qualitative and a quantitative understanding of the temporal spin-cavity dynamics. On this basis we show that coherent oscillations between the spin ensemble and the cavity can be enhanced by a few orders of magnitude, when driving the system with pulses that match special resonance conditions. Our theoretical approach is tested successfully with an experiment based on an ensemble of negatively charged nitrogen-vacancy (NV) centers in diamond strongly coupled to a superconducting coplanar single-mode waveguide resonator.",
    "We investigate the ground-state Riemannian metric and the cyclic quantum distance of an inhomogeneous quantum Ising spin-1/2 chain in a transverse field. This model can be diagonalized by using a general canonical transformation to the fermionic Hamiltonian mapped from the spin system. The ground-state Riemannian metric is derived exactly on a parameter manifold ring $S^1$, which is introduced by performing a gauge transformation to the spin Hamiltonian through a twist operator. The ground-state cyclic quantum distance and the second derivative of the ground-state energy are studied in different inhomogeneous exchange coupling parameter region. Particularly, we show that the quantum ferromagnetic phase in the uniform Ising chain can be characterized by an invariant cyclic quantum distance with a constant ground-state Riemannian metric, and this metric will rapidly decay to zero in the paramagnetic phase.",
    "Rotation measure synthesis allows the estimation of Faraday dispersion via a Fourier transform and is the primary tool to probe cosmic magnetic fields. We show this can be considered mathematically equivalent to the one dimensional interferometric intensity measurement equation, albeit in a different Fourier space. As a result, familiar concepts in two dimensional intensity interferometry designed to correctly account for a range of instrumental conditions can be translated to the analysis of Faraday dispersion. In particular, we show how to model the effect of channel averaging during Faraday reconstruction, which has to date limited the progress of polarimetic science using wide-band measurements. Further, we simulate 1d sparse reconstruction with channel averaging for realistic frequency coverages, and show that it is possible to recover signals with large rotation measure values that were previously excluded from possible detection. This is especially important for low-frequency and wide-band polarimetry. We extended these ideas to introduce mosaicking in Faraday depth into the channel averaging process. This work, thus provides the first framework for correctly undertaking wide-band rotation measure synthesis, including the provision to add data from multiple telescopes, a prospect that should vastly improve the quality and quantity of polarimetric science. This is of particular importance for extreme environments which generate high magnetic fields such as those associated with pulsars and Fast Radio Bursts (FRBs), and will allow such sources to be accurately used as probes of cosmological fields.",
    "Study of the characteristic properties of charged particle production in hadron-nucleus collisions at high energies, by utilising the approaches from different statistical models is performed.~Predictions from different approaches using the Negative Binomial distribution, shifted Gompertz distribution, Weibull distribution and the Krasznovszky-Wagner distribution are utilised for a comparative study of the relative successes of these models.~These distributions derived from a variety of functional forms are based on either phenomenological parameterizations or some model of the underlying dynamics.~Some of these have have also been used to study the data at the LHC for both proton-proton and nucleus-nucleus collisions.~Various physical and derived observables have been used for the analysis.",
    "In 1975 John Tukey proposed a multivariate median which is the 'deepest' point in a given data cloud in R^d. Later, in measuring the depth of an arbitrary point z with respect to the data, David Donoho and Miriam Gasko considered hyperplanes through z and determined its 'depth' by the smallest portion of data that are separated by such a hyperplane. Since then, these ideas has proved extremely fruitful. A rich statistical methodology has developed that is based on data depth and, more general, nonparametric depth statistics. General notions of data depth have been introduced as well as many special ones. These notions vary regarding their computability and robustness and their sensitivity to reflect asymmetric shapes of the data. According to their different properties they fit to particular applications. The upper level sets of a depth statistic provide a family of set-valued statistics, named depth-trimmed or central regions. They describe the distribution regarding its location, scale and shape. The most central region serves as a median. The notion of depth has been extended from data clouds, that is empirical distributions, to general probability distributions on R^d, thus allowing for laws of large numbers and consistency results. It has also been extended from d-variate data to data in functional spaces.",
    "Strain-engineering in SiGe nanostructures is fundamental for the design of optoelectronic devices at the nanoscale. Here we explore a new strategy, where SiGe structures are laterally confined by the Si substrate, to obtain high tensile strain avoiding the use of external stressors, and thus improving the scalability. Spectro-microscopy techniques, finite element method simulations and ab initio calculations are used to investigate the strain state of laterally confined Ge-rich SiGe nano-stripes. Strain information is obtained by tip enhanced Raman spectroscopy with an unprecedented lateral resolution of ~ 30 nm. The nano-stripes exhibit a large tensile hydrostatic strain component, which is maximum at the center of the top free surface, and becomes very small at the edges. The maximum lattice deformation is larger than the typical values of thermally relaxed Ge/Si(001) layers. This strain enhancement originates from a frustrated relaxation in the out-of-plane direction, resulting from the combination of the lateral confinement induced by the substrate side walls and the plastic relaxation of the misfit strain in the (001) plane at the SiGe/Si interface. The effect of this tensile lattice deformation at the stripe surface is probed by work function mapping, performed with a spatial resolution better than 100 nm using X-ray photoelectron emission microscopy. The nano-stripes exhibit a positive work function shift with respect to a bulk SiGe alloy, quantitatively confirmed by electronic structure calculations of tensile strained configurations. The present results have a potential impact on the design of optoelectronic devices at a nanometer length scale.",
    "During an infectious disease pandemic, it is critical to share electronic medical records or models (learned from these records) across regions. Applying one region's data/model to another region often have distribution shift issues that violate the assumptions of traditional machine learning techniques. Transfer learning can be a solution. To explore the potential of deep transfer learning algorithms, we applied two data-based algorithms (domain adversarial neural networks and maximum classifier discrepancy) and model-based transfer learning algorithms to infectious disease detection tasks. We further studied well-defined synthetic scenarios where the data distribution differences between two regions are known. Our experiments show that, in the context of infectious disease classification, transfer learning may be useful when (1) the source and target are similar and the target training data is insufficient and (2) the target training data does not have labels. Model-based transfer learning works well in the first situation, in which case the performance closely matched that of the data-based transfer learning models. Still, further investigation of the domain shift in real world research data to account for the drop in performance is needed.",
    "Bound states in the continuum (BIC) have been at the forefront of research in optics and photonics over the past decade. It is of great interest to study the effects associated with quasi-BICs in the simplest structures, where quasi-BICs are very pronounced. An example is a dielectric cylinder, and in a number of works, quasi-BICs have been studied both in single cylinders and in structures composed of cylinders. In this work, we studied the properties of quasi-BICs during the transition from a homogeneous dielectric cylinder in an air environment to a ring with narrow walls while increasing the diameter of the inner air cylinder gradually. The results demonstrate the quasi-BIC crossover from the strong-coupling to the weak-coupling regime, which manifests itself in the transition from avoided crossing of branches to their intersection with the quasi-BIC being preserved on only one straight branch. In the regime of strong-coupling and quasi-BIC, three waves interfere in the far-field zone: two waves corresponding to the resonant modes of the structure and the wave scattered by the structure as a whole. The validity of the Fano resonance concept is discussed, since it describes the interference of only two waves under weak coupling conditions.",
    "Turbulent thermal diffusion is a combined effect of the temperature stratified turbulence and inertia of small particles. It causes the appearance of a non-diffusive turbulent flux of particles in the direction of the turbulent heat flux. This non-diffusive turbulent flux of particles is proportional to the product of the mean particle number density and the effective velocity of inertial particles. The theory of this effect has been previously developed only for small temperature gradients and small Stokes numbers (Phys. Rev. Lett. {\\bf 76}, 224, 1996). In this study a generalized theory of turbulent thermal diffusion for arbitrary temperature gradients and Stokes numbers has been developed. The laboratory experiments in the oscillating grid turbulence and in the multi-fan produced turbulence have been performed to validate the theory of turbulent thermal diffusion in strongly stratified turbulent flows. It has been shown that the ratio of the effective velocity of inertial particles to the characteristic vertical turbulent velocity for large Reynolds numbers is less than 1. The effective velocity of inertial particles as well as the effective coefficient of turbulent thermal diffusion increase with Stokes numbers reaching the maximum at small Stokes numbers and decreases for larger Stokes numbers. The effective coefficient of turbulent thermal diffusion also decreases with the mean temperature gradient. It has been demonstrated that the developed theory is in a good agreement with the results of the laboratory experiments.",
    "A standard model for the visibility of pulsar radio emission is based on the assumption that the emission is confined to a narrow cone about the tangent to a dipolar field line. The widely accepted rotating vector model (RVM) is an approximation in which the line of sight is fixed and the field line is not strictly tangent to it. We refer to an exact treatment (Gangadhara 2004) as the tangent model. In the tangent model (but not in the RVM) the visible point changes as a function of pulsar rotational phase, $\\psi$, defining a trajectory on a sphere of radius $r$. We solve for the trajectory and for the angular velocity of the visible point around it. We note the recent claim that this motion is observable using interstellar holography (Pen et al. 2014). We estimate the error introduced by use of the RVM and find that it is significant for pulsars with emission over a wide range of $\\psi$. The RVM tends to underestimate the range of $\\psi$ over which emission is visible. We suggest that the geometry alone strongly favors the visible pulsar radio being emitted at a heights more than ten percent of the light-cylinder distance, where our neglect of retardation effects becomes significant.",
    "In image recognition, there are many cases where training samples cannot cover all target classes. Zero-shot learning (ZSL) utilizes the class semantic information to classify samples of the unseen categories that have no corresponding samples contained in the training set. In this paper, we propose an end-to-end framework, called Global Semantic Consistency Network (GSC-Net for short), which makes complete use of the semantic information of both seen and unseen classes, to support effective zero-shot learning. We also adopt a soft label embedding loss to further exploit the semantic relationships among classes. To adapt GSC-Net to a more practical setting, Generalized Zero-shot Learning (GZSL), we introduce a parametric novelty detection mechanism. Our approach achieves the state-of-the-art performance on both ZSL and GZSL tasks over three visual attribute datasets, which validates the effectiveness and advantage of the proposed framework.",
    "The popular view according to which Category theory provides a support for Mathematical Structuralism is erroneous. Category-theoretic foundations of mathematics require a different philosophy of mathematics. While structural mathematics studies invariant forms (Awodey) categorical mathematics studies covariant transformations which, generally, don t have any invariants. In this paper I develop a non-structuralist interpretation of categorical mathematics and show its consequences for history of mathematics and mathematics education.",
    "We show that in a non-equilibrium system of an exciton-polariton condensate, where polaritons are generated from incoherent pumping, a ring-shaped pump allows for stationary vortex memory elements of topological charge $m = 1$ or $m = -1$. Using simple potential guides we can choose whether to copy the same charge or invert it onto another spatially separate ring pump. Such manipulation of binary information opens the possibility of a new type processing using vortices as topologically protected memory components.",
    "During the three years long assessment phase of the LOFT mission, candidate to the M3 launch opportunity of the ESA Cosmic Vision programme, we estimated and measured the radiation damage of the silicon drift detectors (SDDs) of the satellite instrumentation. In particular, we irradiated the detectors with protons (of 0.8 and 11 MeV energy) to study the increment of leakage current and the variation of the charge collection efficiency produced by the displacement damage, and we \"bombarded\" the detectors with hypervelocity dust grains to measure the effect of the debris impacts. In this paper we describe the measurements and discuss the results in the context of the LOFT mission.",
    "In this paper we examine the ability of low-level multimodal features to extract movie similarity, in the context of a content-based movie recommendation approach. In particular, we demonstrate the extraction of multimodal representation models of movies, based on textual information from subtitles, as well as cues from the audio and visual channels. With regards to the textual domain, we emphasize our research in topic modeling of movies based on their subtitles, in order to extract topics that discriminate between movies. Regarding the visual domain, we focus on the extraction of semantically useful features that model camera movements, colors and faces, while for the audio domain we adopt simple classification aggregates based on pretrained models. The three domains are combined with static metadata (e.g. directors, actors) to prove that the content-based movie similarity procedure can be enhanced with low-level multimodal information. In order to demonstrate the proposed content representation approach, we have built a small dataset of 160 widely known movies. We assert movie similarities, as propagated by the individual modalities and fusion models, in the form of recommendation rankings. Extensive experimentation proves that all three low-level modalities (text, audio and visual) boost the performance of a content-based recommendation system, compared to the typical metadata-based content representation, by more than $50\\%$ relative increase. To our knowledge, this is the first approach that utilizes a wide range of features from all involved modalities, in order to enhance the performance of the content similarity estimation, compared to the metadata-based approaches.",
    "We study black hole radiation of a Reissner-Nordstrom black hole with an electric charge in the framework of quantum gravity. Based on a canonical quantization for a spherically symmetric geometry, under physically plausible assumptions, we solve the Wheeler-De Witt equation in the regions not only between the outer apparent horizon and the spatial infinity but also between the spacetime singularity and the inner apparent horizon, and then show that the mass loss rate of an evaporating black hole due to thermal radiation agrees with the semiclassical result when we choose an integration constant properly by physical reasoning. Furthermore, we also solve the Wheeler-De Witt equation in the region between the inner Cauchy horizon and the outer apparent horizon, and show that the mass loss rate of an evaporating black hole has the same expression. The present study is the natural generalization of the case of a Schwarzschild black hole to that of a charged Reissner-Nordstrom black hole.",
    "We present multi-agent A* (MAA*), the first complete and optimal heuristic search algorithm for solving decentralized partially-observable Markov decision problems (DEC-POMDPs) with finite horizon. The algorithm is suitable for computing optimal plans for a cooperative group of agents that operate in a stochastic environment such as multirobot coordination, network traffic control, `or distributed resource allocation. Solving such problems efiectively is a major challenge in the area of planning under uncertainty. Our solution is based on a synthesis of classical heuristic search and decentralized control theory. Experimental results show that MAA* has significant advantages. We introduce an anytime variant of MAA* and conclude with a discussion of promising extensions such as an approach to solving infinite horizon problems.",
    "We present morphological classifications obtained using machine learning for objects in SDSS DR6 that have been classified by Galaxy Zoo into three classes, namely early types, spirals and point sources/artifacts. An artificial neural network is trained on a subset of objects classified by the human eye and we test whether the machine learning algorithm can reproduce the human classifications for the rest of the sample. We find that the success of the neural network in matching the human classifications depends crucially on the set of input parameters chosen for the machine-learning algorithm. The colours and parameters associated with profile-fitting are reasonable in separating the objects into three classes. However, these results are considerably improved when adding adaptive shape parameters as well as concentration and texture. The adaptive moments, concentration and texture parameters alone cannot distinguish between early type galaxies and the point sources/artifacts. Using a set of twelve parameters, the neural network is able to reproduce the human classifications to better than 90% for all three morphological classes. We find that using a training set that is incomplete in magnitude does not degrade our results given our particular choice of the input parameters to the network. We conclude that it is promising to use machine- learning algorithms to perform morphological classification for the next generation of wide-field imaging surveys and that the Galaxy Zoo catalogue provides an invaluable training set for such purposes.",
    "The Lambek calculus is a well-known logical formalism for modelling natural language syntax. The original calculus covered a substantial number of intricate natural language phenomena, but only those restricted to the context-free setting. In order to address more subtle linguistic issues, the Lambek calculus has been extended in various ways. In particular, Morrill and Valentin (2015) introduce an extension with so-called exponential and bracket modalities. Their extension is based on a non-standard contraction rule for the exponential that interacts with the bracket structure in an intricate way. The standard contraction rule is not admissible in this calculus. In this paper we prove undecidability of the derivability problem in their calculus. We also investigate restricted decidable fragments considered by Morrill and Valentin and we show that these fragments belong to the NP class.",
    "The transition between the two phases of 4D Euclidean Dynamical Triangulation [1] was long believed to be of second order until in 1996 first order behavior was found for sufficiently large systems [5,9]. However, one may wonder if this finding was affected by the numerical methods used: to control volume fluctuations, in both studies [5,9] an artificial harmonic potential was added to the action; in [9] measurements were taken after a fixed number of accepted instead of attempted moves which introduces an additional error. Finally the simulations suffer from strong critical slowing down which may have been underestimated. In the present work, we address the above weaknesses: we allow the volume to fluctuate freely within a fixed interval; we take measurements after a fixed number of attempted moves; and we overcome critical slowing down by using an optimized parallel tempering algorithm [12]. With these improved methods, on systems of size up to 64k 4-simplices, we confirm that the phase transition is first order. In addition, we discuss a local criterion to decide whether parts of a triangulation are in the elongated or crumpled state and describe a new correspondence between EDT and the balls in boxes model. The latter gives rise to a modified partition function with an additional, third coupling. Finally, we propose and motivate a class of modified path-integral measures that might remove the metastability of the Markov chain and turn the phase transition into second order.",
    "We characterize the virtually nilpotent finitely generated groups (or, equivalently by Gromov's theorem, groups of polynomial growth) for which the Domino Problem is decidable: These are the virtually free groups, i.e. finite groups, and those having $\\Z$ as a subgroup of finite index.",
    "Gamma rays from the annihilation of dark matter particles in the Galactic halo provide a particularly promising means of indirectly detecting dark matter. Here, we demonstrate that pronounced spectral features at energies near the dark matter particles' mass, which are a generic prediction for most models, can significantly improve the sensitivity of gamma-ray telescopes to dark matter signals. We derive projected limits on such features (including the traditionally looked-for line signals) and show that they can be much more efficient in constraining the nature of dark matter than the model-independent broad spectral features expected at lower energies.",
    "The attainment of carbon neutrality requires a research agenda that addresses the technical and economic challenges that will be encountered as we progress toward 100% renewable electricity generation. Increasing proportions of variable renewable energy (VRE) sources (such as wind turbines and photovoltaic systems) render the supply-and-demand balance of VRE-dominated power grids difficult. The operational characteristics and effects of VRE inverters also require attention. Here, we examine the implications of the paradigm shift to carbon neutrality and summarize the associated research challenges in terms of system planning, operation, and sta-bility, and the need for energy storage integration, demand-side participation, distributed con-trol and estimation, and energy sector coupling. We also highlight the existing literature gaps, and our recent studies that can fill in the gaps, thereby facilitating the improvement of grid op-eration and estimation. The numerical results of comparative case studies are also provided on the operational stability and economics of power grids with a high level of VRE sources, assist-ing stakeholders in establishing specific roadmaps and making relevant decisions.",
    "Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to \"absorb\" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers. We present a novel network architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to drastically better compressed performance than several relevant baselines.",
    "We conducted a Project-Based Learning (PBL)-type exercise incorporating Japanese cartoon (manga) techniques into Requirements Development (RD) processes. Manga has established techniques, such as those for character setting and story development, that we thought are also valid for RD processes. Using this manga-driven method, students were able to clarify high-level project goals early in the development life-cycle, and succeeded in defining high quality and unique system ideas.",
    "The standard Hawking formula predicts the complete evaporation of black holes. Taking into account effects of quantum gravity, we investigate fermions' tunnelling from a 5-dimensional rotating black string. The temperature is determined not only by the string, but also affected by the quantum number of the emitted fermion and the effect of the extra spatial dimension. The quantum correction slows down the increase of the temperature, which naturally leads to the remnant in the evaporation.",
    "We introduce second-order vector representations of words, induced from nearest neighborhood topological features in pre-trained contextual word embeddings. We then analyze the effects of using second-order embeddings as input features in two deep natural language processing models, for named entity recognition and recognizing textual entailment, as well as a linear model for paraphrase recognition. Surprisingly, we find that nearest neighbor information alone is sufficient to capture most of the performance benefits derived from using pre-trained word embeddings. Furthermore, second-order embeddings are able to handle highly heterogeneous data better than first-order representations, though at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information further improves model performance in some cases. Due to variance in the random initializations of word embeddings, utilizing nearest neighbor features from multiple first-order embedding samples can also contribute to downstream performance gains. Finally, we identify intriguing characteristics of second-order embedding spaces for further research, including much higher density and different semantic interpretations of cosine similarity.",
    "RIS-aided millimeter wave wireless systems benefit from robustness to blockage and enhanced coverage. In this paper, we study the ability of RIS to also provide enhanced localization capabilities as a by-product of communication. We consider sparse reconstruction algorithms to obtain high resolution channel estimates that are mapped to position information. In RIS-aided mmWave systems, the complexity of sparse recovery becomes a bottleneck, given the large number of elements of the RIS and the large communication arrays. We propose to exploit a multidimensional orthogonal matching pursuit strategy for compressive channel estimation in a RIS-aided millimeter wave system. We show how this algorithm, based on computing the projections on a set of independent dictionaries instead of a single large dictionary, enables high accuracy channel estimation at reduced complexity. We also combine this strategy with a localization approach which does not rely on the absolute time of arrival of the LoS path. Localization results in a realistic 3D indoor scenario show that RIS-aided wireless system can also benefit from a significant improvement in localization accuracy.",
    "Detection and quantification of information leaks through timing side channels are important to guarantee confidentiality. Although static analysis remains the prevalent approach for detecting timing side channels, it is computationally challenging for real-world applications. In addition, the detection techniques are usually restricted to 'yes' or 'no' answers. In practice, real-world applications may need to leak information about the secret. Therefore, quantification techniques are necessary to evaluate the resulting threats of information leaks. Since both problems are very difficult or impossible for static analysis techniques, we propose a dynamic analysis method. Our novel approach is to split the problem into two tasks. First, we learn a timing model of the program as a neural network. Second, we analyze the neural network to quantify information leaks. As demonstrated in our experiments, both of these tasks are feasible in practice --- making the approach a significant improvement over the state-of-the-art side channel detectors and quantifiers. Our key technical contributions are (a) a neural network architecture that enables side channel discovery and (b) an MILP-based algorithm to estimate the side-channel strength. On a set of micro-benchmarks and real-world applications, we show that neural network models learn timing behaviors of programs with thousands of methods. We also show that neural networks with thousands of neurons can be efficiently analyzed to detect and quantify information leaks through timing side channels.",
    "The inner asteroid belt between 2.1 and 2.5 au is of particular dynamical significance because it is the dominant source of both chondritic meteorites and near-Earth asteroids. This inner belt is bounded by an eccentricity-type secular resonance and by the 1:3 mean motion resonance with Jupiter. Unless asteroid perihelia are low enough to allow scattering by Mars, escape requires transport to one of the bounding resonances. In addition Yarkovsky forces are generally ineffective in changing either the eccentricity and/or inclination for asteroids with diameter $\\gtrsim$30 km. Thus, large asteroids with pericentres far from Mars may only escape from the inner belt through large changes in their eccentricities. In this paper we study chaotic diffusion of orbits near the 1:2 mean motion resonance with Mars in a systematic way. We show that, while chaotic orbital evolution in both resonant and non-resonant orbits increase the dispersion of the inclinations and eccentricities, it does not significantly change their mean values. We show further that, while the dispersive growth is greatest for resonant orbits, at high $e$ the resonance acts to mitigate asteroid scattering by Mars - making the asteroid lifetime in the belt longer than it would have been for a non-resonant orbit. For asteroids of all sizes in both resonant and non-resonant orbits, the changes in eccentricity needed to account for the observations cannot be achieved by gravitational forces alone. The role of resonant trapping in protecting asteroids from encounters with Mars is also analysed.",
    "The presence of nonstandard neutrino interactions (NSI) has a large effect on the precision measurements at next generation neutrino oscillation experiments. Other type of experiments are needed to constrain the NSI parameter space. We study the constraints on NSI with electrons from current and future $e^+e^-$ collider experiments including Belle II, STCF and CEPC. We find that Belle II and STCF will provide competitive and complementary bounds on electron-type NSI parameters compared to the current global analysis, and strong improvements for the constraints on tau-type NSI. In addition, CEPC alone will impose stringent constraints on the parameter space of NSI with electrons. We find that the degeneracy between the left-handed (vector) and right-handed (axial-vector) NSI parameters can be lifted by combining the data from three different running modes, and the allowed ranges for $|\\epsilon_{ee}^{eL}|$ ($|\\epsilon_{ee}^{eV}|$) and $|\\epsilon_{ee}^{eR}|$ ($|\\epsilon_{ee}^{eA}|$) can be constrained to be smaller than 0.002 at CEPC even if both of them are present.",
    "The Deep Underground Neutrino Experiment (DUNE) is a leading-edge experiment designed to perform neutrino science and proton decay searches. In particular, the far detector will consist of four 10-kton Liquid Argon (LAr) Time Projection Chambers using both single and dual-phase technologies. The latter provides charge amplification in the gaseous phase. In order to optimize these designs, two large prototypes are taking data at CERN since 2018. Previously, a dual-phase 4-tonne demonstrator was constructed and exposed to cosmic muons in 2017 and exhibited good performance in terms of charge and light collection. The light detection system is important to provide a trigger to the charge acquisition system and to obtain additional information from the scintillation light produced in the particle interaction. In the demonstrator, five cryogenic photo-multipliers were installed with different base polarity configurations and wavelength shifting methods. During the detector operation, scintillation light data were collected in different drift and amplification field conditions. An overview of the light detection system performance and results on the light production and propagation are presented. Our studies allowed to improve the understanding of some LAr properties.",
    "Major chip manufacturers have all introduced Multithreaded processors. These processors are used for running a variety of workloads. Efficient resource utilization is an important design aspect in such processors. Particularly, it is important to take advantage of available memory-level parallelism(MLP). In this paper I propose a MLP aware operating system (OS) scheduling algorithm for Multithreaded Multi-core processors. By observing the MLP available in each thread and by balancing it with available MLP resources in the system the OS will come up with a new schedule of threads for the next quantum that could potentially improve overall performance. We do a qualitative comparison of our solution with other hardware and software techniques. This work can be extended by doing a quantitative evaluation and by further refining the scheduling optimization.",
    "We consider the problem of calibrating a compressed sensing measurement system under the assumption that the decalibration consists in unknown gains on each measure. We focus on {\\em blind} calibration, using measures performed on a few unknown (but sparse) signals. A naive formulation of this blind calibration problem, using $\\ell_{1}$ minimization, is reminiscent of blind source separation and dictionary learning, which are known to be highly non-convex and riddled with local minima. In the considered context, we show that in fact this formulation can be exactly expressed as a convex optimization problem, and can be solved using off-the-shelf algorithms. Numerical simulations demonstrate the effectiveness of the approach even for highly uncalibrated measures, when a sufficient number of (unknown, but sparse) calibrating signals is provided. We observe that the success/failure of the approach seems to obey sharp phase transitions.",
    "We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder- decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.",
    "An increasing number of use cases require a timely extraction of non-trivial knowledge from semantically annotated data streams, especially on the Web and for the Internet of Things (IoT). Often, this extraction requires expressive reasoning, which is challenging to compute on large streams. We propose Laser, a new reasoner that supports a pragmatic, non-trivial fragment of the logic LARS which extends Answer Set Programming (ASP) for streams. At its core, Laser implements a novel evaluation procedure which annotates formulae to avoid the re-computation of duplicates at multiple time points. This procedure, combined with a judicious implementation of the LARS operators, is responsible for significantly better runtimes than the ones of other state-of-the-art systems like C-SPARQL and CQELS, or an implementation of LARS which runs on the ASP solver Clingo. This enables the application of expressive logic-based reasoning to large streams and opens the door to a wider range of stream reasoning use cases.",
    "The second law of thermodynamics dictates the fundamental limits to the amount of energy and information that can be exchanged between physical systems. In this work, we extend a thermodynamic formalism describing this flow of energy and information developed for a pair of bipartite systems to many multipartite systems. We identify a natural thermodynamic quantity that describes the information exchanged among these systems. We then introduce and discuss a refined version. Our results are illustrated with a model of two, competing Maxwell demons.",
    "The negligible intrinsic spin-orbit coupling (SOC) in graphene can be enhanced by proximity effects in stacked heterostructures of graphene and transition metal dichalcogenides (TMDCs). The composition of the TMDC layer plays a key role in determining the nature and strength of the resultant SOC induced in the graphene layer. Here, we study the evolution of the proximity-induced SOC as the TMDC layer is deliberately defected. Alloyed ${\\rm G/W_{\\chi}Mo_{1-\\chi}Se_2}$ heterostructures with diverse compositions ($\\chi$) and defect distributions are simulated using density functional theory. Comparison with continuum and tight-binding models allows both local and global signatures of the metal-atom alloying to be clarified. Our findings show that, despite some dramatic perturbation of local parameters for individual defects, the low-energy spin and electronic behaviour follow a simple effective medium model which depends only on the composition ratio of the metallic species in the TMDC layer. Furthermore, we demonstrate that the topological state of such alloyed systems can be feasibly tuned by controlling this ratio.",
    "Atomic masses play a crucial role in many nuclear astrophysics calculations. The lack of experimental values for relevant exotic nuclides triggered a rapid development of new mass measurement devices around the world. The Time-of-Flight (TOF) mass measurements offer a complementary technique to the most precise one, Penning trap measurements, the latter being limited by the rate and half-lives of the ions of interest. The NSCL facility provides a well-suited infrastructure for TOF mass measurements of very exotic nuclei. At this facility, we have recently implemented a TOF-Brho technique and performed mass measurements of neutron-rich nuclides in the Fe region, important for r-process calculations and for calculations of processes occurring in the crust of accreting neutron stars.",
    "Although the super-massive (AGN) and stellar mass (XRBs) black holes have many properties in common, the broad emission lines (BELs) are exclusively signatures of the AGN. Based on the detection of these lines from SDSS data bases, there seems to be no AGN with mass M_BH \\lesssim 10^5 M_sun. In this paper we investigate if such low mass black holes are really non-existent or they are undetected because the BELs in them are not produced efficiently. Using the ionizing spectral energy distribution for a wide range of black hole mass, 10 - 10^9 M_sun, spanning XRBs to AGN, we calculate the equivalent widths (EWs) of ultraviolet and optical lines Ly\\alpha 1216 \\AA, H\\beta 4861 \\AA, CIV 1549 \\AA and MgII 2798 \\AA. The LOC (locally optimally emitting cloud) model has been used to describe the broad emission line region (BELR) for the calculations. We find that the hardening of the SED shape with decreasing mass do not decrease the BEL EWs. However, finite size of the BELR, as measured by the line widths, which is controlled by the mass of the black hole, regulates the production of these emission lines. There seems to be a peak in the EWs of the emission lines for typical AGN black holes of ~ 10^8 M_sun, below which the lines become intrinsically fainter with a sharp fall-off below ~ 10^6 M_sun. This may be the cause of the absence of low mass AGN in SDSS.",
    "The precision of synchronization algorithms based on the theory of pulse-coupled oscillators is evaluated on FPGA-based radios for the first time. Measurements show that such algorithms can reach precision in the low microsecond range when being implemented in the physical layer. Furthermore, we propose an algorithm extension accounting for phase rate deviations of the hardware and show that an improved precision below one microsecond is possible with this extension in the given setup. The resulting algorithm can thus be applied in ad hoc wireless systems for fully distributed synchronization of transmission slots or sleep cycles, in particular, if centralized synchronization is impossible.",
    "Human Trajectory Prediction (HTP) has gained much momentum in the last years and many solutions have been proposed to solve it. Proper benchmarking being a key issue for comparing methods, this paper addresses the question of evaluating how complex is a given dataset with respect to the prediction problem. For assessing a dataset complexity, we define a series of indicators around three concepts: Trajectory predictability; Trajectory regularity; Context complexity. We compare the most common datasets used in HTP in the light of these indicators and discuss what this may imply on benchmarking of HTP algorithms. Our source code is released on Github.",
    "The purpose of this paper is to show how a class of classical linear stochastic systems can be physically implemented using quantum optical components. Quantum optical systems typically have much higher bandwidth than electronic devices, meaning faster response and processing times, and hence has the potential for providing better performance than classical systems. A procedure is provided for constructing the quantum optical realization. The paper also describes the use of the quantum optical realization in a measurement feedback loop. Some examples are given to illustrate the application of the main results.",
    "Systems biology uses large networks of biochemical reactions to model the functioning of biological cells from the molecular to the cellular scale. The dynamics of dissipative reaction networks with many well separated time scales can be described as a sequence of successive equilibrations of different subsets of variables of the system. Polynomial systems with separation are equilibrated when at least two monomials, of opposite signs, have the same order of magnitude and dominate the others. These equilibrations and the corresponding truncated dynamics, obtained by eliminating the dominated terms, find a natural formulation in tropical analysis and can be used for model reduction.",
    "We performed spectral analysis of Suzaku data of the galactic disk and outflow regions of the starburst galaxy M82. Thermal modeling of the central disk regions requires at least three temperature components. The Ly$\\beta$ line fluxes of O VIII and Ne X exceed those expected from a plasma in collisional ionization equilibrium. The ratios of Ly$\\beta$/Ly$\\alpha$ lines for O VIII and Ne X are higher than those of collisional ionization equilibrium, which may be caused by the process of charge exchange. In the outflow wind region, the spectra are well reproduced with two-temperature thermal models, and we have derived the metal abundances of O, Ne, Mg, and Fe in the outflow. The ratios of O/Fe, Ne/Fe, and Mg/Fe are about 2, 3, and 2, respectively, relative to the solar value determined by Lodders (2003). Since there is no evidence of charge exchange in outflow region, the metal abundances should be more reliable than those in the central region. This abundance pattern indicates that starburst activity enriches the outflow through SN II metal ejection into intergalactic space.",
    "Dust grains are classically thought to form in the winds of asymptotic giant branch (AGB) stars. However, there is increasing evidence today for dust formation in supernovae (SNe). To establish the relative importance of these two classes of stellar sources of dust, it is important to know the fraction of freshly formed dust in SN ejecta that is able to survive the passage of the reverse shock and be injected in the interstellar medium. We have developed a new code (GRASH\\_Rev) which follows the newly-formed dust evolution throughout the supernova explosion until the merging of the forward shock with the circumstellar ISM. We have considered four well studied SNe in the Milky Way and Large Magellanic Cloud: SN1987A, CasA, the Crab Nebula, and N49. For all the simulated models, we find good agreement with observations and estimate that between 1 and 8$\\%$ of the observed mass will survive, leading to a SN dust production rate of $(3.9 \\pm 3.7) \\times 10^{-4}$ M$_{\\odot}$yr$^{-1}$ in the Milky Way. This value is one order of magnitude larger than the dust production rate by AGB stars but insufficient to counterbalance the dust destruction by SNe, therefore requiring dust accretion in the gas phase.",
    "This paper investigates in hatching process strategies for additive manufacturing using an electron beam by numerical simulations. The underlying physical model and the corresponding three dimensional thermal free surface lattice Boltzmann method of the simulation software are briefly presented. The simulation software has already been validated on the basis of experiments up to 1.2 kW beam power by hatching a cuboid with a basic process strategy, whereby the results are classified into `porous', `good' and `uneven', depending on their relative density and top surface smoothness. In this paper we study the limitations of this basic process strategy in terms of higher beam powers and scan velocities to exploit the future potential of high power electron beam guns up to 10 kW. Subsequently, we introduce modified process strategies, which circumvent these restrictions, to build the part as fast as possible under the restriction of a fully dense part with a smooth top surface. These process strategies are suitable to reduce the build time and costs, maximize the beam power usage and therefore use the potential of high power electron beam guns.",
    "Bayesian optimization (BO) is a class of global optimization algorithms, suitable for minimizing an expensive objective function in as few function evaluations as possible. While BO budgets are typically given in iterations, this implicitly measures convergence in terms of iteration count and assumes each evaluation has identical cost. In practice, evaluation costs may vary in different regions of the search space. For example, the cost of neural network training increases quadratically with layer size, which is a typical hyperparameter. Cost-aware BO measures convergence with alternative cost metrics such as time, energy, or money, for which vanilla BO methods are unsuited. We introduce Cost Apportioned BO (CArBO), which attempts to minimize an objective function in as little cost as possible. CArBO combines a cost-effective initial design with a cost-cooled optimization phase which depreciates a learned cost model as iterations proceed. On a set of 20 black-box function optimization problems we show that, given the same cost budget, CArBO finds significantly better hyperparameter configurations than competing methods.",
    "This work contributes a marsupial robotic system-of-systems involving a legged and an aerial robot capable of collaborative mapping and exploration path planning that exploits the heterogeneous properties of the two systems and the ability to selectively deploy the aerial system from the ground robot. Exploiting the dexterous locomotion capabilities and long endurance of quadruped robots, the marsupial combination can explore within large-scale and confined environments involving rough terrain. However, as certain types of terrain or vertical geometries can render any ground system unable to continue its exploration, the marsupial system can - when needed - deploy the flying robot which, by exploiting its 3D navigation capabilities, can undertake a focused exploration task within its endurance limitations. Focusing on autonomy, the two systems can co-localize and map together by sharing LiDAR-based maps and plan exploration paths individually, while a tailored graph search onboard the legged robot allows it to identify where and when the ferried aerial platform should be deployed. The system is verified within multiple experimental studies demonstrating the expanded exploration capabilities of the marsupial system-of-systems and facilitating the exploration of otherwise individually unreachable areas.",
    "We propose a method for polarising antiprotons in a storage ring by means of a polarised positron beam moving parallel to the antiprotons. If the relative velocity is adjusted to $v/c \\approx 0.002$ the cross section for spin-flip is as large as about $2 \\cdot 10^{13}$ barn as shown by new QED-calculations of the triple spin-cross sections. Two possibilities for providing a positron source with sufficient flux density are presented. A polarised positron beam with a polarisation of 0.70 and a flux density of approximately $1.5 \\cdot 10^{10}$/(mm$^2$ s) appears to be feasible by means of a radioactive $^{11}$C dc-source. A more involved proposal is the production of polarised positrons by pair production with circularly polarised photons. It yields a polarisation of 0.76 and requires the injection into a small storage ring. Such polariser sources can be used at low (100 MeV) as well as at high (1 GeV) energy storage rings providing a time of about one hour for polarisation build-up of about $10^{10}$ antiprotons to a polarisation of about 0.18. A comparison with other proposals show a gain in the figure-of-merit by a factor of about ten.",
    "Loops are essential secondary structure elements in folded DNA and RNA molecules and proliferate close to the melting transition. Using a theory for nucleic acid secondary structures that accounts for the logarithmic entropy c ln m for a loop of length m, we study homopolymeric single-stranded nucleic acid chains under external force and varying temperature. In the thermodynamic limit of a long strand, the chain displays a phase transition between a low temperature / low force compact (folded) structure and a high temperature / high force molten (unfolded) structure. The influence of c on phase diagrams, critical exponents, melting, and force extension curves is derived analytically. For vanishing pulling force, only for the limited range of loop exponents 2 < c < 2.479 a melting transition is possible; for c <= 2 the chain is always in the folded phase and for 2.479 < c always in the unfolded phase. A force induced melting transition with singular behavior is possible for all loop exponents c < 2.479 and can be observed experimentally by single molecule force spectroscopy. These findings have implications for the hybridization or denaturation of double stranded nucleic acids. The Poland-Scheraga model for nucleic acid duplex melting does not allow base pairing between nucleotides on the same strand in denatured regions of the double strand. If the sequence allows these intra-strand base pairs, we show that for a realistic loop exponent c ~ 2.1 pronounced secondary structures appear inside the single strands. This leads to a lower melting temperature of the duplex than predicted by the Poland-Scheraga model. Further, these secondary structures renormalize the effective loop exponent c^, which characterizes the weight of a denatured region of the double strand, and thus affect universal aspects of the duplex melting transition.",
    "We study the Zeeman spin-splitting in hole quantum wires oriented along the $[011]$ and $[01\\bar{1}]$ crystallographic axes of a high mobility undoped (100)-oriented AlGaAs/GaAs heterostructure. Our data shows that the spin-splitting can be switched `on' (finite $g^{*}$) or `off' (zero $g^{*}$) by rotating the field from a parallel to a perpendicular orientation with respect to the wire, and the properties of the wire are identical for the two orientations with respect to the crystallographic axes. We also find that the $g$-factor in the parallel orientation decreases as the wire is narrowed. This is in contrast to electron quantum wires, where the $g$-factor is enhanced by exchange effects as the wire is narrowed. This is evidence for a $k$-dependent Zeeman splitting that arises from the spin-3/2 nature of holes.",
    "An analogy with real Clifford algebras on even-dimensional vector spaces suggests to assign a couple of space and time dimensions modulo 8 to any algebra (represented over a complex Hilbert space) containing two self-adjoint involutions and an anti-unitary operator with specific commutation relations. It is shown that this assignment is compatible with the tensor product: the space and time dimensions of the tensor product are the sums of the space and time dimensions of its factors. This could provide an interpretation of the presence of such algebras in PT-symmetric Hamiltonians or the description of topological matter. This construction is used to build an indefinite (i.e. pseudo-Riemannian) version of the spectral triples of noncommutative geometry, defined over Krein spaces instead of Hilbert spaces. Within this framework, we can express the Lagrangian (both bosonic and fermionic) of a Lorentzian almost-commutative spectral triple. We exhibit a space of physical states that solves the fermion-doubling problem. The example of quantum electrodynamics is described.",
    "We study the space-time symmetries of the actions obtained by expanding the action for a massive free relativistic particle around the Galilean action. We obtain all the point space-time symmetries of the post-Galilean actions by working in canonical space. We also construct an infinite collection of generalized Schr\\\"odinger algebras parameterized by an integer $M$, with $M=0$ corresponding to the standard Schr\\\"odinger algebra. We discuss the Schr\\\"odinger equations associated to these algebras, their solutions and projective phases.",
    "Accretion disc theory is less developed than stellar evolution theory although a similarly mature phenomenological picture is ultimately desired. While the interplay of theory and numerical simulations has amplified community awareness of the role of magnetic fields in angular momentum transport, there remains a long term challenge to incorporate insight gained from simulations back into improving practical models for comparison with observations. Here we emphasize the need to incorporate the role of non-local transport more precisely. To show where large scale transport would fit into the theoretical framework and how it is currently missing, we review why the wonderfully practical approach of Shakura-Sunyaev (1973,SS73) is necessarily a mean field theory, and one which does not include large scale transport. Observations of coronae and jets combined with the interpretation of results even from shearing box simulations of the magnetorotational instability (MRI) suggest that a significant fraction of disc transport is indeed non-local. We show that the Maxwell stresses in saturation are dominated by large scale contributions and the physics of MRI transport is not fully captured by a viscosity. We also clarify the standard physical interpretation of the MRi as it applies to shearing boxes. Computational limitations have so far focused most attention toward local simulations but the next generation of global simulations should help to inform improved mean field theories. Mean field accretion theory and mean field dynamo theory should in fact be unified into a single theory that predicts the time evolution of spectra and luminosity from separate disc, corona, and outflow contributions. Finally, we note that any mean field theory has a finite predictive precision that needs to be quantified when comparing the predictions to observations.",
    "This paper is devoted to the global well-posedness of two Diffuse Interface systems modeling the motion of an incompressible two-phase fluid mixture in presence of capillarity effects in a bounded smooth domain $\\Omega\\subset \\mathbb{R}^d$, $d=2,3$. We focus on dissipative mixing effects originating from the mass-conserving Allen-Cahn dynamics with the physically relevant Flory-Huggins potential. More precisely, we study the mass-conserving Navier-Stokes-Allen-Cahn system for nonhomogeneous fluids and the mass-conserving Euler-Allen-Cahn system for homogeneous fluids. We prove existence and uniqueness of global weak and strong solutions as well as their property of separation from the pure states. In our analysis, we combine the energy and entropy estimates, a novel end-point estimate of the product of two functions, a new estimate for the Stokes problem with non-constant viscosity, and logarithmic type Gronwall arguments.",
    "We present explicit expressions for Fock-space projection operators that correspond to realistic final states in scattering experiments. Our operators automatically sum over unobserved quanta and account for non-emission into sub-regions of momentum space.",
    "In this talk we discuss mathematical structures associated to Feynman graphs. Feynman graphs are the backbone of calculations in perturbative quantum field theory. The mathematical structures -- apart from being of interest in their own right -- allow to derive algorithms for the computation of these graphs. Topics covered are the relations of Feynman integrals to periods, shuffle algebras and multiple polylogarithms.",
    "We report on a calculation of the generalized parton distributions of the photon when there is non-zero momentum transfer both in the transverse and longitudinal directions. By taking Fourier transforms of the GPDs with respect transverse and longitudinal momentum transfer, we obtain the parton distributions of the photon in position space.",
    "Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.",
    "We compute the leading logarithmic behaviour of the cross-section for the production of a pseudoscalar Higgs boson in gluon-gluon fusion to all-orders in perturbation theory, in the limit of large partonic centre of mass energy. We also calculate the Higgs rapidity distribution to the same accuracy. We include the contributions of top and bottom quarks, together with their interference. Our results are given in terms of single and double integrals, evaluated explicitly up to next-to next-to leading order (NNLO). We use our results to improve the known NNLO inclusive cross-section computed in the effective theory where the fermions in the loop are integrated out. The size of finite fermion mass effects on the inclusive cross-section is found to be small, reaching a few percent only for large values of the pseudoscalar mass.",
    "Finding outlying elements in probability distributions can be a hard problem. Taking a real example from Voting Rights Act enforcement, we consider the problem of maximizing the number of simultaneous majority-minority districts in a political districting plan. An unbiased random walk on districting plans is unlikely to find plans that approach this maximum. A common search approach is to use a biased random walk: preferentially select districting plans with more majority-minority districts. Here, we present a third option, called short bursts, in which an unbiased random walk is performed for a small number of steps (called the burst length), then re-started from the most extreme plan that was encountered in the last burst. We give empirical evidence that short-burst runs outperform biased random walks for the problem of maximizing the number of majority-minority districts, and that there are many values of burst length for which we see this improvement. Abstracting from our use case, we also consider short bursts where the underlying state space is a line with various probability distributions, and then explore some features of more complicated state spaces and how these impact the effectiveness of short bursts.",
    "We report on a molecular dynamics investigation of the wetting properties of graphitic surfaces by various solutions at concentrations 1-8 wt% of commercially available non-ionic surfactants with long hydrophilic chains, linear or T-shaped. These are surfactants of length up to 160 [\\AA]. It turns out that molecular dynamics simulations of such systems ask for a number of solvent particles that can be reached without seriously compromising computational efficiency only by employing a coarse-grained model. The MARTINI force field with polarizable water offers a framework particularly suited for the parameterization of our systems. In general, its advantages over other coarse-grained models are the possibility to explore faster long time scales and the wider range of applicability. Although the accuracy is sometimes put under question, the results for the wetting properties by pure water are in good agreement with those for the corresponding atomistic systems and theoretical predictions. On the other hand, the bulk properties of various aqueous surfactant solutions indicate that the micellar formation process is too strong. For this reason, a typical experimental configuration is better approached by preparing the droplets with the surfactants arranged in the initial state in the vicinity of contact line. Cross-comparisons are possible and illuminating, but equilibrium contanct angles as obtained from simulations overestimate the experimental results. Nevertheless, our findings can provide guidelines for the preliminary assessment and screening of surfactants. [See pdf file for full abstract]",
    "We review recent experiments in which superfluid $^3$He has been studied under highly controlled confinement in nanofluidic sample chambers. We discuss the experimental challenges and their resolution. These methods open the way to a systematic investigation of the superfluidity of $^3$He films, and the surface and edge excitations of topological superfluids.",
    "Code-mixed machine translation has become an important task in multilingual communities and extending the task of machine translation to code mixed data has become a common task for these languages. In the shared tasks of WMT 2022, we try to tackle the same for both English + Hindi to Hinglish and Hinglish to English. The first task dealt with both Roman and Devanagari script as we had monolingual data in both English and Hindi whereas the second task only had data in Roman script. To our knowledge, we achieved one of the top ROUGE-L and WER scores for the first task of Monolingual to Code-Mixed machine translation. In this paper, we discuss the use of mBART with some special pre-processing and post-processing (transliteration from Devanagari to Roman) for the first task in detail and the experiments that we performed for the second task of translating code-mixed Hinglish to monolingual English.",
    "Contrastive learning has shown promising potential in self-supervised spatio-temporal representation learning. Most works naively sample different clips to construct positive and negative pairs. However, we observe that this formulation inclines the model towards the background scene bias. The underlying reasons are twofold. First, the scene difference is usually more noticeable and easier to discriminate than the motion difference. Second, the clips sampled from the same video often share similar backgrounds but have distinct motions. Simply regarding them as positive pairs will draw the model to the static background rather than the motion pattern. To tackle this challenge, this paper presents a novel dual contrastive formulation. Concretely, we decouple the input RGB video sequence into two complementary modes, static scene and dynamic motion. Then, the original RGB features are pulled closer to the static features and the aligned dynamic features, respectively. In this way, the static scene and the dynamic motion are simultaneously encoded into the compact RGB representation. We further conduct the feature space decoupling via activation maps to distill static- and dynamic-related features. We term our method as \\textbf{D}ual \\textbf{C}ontrastive \\textbf{L}earning for spatio-temporal \\textbf{R}epresentation (DCLR). Extensive experiments demonstrate that DCLR learns effective spatio-temporal representations and obtains state-of-the-art or comparable performance on UCF-101, HMDB-51, and Diving-48 datasets.",
    "The electronic bandstructure and the Fermi surfaces of ferromagnetic CeRh3B2 are calculated by using FLAPW and LSDA+U method. As assuming several kinds of the ground state to describe the 4f electronic state, we propose a fully orbital- and spin-polarized state |lz=0, sx=1/2> as the ground state, instead of the conventional LS-coupled CEF ground state, generally expected in typical 4f compounds. This is supported by the fact that both the observed magnetic moment and the observed dHvA frequencies are well explained by the calculated electronic structure and the Fermi surfaces. The unconventional ground state is stabilized by the strong 4f-4f direct mixing between the neighbored Ce atoms along the extremely small distance along the c-axis in the hexagonal crystal cell.",
    "Matrix acidization simulation is a challenging task in the study of flows in porous media, due to the changing porosity in the procedure. The improved DBF framework is one model to do this simulation, and its numerical scheme discretises the mass and momentum conservation equations together to form a pressure-velocity linear system. However, this linear system can only be solved by direct solvers to solve for pressure and velocity simultaneously, since zeros appear in the diagonal of the coefficient matrix. Considering the large-scale attribute of matrix acidization simulation, the solving time of direct solvers is not intolerant. Thus, a decoupled scheme is proposed in this work to decouple the coupled pressure-velocity linear system into two independent linear systems: one is to solve for pressure, and the other one is to solve for velocity. Both of the new linear systems can be solved by parallel and iterative solvers, which guarantees the large-scale simulation can be finished in a reasonable time period. A numerical experiment is carried out to demonstrate the correctness of the decoupled scheme and its higher computing efficiency.",
    "Sensemaking and narrative are two inherently interconnected concepts about how people understand the world around them. Sensemaking is the process by which people structure and interconnect the information they encounter in the world with the knowledge and inferences they have made in the past. Narratives are important constructs that people use sensemaking to create; ones that reflect provide a more holistic account of the world than the information within any given narrative is able to alone. Both are important to how human beings parse the world, and both would be valuable for a computational system attempting to do the same. In this paper, we discuss theories of sensemaking and narrative with respect to how people build an understanding of the world based on the information they encounter, as well as the links between the fields of sensemaking and narrative research. We highlight a specific computational task, visual storytelling, whose solutions we believe can be enhanced by employing a sensemaking and narrative component. We then describe our system for visual storytelling using sensemaking and narrative and discuss examples from its current implementation.",
    "Evaluation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. However, currently, there exists no way to quantify how metrics respond to change in the dialect of a generated utterance. We thus formalize dialect robustness and dialect awareness as goals for NLG evaluation metrics. We introduce a suite of methods and corresponding statistical tests one can use to assess metrics in light of the two goals. Applying the suite to current state-of-the-art metrics, we demonstrate that they are not dialect-robust and that semantic perturbations frequently lead to smaller decreases in a metric than the introduction of dialect features. As a first step to overcome this limitation, we propose a training schema, NANO, which introduces regional and language information to the pretraining process of a metric. We demonstrate that NANO provides a size-efficient way for models to improve the dialect robustness while simultaneously improving their performance on the standard metric benchmark.",
    "Geographic routing consists in using the position information of nodes to assist in the routing process, and has been a widely studied subject in sensor networks. One of the outstanding challenges facing geographic routing has been its applicability. Authors either make some broad assumptions on an idealized version of wireless networks which are often unverifiable, or they use costly methods to planarize the communication graph. The overarching questions that drive us are the following. When, and how should we use geographic routing? Is there a criterion to tell whether a communication network is fit for geographic routing? When exactly does geographic routing make sense? In this paper we formulate the four principles that define geographic routing and explore their topological consequences. Given a localized communication network, we then define and compute its geographic eccentricity, which measures its fitness for geographic routing. Finally we propose a distributed algorithm that either enables geographic routing on the network or proves that its geographic eccentricity is too high.",
    "We show that spatial variation and correlation of superconductivity fluctuations in a two-band model are scaled by two characteristic lengths. This results in substantially more complicated picture compared to one-band systems. In particular, short-range correlations are always present in a two-band scenario, even near the phase transition point.",
    "We present online prediction methods for time series that let us explicitly handle nonstationary artifacts (e.g. trend and seasonality) present in most real time series. Specifically, we show that applying appropriate transformations to such time series before prediction can lead to improved theoretical and empirical prediction performance. Moreover, since these transformations are usually unknown, we employ the learning with experts setting to develop a fully online method (NonSTOP-NonSTationary Online Prediction) for predicting nonstationary time series. This framework allows for seasonality and/or other trends in univariate time series and cointegration in multivariate time series. Our algorithms and regret analysis subsume recent related work while significantly expanding the applicability of such methods. For all the methods, we provide sub-linear regret bounds using relaxed assumptions. The theoretical guarantees do not fully capture the benefits of the transformations, thus we provide a data-dependent analysis of the follow-the-leader algorithm that provides insight into the success of using such transformations. We support all of our results with experiments on simulated and real data.",
    "We present a heuristic framework for attacking the undecidable termination problem of logic programs, as an alternative to current termination/non-termination proof approaches. We introduce an idea of termination prediction, which predicts termination of a logic program in case that neither a termination nor a non-termination proof is applicable. We establish a necessary and sufficient characterization of infinite (generalized) SLDNF-derivations with arbitrary (concrete or moded) queries, and develop an algorithm that predicts termination of general logic programs with arbitrary non-floundering queries. We have implemented a termination prediction tool and obtained quite satisfactory experimental results. Except for five programs which break the experiment time limit, our prediction is 100% correct for all 296 benchmark programs of the Termination Competition 2007, of which eighteen programs cannot be proved by any of the existing state-of-the-art analyzers like AProVE07, NTI, Polytool and TALP.",
    "Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoretically tractable variant of random regression forests and prove that our algorithm is consistent. We also provide an empirical evaluation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in practice. Our experiments provide insight into the relative importance of different simplifications that theoreticians have made to obtain tractable models for analysis.",
    "Factorial Hidden Markov Models (FHMMs) are powerful models for sequential data but they do not scale well with long sequences. We propose a scalable inference and learning algorithm for FHMMs that draws on ideas from the stochastic variational inference, neural network and copula literatures. Unlike existing approaches, the proposed algorithm requires no message passing procedure among latent variables and can be distributed to a network of computers to speed up learning. Our experiments corroborate that the proposed algorithm does not introduce further approximation bias compared to the proven structured mean-field algorithm, and achieves better performance with long sequences and large FHMMs.",
    "Molecular dynamics simulations have been performed on pure liquid water, aqueous solutions of sodium chloride, and polymer solutions exposed to a strong external electric field with the goal to gain molecular insight into the structural response to the field. Several simulation methodologies have been used to elucidate the molecular mechanisms of the processes leading to the formation of liquid bridges and jets (in the production of nanofibers). It is shown that in the established nanoscale structures, the molecules form a chain with their dipole moments oriented parallel to the applied field throughout the entire sample volume. The presence of ions may disturb this structure leading to its ultimate disintegration into droplets; the concentration dependence of the threshold field required to stabilize a liquid column has been determined. Conformational changes of the polymer in the jetting process have also been observed.",
    "Recommender systems are increasingly used to predict and serve content that aligns with user taste, yet the task of matching new users with relevant content remains a challenge. We consider podcasting to be an emerging medium with rapid growth in adoption, and discuss challenges that arise when applying traditional recommendation approaches to address the cold-start problem. Using music consumption behavior, we examine two main techniques in inferring Spotify users preferences over more than 200k podcasts. Our results show significant improvements in consumption of up to 50\\% for both offline and online experiments. We provide extensive analysis on model performance and examine the degree to which music data as an input source introduces bias in recommendations.",
    "We calculate the Casimir energy and entropy for two perfect metal spheres in the large and short separation limit. We obtain nonmonotonic behavior of the Helmholtz free energy with separation and temperature, leading to parameter ranges with negative entropy, and also nonmonotonic behavior of the entropy with temperature and with the separation between the spheres. The appearance of this anomalous behavior of the entropy is discussed as well as its thermodynamic consequences.",
    "Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.",
    "In this paper, we propose two mask-based beamforming methods using a deep neural network (DNN) trained by multichannel loss functions. Beamforming technique using time-frequency (TF)-masks estimated by a DNN have been applied to many applications where TF-masks are used for estimating spatial covariance matrices. To train a DNN for mask-based beamforming, loss functions designed for monaural speech enhancement/separation have been employed. Although such a training criterion is simple, it does not directly correspond to the performance of mask-based beamforming. To overcome this problem, we use multichannel loss functions which evaluate the estimated spatial covariance matrices based on the multichannel Itakura--Saito divergence. DNNs trained by the multichannel loss functions can be applied to construct several beamformers. Experimental results confirmed their effectiveness and robustness to microphone configurations.",
    "Nano-FTIR imaging is a powerful scanning-based technique at nanometer spatial resolution which combines Fourier transform infrared spectroscopy (FTIR) and scattering-type scanning near-field optical microscopy (s-SNOM). However, recording large spatial areas with nano-FTIR is limited by long measurement times due to its sequential data acquisition. Several mathematical approaches have been proposed to tackle this problem. All of them have in common that only a small fraction of randomly chosen measurements is required. However, choosing the fraction of measurements in a random fashion poses practical challenges for scanning procedures and does not lead to time savings as large as desired. We consider different, practically relevant sub-sampling schemes assuring a faster acquisition. It is demonstrated that results for almost all considered sub-sampling schemes, namely original Lissajous, triangle Lissajous, and random reflection sub-sampling, at a sub-sampling rate of 10%, are comparable to results when using a random sub-sampling of 10%. This implies that random sub-sampling is not required for efficient data acquisition.",
    "We determine from Polyakov loop correlators the screening masses in th e deconfined phase of the (3+1)d SU(3) pure gauge theory at finite temperature near transition, for two different channels of angular momentum and parity. Their ratio is compared with that of the massive excitations with the same quantum numbers in the 3d 3-state Potts model in the broken phase near the transition point at zero magnetic field. Moreover we study the inverse decay length of the correlation between the real parts and between the imaginary parts of the Polyakov loop and compare the results with expectations from perturbation theory and mean-field Polyakov loop models.",
    "The Mahalanobis distance-based confidence score, a recently proposed anomaly detection method for pre-trained neural classifiers, achieves state-of-the-art performance on both out-of-distribution (OoD) and adversarial examples detection. This work analyzes why this method exhibits such strong performance in practical settings while imposing an implausible assumption; namely, that class conditional distributions of pre-trained features have tied covariance. Although the Mahalanobis distance-based method is claimed to be motivated by classification prediction confidence, we find that its superior performance stems from information not useful for classification. This suggests that the reason the Mahalanobis confidence score works so well is mistaken, and makes use of different information from ODIN, another popular OoD detection method based on prediction confidence. This perspective motivates us to combine these two methods, and the combined detector exhibits improved performance and robustness. These findings provide insight into the behavior of neural classifiers in response to anomalous inputs.",
    "Algorithms like those for differentiating functional expressions manipulate the syntactic structure of mathematical expressions in a mathematically meaningful way. A formalization of such an algorithm should include a specification of its computational behavior, a specification of its mathematical meaning, and a mechanism for applying the algorithm to actual expressions. Achieving these goals requires the ability to integrate reasoning about the syntax of the expressions with reasoning about what the expressions mean. A syntax framework is a mathematical structure that is an abstract model for a syntax reasoning system. It contains a mapping of expressions to syntactic values that represent the syntactic structures of the expressions; a language for reasoning about syntactic values; a quotation mechanism to refer to the syntactic value of an expression; and an evaluation mechanism to refer to the value of the expression represented by a syntactic value. We present and compare two approaches, based on instances of a syntax framework, to formalize a syntax-based mathematical algorithm in a formal theory T. In the first approach the syntactic values for the expressions manipulated by the algorithm are members of an inductive type in T, but quotation and evaluation are functions defined in the metatheory of T. In the second approach every expression in T is represented by a syntactic value, and quotation and evaluation are operators in T itself.",
    "The paper concerns two interacting consumer-resource pairs based on chemostat-like equations under the assumption that the dynamics of the resource is considerably slower than that of the consumer. The presence of two different time scales enables to carry out a fairly complete analysis of the problem. This is done by treating consumers and resources in the coupled system as fast-scale and slow-scale variables respectively and subsequently considering developments in phase planes of these variables, fast and slow, as if they are independent. When uncoupled, each pair has unique asymptotically stable steady state and no self-sustained oscillatory behavior (although damped oscillations about the equilibrium are admitted). When the consumer-resource pairs are weakly coupled through direct reciprocal inhibition of consumers, the whole system exhibits self-sustained relaxation oscillations with a period that can be significantly longer than intrinsic relaxation time of either pair. It is shown that the model equations adequately describe locally linked consumer-resource systems of quite different nature: living populations under interspecific interference competition and lasers coupled via their cavity losses.",
    "Wireless local area networks (WLAN) still suffer from a severe performance discrepancy between different users in the uplink. This is because of the spatially varying channel conditions provided by the wireless medium. Cooperative medium access control (MAC) protocols as for example CoopMAC were proposed to mitigate this problem. In this work, it is shown that cooperation implies for cooperating nodes a tradeoff between throughput and bit-cost, which is the energy needed to transmit one bit. The tradeoff depends on the degree of cooperation. For carrier sense multiple access (CSMA) based networks, the throughput/bit-cost tradeoff curve is theoretically derived. A new distributed CSMA protocol called fairMAC is proposed and it is theoretically shown that fairMAC can asymptotically achieve any operating point on the tradeoff curve when the packet lengths go to infinity. The theoretical results are validated through Monte Carlo simulations.",
    "Social tagging has become an interesting approach to improve search and navigation over the actual Web, since it aggregates the tags added by different users to the same resource in a collaborative way. This way, it results in a list of weighted tags describing its resource. Combined to a classical taxonomic classification system such as that by Wikipedia, social tags can enhance document navigation and search. On the one hand, social tags suggest alternative navigation ways, including pivot-browsing, popularity-driven navigation, and filtering. On the other hand, it provides new metadata, sometimes uncovered by documents' content, that can substantially improve document search. In this work, the inclusion of an interface to add user-defined tags describing Wikipedia articles is proposed, as a way to improve article navigation and retrieval. As a result, a prototype on applying tags over Wikipedia is proposed in order to evaluate its effectiveness.",
    "Quantum Computing and especially Quantum Machine Learning, in a short period of time, has gained a lot of interest through research groups around the world. This can be seen in the increasing number of proposed models for pattern classification applying quantum principles to a certain degree. Despise the increasing volume of models, there is a void in testing these models on real datasets and not only on synthetic ones. The objective of this work is to classify patterns with binary attributes using a quantum classifier. Specially, we show results of a complete quantum classifier applied to image datasets. The experiments show favorable output while dealing with balanced classification problems as well as with imbalanced classes where the minority class is the most relevant. This is promising in medical areas, where usually the important class is also the minority class.",
    "We present near- and mid-infrared observations on the shock-cloud interaction region in the southern part of the supernova remnant HB 21, performed with the InfraRed Camera (IRC) aboard AKARI satellite and the Wide InfraRed Camera (WIRC) at the Palomar 5 m telescope. The IRC 4 um (N4), 7 um (S7), and 11 um (S11) band images and the WIRC H2 v=1->0 S(1) 2.12 um image show similar diffuse features, around a shocked CO cloud. We analyzed the emission through comparison with the H2 line emission of several shock models. The IRC colors are well explained by the thermal admixture model of H2 gas--whose infinitesimal H2 column density has a power-law relation with the temperature $T$, $dN\\sim T^{-b}dT$--with n(H2) $\\sim3.9\\times10^4$ cm^{-2}, $b\\sim4.2$, and N(H2;T>100K) $\\sim2.8\\times10^{21}$ cm^{-2}. We interpreted these parameters with several different pictures of the shock-cloud interactions--multiple planar C-shocks, bow shocks, and shocked clumps--and discuss their weaknesses and strengths. The observed H2 v=1->0 S(1) intensity is four times greater than the prediction from the power-law admixture model, the same tendency as found in the northern part of HB 21 (Paper I). We also explored the limitation of the thermal admixture model with respect to the derived model parameters.",
    "Recently, vision transformers started to show impressive results which outperform large convolution based models significantly. However, in the area of small models for mobile or resource constrained devices, ConvNet still has its own advantages in both performance and model complexity. We propose ParC-Net, a pure ConvNet based backbone model that further strengthens these advantages by fusing the merits of vision transformers into ConvNets. Specifically, we propose position aware circular convolution (ParC), a light-weight convolution op which boasts a global receptive field while producing location sensitive features as in local convolutions. We combine the ParCs and squeeze-exictation ops to form a meta-former like model block, which further has the attention mechanism like transformers. The aforementioned block can be used in plug-and-play manner to replace relevant blocks in ConvNets or transformers. Experiment results show that the proposed ParC-Net achieves better performance than popular light-weight ConvNets and vision transformer based models in common vision tasks and datasets, while having fewer parameters and faster inference speed. For classification on ImageNet-1k, ParC-Net achieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11% parameters and 13% computational cost but gaining 0.2% higher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288) compared with MobileViT, and uses only 0.5 times parameters but gaining 2.7% accuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC segmentation tasks, ParC-Net also shows better performance. Source code is available at https://github.com/hkzhang91/ParC-Net",
    "From the algebraic solution of $x^{n}-x+t=0$ for $n=2,3,4$ and the corresponding solution in terms of hypergeometric functions, we obtain a set of reduction formulas for hypergeometric functions. By differentiation and integration of these results, and applying other known reduction formulas of hypergeometric functions, we derive new reduction formulas of special functions as well as the calculation of some infinite integrals in terms of elementary functions.",
    "Air-gap covert channels are special types of covert communication channels that enable attackers to exfiltrate data from isolated, network-less computers. Various types of air-gap covert channels have been demonstrated over the years, including electromagnetic, magnetic, acoustic, optical, and thermal. In this paper, we introduce a new type of vibrational (seismic) covert channel. We observe that computers vibrate at a frequency correlated to the rotation speed of their internal fans. These inaudible vibrations affect the entire structure on which the computer is placed. Our method is based on malware's capability of controlling the vibrations generated by a computer, by regulating its internal fan speeds. We show that the malware-generated covert vibrations can be sensed by nearby smartphones via the integrated, sensitive \\textit{accelerometers}. Notably, the accelerometer sensors in smartphones can be accessed by any app without requiring the user permissions, which make this attack highly evasive. We implemented AiR-ViBeR, malware that encodes binary information, and modulate it over a low frequency vibrational carrier. The data is then decoded by malicious application on a smartphone placed on the same surface (e.g., on a desk). We discuss the attack model, provide technical background, and present the implementation details and evaluation results. Our results show that using AiR-ViBeR, data can be exfiltrated from air-gapped computer to a nearby smartphone on the same table, or even an adjacent table, via vibrations. Finally, we propose a set of countermeasures for this new type of attack.",
    "The total cost of a 25 W average load magnetic refrigerator using commercial grade Gd is calculated using a numerical model. The price of magnetocaloric material, magnet material and cost of operation are considered, and all influence the total cost. The lowest combined total cost with a device lifetime of 15 years is found to be in the range \\$150-\\$400 depending on the price of the magnetocaloric and magnet material. The cost of the magnet is largest, followed closely by the cost of operation, while the cost of the magnetocaloric material is almost negligible. For the lowest cost device, the optimal magnetic field is about 1.4 T, the particle size is 0.23 mm, the length of the regenerator is 40-50 mm and the utilization is about 0.2, for all device lifetimes and material and magnet prices, while the operating frequency vary as function of device lifetime. The considered performance characteristics are based on the performance of a conventional A$^{+++}$ refrigeration unit. In a rough life time cost comparison between the magnetic refrigeration device and such a unit we find similar costs, the former being slightly cheaper, assuming the cost of the magnet can be recuperated at end of life.",
    "We prove the existence of initial data sets which possess an asymptotically flat and an asymptotically cylindrical end. Such geometries are known as trumpets in the community of numerical relativists.",
    "Enzymes utilize protein architectures to create highly specialized structural motifs that can greatly enhance the rates of complex chemical transformations. Here we use experiments, combined with ab initio simulations that exactly include nuclear quantum effects, to show that a triad of strongly hydrogen bonded tyrosine residues within the active site of the enzyme ketosteroid isomerase (KSI) facilitates quantum proton delocalization. This delocalization dramatically stabilizes the deprotonation of an active site tyrosine residue, resulting in a very large isotope effect on its acidity. When an intermediate analog is docked, it is incorporated into the hydrogen bond network, giving rise to extended quantum proton delocalization in the active site. These results shed light on the role of nuclear quantum effects in the hydrogen bond network that stabilizes the reactive intermediate of KSI, and the behavior of protons in biological systems containing strong hydrogen bonds.",
    "In this work, we propose ENSEI, a secure inference (SI) framework based on the frequency-domain secure convolution (FDSC) protocol for the efficient execution of privacy-preserving visual recognition. Our observation is that, under the combination of homomorphic encryption and secret sharing, homomorphic convolution can be obliviously carried out in the frequency domain, significantly simplifying the related computations. We provide protocol designs and parameter derivations for number-theoretic transform (NTT) based FDSC. In the experiment, we thoroughly study the accuracy-efficiency trade-offs between time- and frequency-domain homomorphic convolution. With ENSEI, compared to the best known works, we achieve 5--11x online time reduction, up to 33x setup time reduction, and up to 10x reduction in the overall inference time. A further 33% of bandwidth reductions can be obtained on binary neural networks with only 1% of accuracy degradation on the CIFAR-10 dataset.",
    "Recommender systems benefit us in tackling the problem of information overload by predicting our potential choices among diverse niche objects. So far, a variety of personalized recommendation algorithms have been proposed and most of them are based on similarities, such as collaborative filtering and mass diffusion. Here, we propose a novel vertex similarity index named CosRA, which combines advantages of both the cosine index and the resource-allocation (RA) index. By applying the CosRA index to real recommender systems including MovieLens, Netflix and RYM, we show that the CosRA-based method has better performance in accuracy, diversity and novelty than some benchmark methods. Moreover, the CosRA index is free of parameters, which is a significant advantage in real applications. Further experiments show that the introduction of two turnable parameters cannot remarkably improve the overall performance of the CosRA index.",
    "Many real-world applications adopt multi-label data streams as the need for algorithms to deal with rapidly changing data increases. Changes in data distribution, also known as concept drift, cause the existing classification models to rapidly lose their effectiveness. To assist the classifiers, we propose a novel algorithm called Label Dependency Drift Detector (LD3), an implicit (unsupervised) concept drift detector using label dependencies within the data for multi-label data streams. Our study exploits the dynamic temporal dependencies between labels using a label influence ranking method, which leverages a data fusion algorithm and uses the produced ranking to detect concept drift. LD3 is the first unsupervised concept drift detection algorithm in the multi-label classification problem area. In this study, we perform an extensive evaluation of LD3 by comparing it with 14 prevalent supervised concept drift detection algorithms that we adapt to the problem area using 12 datasets and a baseline classifier. The results show that LD3 provides between 19.8\\% and 68.6\\% better predictive performance than comparable detectors on both real-world and synthetic data streams.",
    "The universality of the Cepheid Period-Luminosity relations has been under discussion since metallicity effects have been assumed to play a role in the value of the intercept and, more recently, of the slope of these relations. The goal of the present study is to calibrate the Galactic PL relations in various photometric bands (from B to K) and to compare the results to the well-established PL relations in the LMC. We use a set of 59 calibrating stars, the distances of which are measured using five different distance indicators: Hubble Space Telescope and revised Hipparcos parallaxes, infrared surface brightness and interferometric Baade-Wesselink parallaxes, and classical Zero-Age-Main-Sequence-fitting parallaxes for Cepheids belonging to open clusters or OB stars associations. A detailed discussion of absorption corrections and projection factor to be used is given. We find no significant difference in the slopes of the PL relations between LMC and our Galaxy. We conclude that the Cepheid PL relations have universal slopes in all photometric bands, not depending on the galaxy under study (at least for LMC and Milky Way). The possible zero-point variation with metal content is not discussed in the present work, but an upper limit of 18.50 for the LMC distance modulus can be deduced from our data.",
    "Ensembling methods are well known for improving prediction accuracy. However, they are limited in the sense that they cannot discriminate among component models effectively. In this paper, we propose stacking with auxiliary features that learns to fuse relevant information from multiple systems to improve performance. Auxiliary features enable the stacker to rely on systems that not just agree on an output but also the provenance of the output. We demonstrate our approach on three very different and difficult problems -- the Cold Start Slot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet object detection tasks. We obtain new state-of-the-art results on the first two tasks and substantial improvements on the detection task, thus verifying the power and generality of our approach.",
    "We present a simple and fast method to simulate spin-torque driven magnetisation dynamics in nano-pillar spin-valve structures. The approach is based on the coupling between a spin transport code based on random matrix theory and a micromagnetics finite-elements software. In this way the spatial dependence of both spin transport and magnetisation dynamics is properly taken into account. Our results are compared with experiments. The excitation of the spin-wave modes, in- cluding the threshold current for steady state magnetisation precession and the nonlinear frequency shift of the modes are reproduced correctly. The giant magneto resistance effect and the magnetisa- tion switching also agree with experiment. The similarities with recently described spin-caloritronics devices are also discussed.",
    "We present a simple framework to compute hyperbolic Voronoi diagrams of finite point sets as affine diagrams. We prove that bisectors in Klein's non-conformal disk model are hyperplanes that can be interpreted as power bisectors of Euclidean balls. Therefore our method simply consists in computing an equivalent clipped power diagram followed by a mapping transformation depending on the selected representation of the hyperbolic space (e.g., Poincar\\'e conformal disk or upper-plane representations). We discuss on extensions of this approach to weighted and $k$-order diagrams, and describe their dual triangulations. Finally, we consider two useful primitives on the hyperbolic Voronoi diagrams for designing tailored user interfaces of an image catalog browsing application in the hyperbolic disk: (1) finding nearest neighbors, and (2) computing smallest enclosing balls.",
    "The problem of diffusive bond-dissociation in a double well potential under application of an external force is scrutinized. We compute the probability distribution of rupture forces and present a detailed discussion of the influence of finite rebinding probabilities on the dynamic force spectrum. In particular, we focus on barrier crossing upon extension, i.e. under linearly increased load, and upon relaxation starting from completely separated bonds. For large loading rates the rupture force and the rejoining force depend on the loading rate in the expected manner determined by the shape of the potential. For small loading rates the mean forces obtained from pull and relax modes approach each other as the system reaches equilibrium. We investigate the dependence of the rupture force distributions and mean rupture forces on external parameters like cantilever stiffness and influence of a soft linker. We find that depending on the implementation of a soft linker the equilibrium rupture force is either unaffected by the presence of the linker or changes in a predictable way with the linker-compliance. Additionally, we show that it is possible to extract the equilibrium constant of the on- and off-rates from the determination of the equilibrium rupture forces.",
    "We propose an invariant feature space for the detection of viscous dominated and turbulent regions (i.e., boundary layers and wakes). The developed methodology uses the principal invariants of the strain and rotational rate tensors as input to an unsupervised Machine Learning Gaussian mixture model. The selected feature space is independent of the coordinate frame used to generate the processed data, as it relies on the principal invariants of strain and rotational rate, which are Galilean invariants. This methodology allows us to identify two distinct flow regions: a viscous dominated, rotational region (boundary layer and wake region) and an inviscid, irrotational region (outer flow region). We test the methodology on a laminar and a turbulent (using Large Eddy Simulation) case for flows past a circular cylinder at $Re=40$ and $Re=3900$. The simulations have been conducted using a high-order nodal Discontinuous Galerkin Spectral Element Method (DGSEM). The results obtained are analysed to show that Gaussian mixture clustering provides an effective identification method of viscous dominated and rotational regions in the flow. We also include comparisons with traditional sensors to show that the proposed clustering does not depend on the selection of an arbitrary threshold, as required when using traditional sensors.",
    "Engineered quantum systems allow us to observe phenomena that are not easily accessible naturally. The LEGO-like nature of superconducting circuits makes them particularly suited for building and coupling artificial atoms. Here, we introduce an artificial molecule, composed of two strongly coupled fluxonium atoms, which possesses a tunable magnetic moment. Using an applied external flux, one can tune the molecule between two regimes: one in which the ground-excited state manifold has a magnetic dipole moment and one in which the ground-excited state manifold has only a magnetic quadrupole moment. By varying the applied external flux, we find the coherence of the molecule to be limited by local flux noise. The ability to engineer and control artificial molecules paves the way for building more complex circuits for protected qubits and quantum simulation.",
    "We describe a method for time-critical decision making involving sequential tasks and stochastic processes. The method employs several iterative refinement routines for solving different aspects of the decision making problem. This paper concentrates on the meta-level control problem of deliberation scheduling, allocating computational resources to these routines. We provide different models corresponding to optimization problems that capture the different circumstances and computational strategies for decision making under time constraints. We consider precursor models in which all decision making is performed prior to execution and recurrent models in which decision making is performed in parallel with execution, accounting for the states observed during execution and anticipating future states. We describe algorithms for precursor and recurrent models and provide the results of our empirical investigations to date.",
    "The role model strategy is introduced as a method for designing an estimator by approaching the output of a superior estimator that has better input observations. This strategy is shown to yield the optimal Bayesian estimator when a Markov condition is fulfilled. Two examples involving simple channels are given to illustrate its use. The strategy is combined with time averaging to construct a statistical model by numerically solving a convex program. The role model strategy was developed in the context of low complexity decoder design for iterative decoding. Potential applications outside the field of communications are discussed.",
    "Computer vision systems currently lack the ability to reliably recognize artistically rendered objects, especially when such data is limited. In this paper, we propose a method for recognizing objects in artistic modalities (such as paintings, cartoons, or sketches), without requiring any labeled data from those modalities. Our method explicitly accounts for stylistic domain shifts between and within domains. To do so, we introduce a complementary training modality constructed to be similar in artistic style to the target domain, and enforce that the network learns features that are invariant between the two training modalities. We show how such artificial labeled source domains can be generated automatically through the use of style transfer techniques, using diverse target images to represent the style in the target domain. Unlike existing methods which require a large amount of unlabeled target data, our method can work with as few as ten unlabeled images. We evaluate it on a number of cross-domain object and scene classification tasks and on a new dataset we release. Our experiments show that our approach, though conceptually simple, significantly improves the accuracy that existing domain adaptation techniques obtain for artistic object recognition.",
    "This paper studies the large time behavior of solutions to semi-linear Cauchy problems with quadratic nonlinearity in gradients. The Cauchy problem considered has a general state space and may degenerate on the boundary of the state space. Two types of large time behavior are obtained: i) pointwise convergence of the solution and its gradient; ii) convergence of solutions to associated backward stochastic differential equations. When the state space is R^d or the space of positive definite matrices, both types of convergence are obtained under growth conditions on model coefficients. These large time convergence results have direct applications in risk sensitive control and long term portfolio choice problems.",
    "The decaying vacuum model (DV), treating dark energy as a varying vacuum, has been studied well recently. The vacuum energy decays linearly with the Hubble parameter in the late-times, $\\rho_\\Lambda(t) \\propto H(t)$, and produces the additional matter component. We constrain the parameters of the DV model using the recent data-sets from supernovae, gamma-ray bursts, baryon acoustic oscillations, CMB, the Hubble rate and x-rays in galaxy clusters. It is found that the best fit of matter density contrast $\\Omega_m$ in the DV model is much lager than that in $\\Lambda$CDM model. We give the confidence contours in the $\\Omega_m-h$ plane up to $3\\sigma$ confidence level. Besides, the normalized likelihoods of $\\Omega_m$ and $h$ are presented, respectively. %",
    "Perpendicular MgO-based Magnetic Tunnel Junctions are optimal candidates as building block of Spin Transfer Torque (STT) magnetoresistive memories. However, up to now, the only STT is not enough to achieve switching current density below 106 A/cm2. A recent work [Wang et al., Nature Mater., vol. 11, pp 64-68, Jan. 2012] has experimentally demonstrated the possibility to perform magnetization switching assisted by an electric-field at ultra-low current density. Theoretically, this switching has been studied by using a macrospin approach only. Here, we show a full micromagnetic study. We found that the switching occurs via a complex nucleation process including the nucleation of magnetic vortexes.",
    "We present a generalisation of Rosenblatt's traditional perceptron learning algorithm to the class of proximal activation functions and demonstrate how this generalisation can be interpreted as an incremental gradient method applied to a novel energy function. This novel energy function is based on a generalised Bregman distance, for which the gradient with respect to the weights and biases does not require the differentiation of the activation function. The interpretation as an energy minimisation algorithm paves the way for many new algorithms, of which we explore a novel variant of the iterative soft-thresholding algorithm for the learning of sparse perceptrons.",
    "The radiation force exerted on an object by an acoustic wave is a widely studied phenomenon since the early work of Rayleigh, Langevin and Brillouin and has led in the last decade to tremendous developments for acoustic micromanipulation. Despite extensive work on this phenomenon, the expressions of the acoustic radiation force applied on a particle have so far been derived only for a steady particle, hence neglecting the effect of its displacement on the radiated wave. In this work we study the acoustic radiation force exerted on a monopolar source translating at a constant velocity small compared to the sound speed. We demonstrate that the asymmetry of the emitted field resulting from Doppler effect induces a radiation force on the source opposite to its motion.",
    "Modelling the base of the solar convective envelope is a tedious problem. Since the first rotation inversions, solar modellers are confronted with the fact that a region of very limited extent has an enormous physical impact on the Sun. Indeed, it is the transition region from differential to solid body rotation, the tachocline, which furthermore is influenced by turbulence and is also supposed to be the seat of the solar magnetic dynamo. Moreover, solar models show significant disagreement with the sound speed profile in this region. In this paper, we show how helioseismology can provide further constraints on this region by carrying out an inversion of the Ledoux discriminant. We compare these inversions for Standard Solar Models built using various opacity tables and chemical abundances and discuss the origins of the discrepancies between Solar Models and the Sun.",
    "There is a clear desire to model and comprehend human behavior. Trends in research covering this topic show a clear assumption that many view human reasoning as the presupposed standard in artificial reasoning. As such, topics such as game theory, theory of mind, machine learning, etc. all integrate concepts which are assumed components of human reasoning. These serve as techniques to attempt to both replicate and understand the behaviors of humans. In addition, next generation autonomous and adaptive systems will largely include AI agents and humans working together as teams. To make this possible, autonomous agents will require the ability to embed practical models of human behavior, which allow them not only to replicate human models as a technique to \"learn\", but to to understand the actions of users and anticipate their behavior, so as to truly operate in symbiosis with them. The main objective of this paper it to provide a succinct yet systematic review of the most important approaches in two areas dealing with quantitative models of human behaviors. Specifically, we focus on (i) techniques which learn a model or policy of behavior through exploration and feedback, such as Reinforcement Learning, and (ii) directly model mechanisms of human reasoning, such as beliefs and bias, without going necessarily learning via trial-and-error.",
    "Breaking down botnets have always been a big challenge. The robustness of C&C channels is increased, and the detection of botmaster is harder in P2P botnets. In this paper, we propose a probabilistic method to reconstruct the topologies of the C&C channel for P2P botnets. Due to the geographic dispersion of P2P botnet members, it is not possible to supervise all members, and there does not exist all necessary data for applying other graph reconstruction methods. So far, no general method has been introduced to reconstruct C&C channel topology for all type of P2P botnet. In our method, the probability of connections between bots is estimated by using the inaccurate receiving times of several cascades, network model parameters of C&C channel, and end-to-end delay distribution of the Internet. The receiving times can be collected by observing the external reaction of bots to commands. The results of our simulations show that more than 90% of the edges in a 1000-member network with node degree mean 50, have been accurately estimated by collecting the inaccurate receiving times of 22 cascades. In case the receiving times of just half of the bots are collected, this accuracy of estimation is obtained by using 95 cascades.",
    "In grand unified theories (GUT), non-universal boundary conditions for the gaugino masses may arise at the unification scale, and affect the observability of the neutral MSSM Higgs bosons (h/H/A) at the LHC. The implications of such non-universal gaugino masses are investigated for the Higgs boson production in the SUSY cascade decay chain gluino --> squark quark, squark --> neutralino_2 quark, neutralino_2 --> neutralino_1 h/H/A, h/H/A --> b b-bar produced in pp interactions. In the singlet representation with universal gaugino masses only the light Higgs boson can be produced in this cascade with the parameter region of interest for us, while with non-universal gaugino masses heavy neutral MSSM Higgs boson production may dominate. The allowed parameter space in the light of the WMAP constraints on the cold dark matter relic density is investigated in the above scenarios for gaugino mass parameters. We also demonstrate that combination of representations can give the required amount of dark matter in any point of the parameter space. In the non-universal case we show that heavy Higgs bosons can be detected in the studied cascade in parameter regions with the WMAP preferred neutralino relic density.",
    "Subwavelength modulators play an indispensable role in integrated photonic-electronic circuits. Due to weak light-matter interactions, it is always a challenge to develop a modulator with a nanometer scale footprint, low switching energy, low insertion loss and large modulation depth. In this paper, we propose the design of a vanadium dioxide dual-mode plasmonic waveguide electroabsorption modulator using a metal-insulator-VO$_2$-insulator-metal (MIVIM) waveguide platform. By varying the index of vanadium dioxide, the modulator can route plasmonic waves through the low-loss dielectric insulator layer during the \"on\" state and high-loss VO$_2$ layer during the \"off\" state, thereby significantly reducing the insertion loss while maintaining a large modulation depth. This ultracompact waveguide modulator, for example, can achieve a large modulation depth of ~10dB with an active size of only 200x50x220nm$^3$ (or ~{\\lambda}$^3$/1700), requiring a drive-voltage of ~4.6V. This high performance plasmonic modulator could potentially be one of the keys towards fully-integrated plasmonic nanocircuits in the next-generation chip technology.",
    "As a car becomes more connected, a countermeasure against automobile theft has become a significant task in the real world. To respond to automobile theft, data mining, biometrics, and additional authentication methods are proposed. Among current countermeasures, data mining method is one of the efficient ways to capture the owner driver's unique characteristics. To identify the owner driver from thieves, previous works applied various algorithms toward driving data. Such data mining methods utilized supervised learning, thus required labeled data set. However, it is unrealistic to gather and apply the thief's driving pattern. To overcome this problem, we propose driver identification method with GAN. GAN has merit to build identification model by learning the owner driver's data only. We trained GAN only with owner driver's data and used trained discriminator to identify the owner driver. From actual driving data, we evaluated our identification model recognizes the owner driver well. By ensembling various driver authentication methods with the proposed model, we expect industry can develop automobile theft countermeasures available in the real world.",
    "Slow oscillations (SlO) of magnetoresistance is a convenient tool to measure electronic structure parameters in quasi-two-dimensional metals. We study the possibility to apply this method to multi-band conductors, e.g. to iron-based high-temperature superconducting materials. We show that SlO can be used to measure the interlayer transfer integral in multi-band conductors similar to single-band metals. In addition, the SlO allow to measure and compare the effective masses or the electron scattering rates in various bands.",
    "Recent progress in the field of precision calculations for Standard Model processes at the LHC is reviewed, highlighting examples of weak gauge-boson and Higgs-boson production, as discussed at the 27th Rencontres de Blois, 2015.",
    "This paper proposes a speech emotion recognition method based on speech features and speech transcriptions (text). Speech features such as Spectrogram and Mel-frequency Cepstral Coefficients (MFCC) help retain emotion-related low-level characteristics in speech whereas text helps capture semantic meaning, both of which help in different aspects of emotion detection. We experimented with several Deep Neural Network (DNN) architectures, which take in different combinations of speech features and text as inputs. The proposed network architectures achieve higher accuracies when compared to state-of-the-art methods on a benchmark dataset. The combined MFCC-Text Convolutional Neural Network (CNN) model proved to be the most accurate in recognizing emotions in IEMOCAP data.",
    "In this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efficiently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the benefit of variational semantic memory in boosting few-shot recognition.",
    "The relativistic four-quark equations with the open-charm and the open-strange are found in the framework of coupled-channel formalism. The dynamical mixing of the meson-meson states with the four-quark states is considered. The four-quark amplitudes including the quarks of four flavors (u, d, s, c) are constructed. The poles of these amplitudes determine the masses of tetraquarks. The mass values of the tetraquarks with the spin-parity JP=1-,2- are calculated.",
    "The Fisher Matrix is the backbone of modern cosmological forecasting. We describe the Fisher4Cast software: a general-purpose, easy-to-use, Fisher Matrix framework. It is open source, rigorously designed and tested and includes a Graphical User Interface (GUI) with automated LATEX file creation capability and point-and-click Fisher ellipse generation. Fisher4Cast was designed for ease of extension and, although written in Matlab, is easily portable to open-source alternatives such as Octave and Scilab. Here we use Fisher4Cast to present new 3-D and 4-D visualisations of the forecasting landscape and to investigate the effects of growth and curvature on future cosmological surveys. Early releases have been available at http://www.cosmology.org.za since May 2008 with 750 downloads in the first year. Version 2.2 is made public with this paper and includes a Quick Start guide and the code used to produce the figures in this paper, in the hope that it will be useful to the cosmology and wider scientific communities.",
    "The representation of knowledge based on first-order logic captures the richness of natural language and supports multiple probabilistic inference models. Although symbolic representation enables quantitative reasoning with statistical probability, it is difficult to utilize with machine learning models as they perform numerical operations. In contrast, knowledge embedding (i.e., high-dimensional and continuous vectors) is a feasible approach to complex reasoning that can not only retain the semantic information of knowledge but also establish the quantifiable relationship among them. In this paper, we propose recursive neural knowledge network (RNKN), which combines medical knowledge based on first-order logic with recursive neural network for multi-disease diagnosis. After RNKN is efficiently trained from manually annotated Chinese Electronic Medical Records (CEMRs), diagnosis-oriented knowledge embeddings and weight matrixes are learned. Experimental results verify that the diagnostic accuracy of RNKN is superior to that of some classical machine learning models and Markov logic network (MLN). The results also demonstrate that the more explicit the evidence extracted from CEMRs is, the better is the performance achieved. RNKN gradually exhibits the interpretation of knowledge embeddings as the number of training epochs increases.",
    "Several solenoids are usually installed in electron cooler device to guide the motion of the electron beam in the cooler. However, the solenoids also have influence to the ion beam in the cooler storage ring. The transverse motion of the ion beam in storage ring will become coupled, if the solenoids installed in the electron cooler are not compensated perfectly. In this paper, the coupled transverse motion due to the uncompensated cooler's solenoids of CSRm (The main storage ring in the IMP, Lan Zhou, China) is studied, and the coupled beam envelopes are calculated by a new method.",
    "A near-infrared excess is detected at the white dwarf PHL5038 in UKIDSS photometry, consistent with the presence of a cool, substellar companion. We have obtained H- and K-grism spectra and images of PHL5038 using NIRI on Gemini North. The target is spatially and spectrally resolved into two components; an 8000K DA white dwarf, and a likely L8 brown dwarf companion, separated by 0.94\". The spectral type of the secondary was determined using standard spectral indices for late L and T dwarfs. The projected orbital separation of the binary is 55AU, and so it becomes only the second known wide WD+dL binary to be found after GD165AB. This object could potentially be used as a benchmark for testing substellar evolutionary models at intermediate to older ages.",
    "We investigate the impact of dynamical streams and substructure on estimates of the local escape speed and total mass of Milky Way-mass galaxies from modelling the high velocity tail of local halo stars. We use a suite of high-resolution, magneto-hydrodynamical cosmological zoom-in simulations, which resolve phase space substructure in local volumes around solar-like positions. We show that phase space structure varies significantly between positions in individual galaxies and across the suite. Substructure populates the high velocity tail unevenly and leads to discrepancies in the mass estimates. We show that a combination of streams, sample noise and truncation of the high velocity tail below the escape speed leads to a distribution of mass estimates with a median that falls below the true value by $\\sim 20 \\%$, and a spread of a factor of 2 across the suite. Correcting for these biases, we derive a revised value for the Milky Way mass presented in Deason et al. of $1.29 ^{+0.37}_{-0.47} \\times 10^{12}$ $\\rm M_{\\odot}$.",
    "Using no conventional measurements in position space, information extraction rates exceeding one bit per photon are achieved by employing high-dimensional correlated orbital angular momentum (OAM) states for object recognition. The correlations are shown to be insensitive to axial rotation of the target object: the information structure of an object's joint OAM coincidence spectrum is unchanged even when the object undergoes random rotations between each measurement. Additionally, OAM correlations alone are shown to be sufficient for full image reconstruction of complex, off-axis objects, and novel object symmetries are observed in the phases of OAM-object interaction transition amplitudes. Variations in mutual information rates, due to off-axis translation in the beam field, are studied, and it is shown that object symmetry signatures and information rates are independent of environmental factors sufficiently far from the beam center. The results motivate dynamic scanning applications in contexts where symmetry and small numbers of noninvasive measurements are desired.",
    "During Parker Solar Probe's first two orbits there are widespread observations of rapid magnetic field reversals known as switchbacks. These switchbacks are extensively found in the near-Sun solar wind, appear to occur in patches, and have possible links to various phenomena such as magnetic reconnection near the solar surface. As switchbacks are associated with faster plasma flows, we questioned whether they are hotter than the background plasma and whether the microphysics inside a switchback is different to its surroundings. We have studied the reduced distribution functions from the Solar Probe Cup instrument and considered time periods with markedly large angular deflections, to compare parallel temperatures inside and outside switchbacks. We have shown that the reduced distribution functions inside switchbacks are consistent with a rigid phase space rotation of the background plasma. As such, we conclude that the proton core parallel temperature is the same inside and outside of switchbacks, implying that a T-V relationship does not hold for the proton core parallel temperature inside magnetic field switchbacks. We further conclude that switchbacks are consistent with Alfv\\'enic pulses travelling along open magnetic field lines. The origin of these pulses, however, remains unknown. We also found that there is no obvious link between radial Poynting flux and kinetic energy enhancements suggesting that the radial Poynting flux is not important for the dynamics of switchbacks.",
    "Under the Nainital-Cape Survey, eight $\\delta\\,$Scuti type pulsators have been discovered with the pulsation periods in the range of several minutes to few hours. In order to understand these observed pulsational variabilities, we have performed non-adiabatic linear stability analyses in models of these stars having mass in the range of 1 to 3 M$_{\\odot}$. Several low order p-modes are found to be unstable where the pulsation periods associated with these unstable modes are in good agreement with the observed periods. Particularly for HD$\\,$118660, HD$\\,$113878, HD$\\,$102480, HD$\\,$98851, and HD$\\,$25515, we demonstrate that the observed variabilities can be explained with the low order radial p-mode pulsations.",
    "A new perspective on the classical mechanical formulation of particle trajectories in lorentz-violating theories is presented. Using the extended hamiltonian formalism, a Legendre Transformation between the associated covariant Lagrangian and Hamiltonian varieties is constructed. This approach enables calculation of trajectories using hamilton's equations in momentum space and the Euler-Lagrange equations in velocity space away from certain singular points that arise in the theory. Singular points are naturally de-singularized by requiring the trajectories to be smooth functions of both velocity and momentum variables. In addition, it is possible to identify specific sheets of the dispersion relations that correspond to specific solutions for the lagrangian. Examples corresponding to bipartite Finsler functions are computed in detail. A direct connection between the lagrangians and the field-theoretic solutions to the Dirac equation is also established for a special case.",
    "Spectrum management has been identified as a crucial step towards enabling the technology of a cognitive radio network (CRN). Most of the current works dealing with spectrum management in the CRN focus on a single task of the problem, e.g., spectrum sensing, spectrum decision, spectrum sharing or spectrum mobility. In this two-part paper, we argue that for certain network configurations, jointly performing several tasks of the spectrum management improves the spectrum efficiency. Specifically, our aim is to study the uplink resource management problem in a CRN where there exist multiple cognitive users (CUs) and access points (APs). The CUs, in order to maximize their uplink transmission rates, have to associate to a suitable AP (spectrum decision), and to share the channels used by this AP with other CUs (spectrum sharing). These tasks are clearly interdependent, and the problem of how they should be carried out efficiently and in a distributed manner is still open in the literature.",
    "A simple, exactly solvable statistical model is presented for the description of baryonic matter in the thermodynamic conditions associated to the evolution of core-collapsing supernova. It is shown that the model presents a first order phase transition in the grandcanonical ensemble which is not observed in the canonical ensemble. Similar to other model systems studied in condensed matter physics, this ensemble in-equivalence is accompanied by negative susceptibility and discontinuities in the intensive observables conjugated to the order parameter. This peculiar behavior originates from the fact that baryonic matter is subject to attractive short range strong forces as well as repulsive long range electromagnetic interactions, partially screened by a background of electrons. As such, it is expected in any theoretical treatment of nuclear matter in the stellar environment. Consequences for the phenomenology of supernova dynamics are drawn.",
    "To meet requirements of high performance THz-FEL (Free Electron Laser), a compact scheme of FEL injector was proposed. Thermionic cathode was chosen to emit electrons instead of photo-cathode with complex structure and high cost. The effective bunch charge was improved to ~200pC by adopting enhanced EC-ITC (External Cathode Independently Tunable Cells) RF gun to extract micro-bunches, and back bombardment effects were almost eliminated as well. Constant gradient accelerator structures were designed to improve energy to ~14MeV, while focusing system was applied for emittance suppressing and bunch state maintenance. Physical design and beam dynamics of key components for FEL injector were analyzed. Furthermore, start-to-end simulations with multi-pulses were performed by using homemade MATLAB and Parmela. The results show that continual high brightness electron bunches with low energy spread and emittance could be obtained stably.",
    "Several claims have been made of anomalies in the large-angle properties of the cosmic microwave background anisotropy as measured by WMAP. In most cases, the statistical significance of these anomalies is hard or even impossible to assess, due to the fact that the statistics used to quantify the anomalies were chosen a posteriori. On the other hand, the possibility of detecting new physics on the largest observable scales is so exciting that, in my opinion, it is worthwhile to examine the claims carefully. I will focus on three particular claims: the lack of large-angle power, the north-south power asymmetry, and multipole alignments. In all cases, the problem of a posteriori statistics can best be solved by finding a new data set that probes similar physical scales to the large-angle CMB. This is a difficult task, but there are some possible routes to achieving it.",
    "Multi-photon states can be produced in multiple parametric down conversion (PDC) processes. The nonlinear crystal in such a case is pumped with high power. In theory, the more populated these states are, the deeper is the conflict with local realistic description. However, the interference contrast in multi-photon PDC experiments can be quite low for high pumping. We show how the contrast can be improved. The idea employs currently accessible optical devices, the multiport beam splitters. They are capable of splitting the incoming light in one input mode to $M$ output modes. Our scheme works as a POVM filter. It may provide a feasible CHSH-Bell inequality test, and thus can be useful in e.g. schemes reducing communication complexity.",
    "The spectrum of exponents of the transfer matrix provides the localization lengths of Anderson's model for a particle in a lattice with disordered potential. I show that a duality identity for determinants and Jensen's identity for subharmonic functions, give a formula for the spectrum in terms of eigenvalues of the Hamiltonian with non-Hermitian boundary conditions. The formula is exact; it involves an average over a Bloch phase, rather than disorder. A preliminary investigation of non-Hermitian spectra of Anderson's model in D=1,2 and on the smallest exponent is presented.",
    "In this article, we improve extreme learning machines for regression tasks using a graph signal processing based regularization. We assume that the target signal for prediction or regression is a graph signal. With this assumption, we use the regularization to enforce that the output of an extreme learning machine is smooth over a given graph. Simulation results with real data confirm that such regularization helps significantly when the available training data is limited in size and corrupted by noise.",
    "To properly describe heating in weakly collisional turbulent plasmas such as the solar wind, inter-particle collisions should be taken into account. Collisions can convert ordered energy into heat by means of irreversible relaxation towards the thermal equilibrium. Recently, Pezzi et al. (Phys. Rev. Lett., vol. 116, 2016, p. 145001) showed that the plasma collisionality is enhanced by the presence of fine structures in velocity space. Here, the analysis is extended by directly comparing the effects of the fully nonlinear Landau operator and a linearized Landau operator. By focusing on the relaxation towards the equilibrium of an out of equilibrium distribution function in a homogeneous force-free plasma, here it is pointed out that it is significant to retain nonlinearities in the collisional operator to quantify the importance of collisional effects. Although the presence of several characteristic times associated with the dissipation of different phase space structures is recovered in both the cases of the nonlinear and the linearized operators, the influence of these times is different in the two cases. In the linearized operator case, the recovered characteristic times are systematically larger than in the fully nonlinear operator case, this suggesting that fine velocity structures are dissipated slower if nonlinearities are neglected in the collisional operator.",
    "In this research work, we have demonstrated the application of Mask-RCNN (Regional Convolutional Neural Network), a deep-learning algorithm for computer vision and specifically object detection, to semiconductor defect inspection domain. Stochastic defect detection and classification during semiconductor manufacturing has grown to be a challenging task as we continuously shrink circuit pattern dimensions (e.g., for pitches less than 32 nm). Defect inspection and analysis by state-of-the-art optical and e-beam inspection tools is generally driven by some rule-based techniques, which in turn often causes to misclassification and thereby necessitating human expert intervention. In this work, we have revisited and extended our previous deep learning-based defect classification and detection method towards improved defect instance segmentation in SEM images with precise extent of defect as well as generating a mask for each defect category/instance. This also enables to extract and calibrate each segmented mask and quantify the pixels that make up each mask, which in turn enables us to count each categorical defect instances as well as to calculate the surface area in terms of pixels. We are aiming at detecting and segmenting different types of inter-class stochastic defect patterns such as bridge, break, and line collapse as well as to differentiate accurately between intra-class multi-categorical defect bridge scenarios (as thin/single/multi-line/horizontal/non-horizontal) for aggressive pitches as well as thin resists (High NA applications). Our proposed approach demonstrates its effectiveness both quantitatively and qualitatively.",
    "We give a new bound on the parameter $\\lambda$ (number of common neighbors of a pair of adjacent vertices) in a distance-regular graph $G$, improving and generalizing bounds for strongly regular graphs by Spielman (1996) and Pyber (2014). The new bound is one of the ingredients of recent progress on the complexity of testing isomorphism of strongly regular graphs (Babai, Chen, Sun, Teng, Wilmes 2013). The proof is based on a clique geometry found by Metsch (1991) under certain constraints on the parameters. We also give a simplified proof of the following asymptotic consequence of Metsch's result: if $k\\mu = o(\\lambda^2)$ then each edge of $G$ belongs to a unique maximal clique of size asymptotically equal to $\\lambda$, and all other cliques have size $o(\\lambda)$. Here $k$ denotes the degree and $\\mu$ the number of common neighbors of a pair of vertices at distance 2. We point out that Metsch's cliques are \"asymptotically Delsarte\" when $k\\mu = o(\\lambda^2)$, so families of distance-regular graphs with parameters satisfying $k\\mu = o(\\lambda^2)$ are \"asymptotically Delsarte-geometric.\"",
    "Recent galaxy observations show that star formation activity changes depending on galactic environments. In order to understand the diversity of galactic-scale star formation, it is crucial to understand the formation and evolution of giant molecular clouds in an extreme environment. We focus on observational evidence that bars in strongly barred galaxies lack massive stars even though quantities of molecular gas are sufficient to form stars. In this paper, we present a hydrodynamical simulation of a strongly barred galaxy, using a stellar potential which is taken from observational results of NGC1300, and we compare cloud properties between different galactic environments: bar, bar-end and spiral arms. We find that the mean of cloud's virial parameter is ~1 and that there is no environmental dependence, indicating that the gravitationally-bound state of a cloud is not behind the observational evidence of the lack of massive stars in strong bars. Instead, we focus on cloud-cloud collisions, which have been proposed as a triggering mechanism for massive star formation. We find that the collision speed in the bar is faster than those in the other regions. We examine the collision frequency using clouds' kinematics and conclude that the fast collisions in the bar could originate from random-like motion of clouds due to elliptical gas orbits shifted by the bar potential. These results suggest that the observed regions of lack of active star-formation in the strong bar originate from the fast cloud-cloud collisions, which are inefficient in forming massive stars, due to the galactic-scale violent gas motion.",
    "Context: The mass-metallicity relationship (MMR) of star-forming galaxies is well-established, however there is still some disagreement with respect to its exact shape and its possible dependence on other observables. Aims: We measure the MMR in the Galaxy And Mass Assembly (GAMA) survey. We compare our measured MMR to that measured in the Sloan Digital Sky Survey (SDSS) and study the dependence of the MMR on various selection criteria to identify potential causes for disparities seen in the literature. Methods: We use strong emission line ratio diagnostics to derive oxygen abundances. We then apply a range of selection criteria for the minimum signal-to-noise in various emission lines, as well as the apparent and absolute magnitude to study variations in the inferred MMR. Results: The shape and position of the MMR can differ significantly depending on the metallicity calibration and selection used. After selecting a robust metallicity calibration amongst those tested, we find that the mass-metallicity relation for redshifts 0.061< z<0.35 in GAMA is in reasonable agreement with that found in the SDSS despite the difference in the luminosity range probed. Conclusions: In view of the significant variations of the MMR brought about by reasonable changes in the sample selection criteria and method, we recommend that care be taken when comparing the MMR from different surveys and studies directly. We also conclude that there could be a modest level of evolution over 0.06<z<0.35 within the GAMA sample.",
    "Based on thermodynamic considerations we derive a set of equations relating the seepage velocities of the fluid components in immiscible and incompressible two-phase flow in porous media. They necessitate the introduction of a new velocity function, the co-moving velocity. This velocity function is a characteristic of the porous medium. Together with a constitutive relation between the velocities and the driving forces, such as the pressure gradient, these equations form a closed set. We solve four versions of the capillary tube model analytically using this theory. We test the theory numerically on a network model.",
    "Nature's spectacular inventiveness, reflected in the enormous diversity of form and function displayed by the biosphere, is a feature of life that distinguishes living most strongly from nonliving. It is, therefore, not surprising that this aspect of life should become a central focus of artificial life. We have known since Darwin that the diversity is produced dynamically, through the process of evolution; this has led life's creative productivity to be called Open-Ended Evolution (OEE) in the field. This article introduces the second of two special issues on current research in OEE and provides an overview of the contents of both special issues. Most of the work was presented at a workshop on open-ended evolution that was held as a part of the 2018 Conference on Artificial Life in Tokyo, and much of it had antecedents in two previous workshops on open-ended evolution at artificial life conferences in Cancun and York. We present a simplified categorization of OEE and summarize progress in the field as represented by the articles in this special issue.",
    "The properties of MgO/Ag(001) ultrathin films with substitutional Mg atoms in the interface metal layer have been investigated by means of Auger electron diffraction experiments, ultraviolet photoemission spectroscopy, and density functional theory (DFT) calculations. Exploiting the layer-by-layer resolution of the Mg KL_23 L_23 Auger spectra and using multiple scattering calculations, we first determine the interlayer distances as well as the morphological parameters of the MgO/Ag(001) system with and without Mg atoms incorporated at the interface. We find that the Mg atoms incorporation drives a strong distortion of the interface layers and that its impact on the metal/oxide electronic structure is an important reduction of the work function (0.5 eV) related to band-offset variations at the interface. These experimental observations are in very good agreement with our DFT calculations which reproduce the induced lattice distortion and which reveal (through a Bader analysis) that the increase of the interface Mg concentration results in an electron transfer from Mg to Ag atoms of the metallic interface layer. Although the local lattice distortion appears as a consequence of the attractive (repulsive) Coulomb interaction between O2- ions of the MgO interface layer and the nearest positively (negatively) charged Mg (Ag) neighbors of the metallic interface layer, its effect on the work function reduction is only limited. Finally, an analysis of the induced work function changes in terms of charge transfer, rumpling, and electrostatic compression contributions is attempted and reveals that the metal/oxide work function changes induced by interface Mg atoms incorporation are essentially driven by the increase of the electrostatic compression effect.",
    "The need to monitor industrial processes, detecting changes in process parameters in order to promptly correct problems that may arise, generates a particular area of interest. This is particularly critical and complex when the measured value falls below the sensitivity limits of the measuring system or below detection limits, causing much of their observations are incomplete. Such observations to be called incomplete observations or left censored data. With a high level of censorship, for example greater than 70%, the application of traditional methods for monitoring processes is not appropriate. It is required to use appropriate data analysis statistical techniques, to assess the actual state of the process at any time. This paper proposes a way to estimate process parameters in such cases and presents the corresponding control chart, from an algorithm that is also presented.",
    "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",
    "We refine a stimulating study by Sarvotham et al. [2005] which highlighted the influence of peak transmission rate on network burstiness. From TCP packet headers, we amalgamate packets into sessions where each session is characterized by a 5-tuple (S, D, R, Peak R, Initiation T)=(total payload, duration, average transmission rate, peak transmission rate, initiation time). After careful consideration, a new definition of peak rate is required. Unlike Sarvotham et al. [2005] who segmented sessions into two groups labelled alpha and beta, we segment into 10 sessions according to the empirical quantiles of the peak rate variable as a demonstration that the beta group is far from homogeneous. Our more refined segmentation reveals additional structure that is missed by segmentation into two groups. In each segment, we study the dependence structure of (S, D, R) and find that it varies across the groups. Furthermore, within each segment, session initiation times are well approximated by a Poisson process whereas this property does not hold for the data set taken as a whole. Therefore, we conclude that the peak rate level is important for understanding structure and for constructing accurate simulations of data in the wild. We outline a simple method of simulating network traffic based on our findings.",
    "The Brouwer fixed-point theorem in topology states that for any continuous mapping $f$ on a compact convex set into itself admits a fixed point, i.e., a point $x_0$ such that $f(x_0)=x_0$. Under certain conditions, this fixed point corresponds to the throat of a traversable wormhole, i.e., $b(r_0)=r_0$ for the shape function $b=b(r)$. The possible existence of wormholes can therefore be deduced from purely mathematical considerations without going beyond the existing physical requirements.",
    "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.",
    "As well known, the spectrum of a non-relativistic two-body system interacting by the Coulomb potential is the Balmer series $E_n=\\frac{\\alpha^2m}{4n^2}$ produced by the Schr\\\"odinger equation. In 1954, Wick and Cutkosky have found, in the Bethe-Salpeter equation framework, that for $\\alpha>\\frac{\\pi}{4}$ the relativistic effects result in new levels (in addition to the Balmer series). However, the physical nature of these new states remained unclear and therefore their existence was being questioned. We have recently shown that these extra states are dominated by the exchange (massless) particles, moving with speed of light. That's why they did not appear in the non-relativistic (Schr\\\"odinger) framework.",
    "We study the fundamental properties of the quantum f-relative entropy, where f(.) is an operator convex function. We give the equality conditions under monotonicity and joint convexity, and these conditions are more general than, since they hold for a class of operator convex functions, and different for f(t) = -ln(t) from, the previously known conditions. The quantum f-entropy is defined in terms of the quantum f-relative entropy and we study its properties giving the equality conditions in some cases. We then show that the f-generalizations of the Holevo information, the entanglement-assisted capacity, and the coherent information also satisfy the data processing inequality, and give the equality conditions for the f-coherent information.",
    "Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch. H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.",
    "We measure and analyze reflection spectra of directly coupled systems of waveguides and cavities. The observed Fano lines offer insight in the reflection and coupling processes. Very different from side-coupled systems, the observed Fano line shape is not caused by the termini of the waveguide, but the coupling process between the measurement device fiber and the waveguide. Our experimental results and analytical model show that the Fano parameter that describes the Fano line shape is very sensitive to the coupling condition. A movement of the fiber well below the Rayleigh range can lead to a drastic change of the Fano line shape.",
    "The strength and vertical distribution of atmospheric turbulence is a key factor determining the performance of optical and infrared telescopes, with and without adaptive optics. Yet, this remains challenging to measure. We describe a new technique using a sequence of short-exposure images of a star field, obtained with a small telescope. Differential motion between all pairs of star images is used to compute the structure functions of longitudinal and transverse wavefront tilt for a range of angular separations. These are compared with theoretical predictions of simple turbulence models by means of a Markov-Chain Monte-Carlo optimization. The method is able to estimate the turbulence profile in the lower atmosphere, the total and free-atmosphere seeing, and the outer scale. We present results of Monte-Carlo simulations used to verify the technique, and show some examples using data from the second AST3 telescope at Dome A in Antarctica.",
    "We define an n-plectic structure as a commutative and torsionless Lie Rinehart pair, together with a distinguished cocycle from its Chevalley-Eilenberg complex. This 'n-plectic cocycle' gives rise to an extension of the Chevalley-Eilenberg complex by so called symplectic tensors. The cohomology of this extension generalizes Hamiltonian functions and vector fields to tensors and cotensors in a range of degrees, up to certain coboundaries and has the structure of a Lie oo-algebra. Finally we show, that momentum maps appear in this context just as weak Lie oo-morphisms from an arbitrary Lie oo-algebra into the Lie oo-algebra of Hamiltonian (co)tensors.",
    "Amorphous solids or glasses are known to exhibit stretched-exponential decay over broad time intervals in several of their macroscopic observables: intermediate scattering function, dielectric relaxation modulus, time-elastic modulus etc. This behaviour is prominent especially near the glass transition. In this Letter we show, on the example of dielectric relaxation, that stretched-exponential relaxation is intimately related to the peculiar lattice dynamics of glasses. By reformulating the Lorentz model of dielectric matter in a more general form, we express the dielectric response as a function of the vibrational density of states (DOS) for a random assembly of spherical particles interacting harmonically with their nearest-neighbours. Surprisingly we find that near the glass transition for this system (which coincides with the Maxwell rigidity transition), the dielectric relaxation is perfectly consistent with stretched-exponential behaviour with Kohlrausch exponents $0.56 < \\beta < 0.65$, which is the range where exponents are measured in most experimental systems. Crucially, the root cause of stretched-exponential relaxation can be traced back to soft modes (boson-peak) in the DOS.",
    "To highlight the challenges of achieving representation disentanglement for text domain in an unsupervised setting, in this paper we select a representative set of successfully applied models from the image domain. We evaluate these models on 6 disentanglement metrics, as well as on downstream classification tasks and homotopy. To facilitate the evaluation, we propose two synthetic datasets with known generative factors. Our experiments highlight the existing gap in the text domain and illustrate that certain elements such as representation sparsity (as an inductive bias), or representation coupling with the decoder could impact disentanglement. To the best of our knowledge, our work is the first attempt on the intersection of unsupervised representation disentanglement and text, and provides the experimental framework and datasets for examining future developments in this direction.",
    "This paper proposes a hybrid quantum-classical algorithm to solve a fundamental power system problem called unit commitment (UC). The UC problem is decomposed into a quadratic subproblem, a quadratic unconstrained binary optimization (QUBO) subproblem, and an unconstrained quadratic subproblem. A classical optimization solver solves the first and third subproblems, while the QUBO subproblem is solved by a quantum algorithm called quantum approximate optimization algorithm (QAOA). The three subproblems are then coordinated iteratively using a three-block alternating direction method of multipliers algorithm. Using Qiskit on the IBM Q system as the simulation environment, simulation results demonstrate the validity of the proposed algorithm to solve the UC problem.",
    "It has also been suggested that the detection of a wealth of very low amplitude modes in Delta Sct stars was only a matter of signal--to--noise ratio. Access to this treasure, impossible from the ground, is one of the scientific aims of the space mission CoRoT, a space mission developed and operated by CNES. This work presents the results obtained on HD 50844: the 140,016 datapoints were analysed using independent approaches and several checks performed. A level of 10^{-5} mag was reached in the amplitude spectra of the CoRoT timeseries. The frequency analysis of the CoRoT timeseries revealed hundreds of terms in the frequency range 0--30 d^{-1}. All the cross--checks confirmed this new result. The initial guess that Delta Sct stars have a very rich frequency content is confirmed. The spectroscopic mode identification gives theoretical support since very high--degree modes (up to ell=14) are identified. We also prove that cancellation effects are not sufficient in removing the flux variations associated to these modes at the noise level of the CoRoT measurements. The ground--based observations indicate that HD 50844 is an evolved star that is slightly underabundant in heavy elements, located on the Terminal Age Main Sequence. Probably due to this unfavourable evolutionary status, no clear regular distribution is observed in the frequency set. The predominant term (f_1=6.92 d^{-1}) has been identified as the fundamental radial mode combining ground-based photometric and spectroscopic data. This work is also based on observations made with ESO telescopes under the ESO Large Programme LP178.D-0361 and on data collected at the Observatorio de Sierra Nevada, at the Observatorio Astronomico Nacional San Pedro Martir, and at the Piszkesteto Mountain Station of Konkoly Observatory.",
    "The article deals with observations of star-forming regions S231-S235 in 'quasi-thermal' lines of ammonia (NH$_3$), cyanoacetylene (HC$_3$N) and maser lines of methanol (CH$_3$OH) and water vapor (H$_2$O). S231-S235 regions is situated in the giant molecular cloud G174+2.5. We selected all massive molecular clumps in G174+2.5 using archive CO data. For the each clump we determined mass, size and CO column density. After that we performed observations of these clumps. We report about first detections of NH$_3$ and HC$_3$N lines toward the molecular clumps WB89 673 and WB89 668. This means that high-density gas is present there. Physical parameters of molecular gas in the clumps were estimated using the data on ammonia emission. We found that the gas temperature and the hydrogen number density are in the ranges 16-30 K and 2.8-7.2$\\times10^3$ cm$^{-3}$, respectively. The shock-tracing line of CH$_3$OH molecule at 36.2 GHz is newly detected toward WB89 673.",
    "We report the lowest frequency measurements of gamma-ray burst (GRB) 171205A with the upgraded Giant Metrewave Radio Telescope (uGMRT) covering a frequency range from 250--1450 MHz and a period of $4-937$ days. It is the first GRB afterglow detected at 250--500 MHz frequency range and the second brightest GRB detected with the uGMRT. Even though the GRB is observed for nearly 1000 days, there is no evidence of transition to non-relativistic regime. We also analyse the archival ${\\it Chandra}$ X-ray data on day $\\sim 70$ and day $\\sim 200$. We also find no evidence of a jet break from the analysis of combined data. We fit synchrotron afterglow emission arising from a relativistic, isotropic, self-similar deceleration as well as from a shock-breakout of wide-angle cocoon. Our data also allow us to discern the nature and the density of the circumburst medium. We find that the density profile deviates from a standard constant density medium and suggests that the GRB exploded in a stratified wind like medium. Our analysis shows that the lowest frequency measurements covering the absorbed part of the light curves are critical to unravel the GRB environment. Our data combined with other published measurements indicate that the radio afterglow has contribution from two components: a weak, possibly slightly off-axis jet and a surrounding wider cocoon, consistent with the results of Izzo et al. (2019). The cocoon emission likely dominates at early epochs, whereas the jet starts to dominate at later epochs, resulting in flatter radio lightcurves.",
    "The recently developed theory of quasi-Lie schemes is studied and applied to investigate several equations of Emden type and a scheme to deal with them and some of their generalisations is given. As a first result we obtain t-dependent constants of the motion for particular instances of Emden equations by means of some of their particular solutions. Previously known results are recovered from this new perspective. Finally some t-dependent constants of the motion for equations of Emden type satisfying certain conditions are recovered.",
    "We undertake the study of the charged Higgs bosons predicted by the model with gauge symmetry $SU(3)_c \\otimes SU(3)_L \\otimes U(1)_X$. By considering Yukawa mixing couplings between small ($\\sim$ GeV) and large ($\\sim$ TeV) scales, we show that the hypercharge-one $H_1^{\\pm}$ and hypercharge-two $H_2^{\\pm}$ Higgs bosons predicted by the model, can be simultaneously produced in $pp$ collisions at different production rates. At low energy, the $H_1^{\\pm}$ bosons exhibit the same properties as the charged Higgs bosons from a two Higgs doublet model (2HDM), while $H_2^{\\pm}$ are additional like-charged Higgs bosons from the underlying 3-3-1 model. Thus, the identification of multiple like-charged Higgs boson resonances may test the compatibility of theoretical models with experimental data. We study $H_{1,2}^{\\pm}$ pair and associated $tbH_{1,2}^{\\pm}$ productions at CERN LHC collider. In particular, we obtain that pair production can be as large as the single production in gluon-gluon collisions due to the interchange of a heavy neutral $Z'$ gauge boson predicted by the model. By considering decays to leptons $H_{1,2}^{\\pm} \\rightarrow \\tau \\nu_{\\tau}$, we obtain scenarios where small peaks of $H_{2}^{\\pm}$-boson events in transverse mass distributions can be identified over the $H_{1}^{\\pm}$ background.",
    "Isospin breaking in the $K_{\\ell 4}$ form factors induced by the difference between charged and neutral pion masses is discussed within a framework built on suitably subtracted dispersion representations. The $K_{\\ell 4}$ form factors are constructed in an iterative way up to two loops in the low-energy expansion by implementing analyticity, crossing, and unitarity due to two-meson intermediate states. Analytical expressions for the phases of the two-loop form factors of the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ channel are presented, allowing one to connect the difference of form-factor phase shifts measured experimentally (out of the isospin limit) and the difference of $S$- and $P$-wave $\\pi\\pi$ phase shifts studied theoretically (in the isospin limit). The dependence with respect to the two $S$-wave scattering lengths $a_0^0$ and $a_0^2$ in the isospin limit is worked out in a general way, in contrast to previous analyses based on one-loop chiral perturbation theory. The results on the phases of the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ form factors obtained by the NA48/2 collaboration at the CERN SPS are reanalysed including isospin-breaking correction to extract values for the scattering lengths $a_0^0$ and $a_0^2$.",
    "In this paper we use and extend the results present in \\cite{1,2,3,4} and in particular in \\cite{4} to obtain a statistical description of the cosmological constant in a cosmological de Sitter universe in terms of massless excitations with Planckian effects. First of all, we show that at a classical level, the cosmological constant $\\Lambda>0$ can be obtained only for $T\\rightarrow 0$. Similarly to the black hole case, when quantum effects are taken into account, a representation for $\\Lambda$ is possible in terms of massless excitations, provided that quantum corrections to the Misner-Sharp mass are considered. Moreover, thanks to quantum fluctuations, an effective cosmological constant arises depending on the physical scale under consideration, thus representing a possible solution to the cosmological constant problem without introducing a quintessence field. The smalness of the actual value for $\\Lambda$ can be due to the existence of a quantum decoherence scale above the Planck length such that the spacetime evolves as a pure de Sitter universe with a small averaged cosmological constant frozen in the lowest energy state.",
    "We investigate zero and finite temperature properties of the one-dimensional spin-glass model for vector spins in the limit of an infinite number m of spin components where the interactions decay with a power, \\sigma, of the distance. A diluted version of this model is also studied, but found to deviate significantly from the fully connected model. At zero temperature, defect energies are determined from the difference in ground-state energies between systems with periodic and antiperiodic boundary conditions to determine the dependence of the defect-energy exponent \\theta on \\sigma. A good fit to this dependence is \\theta =3/4-\\sigma. This implies that the upper critical value of \\sigma is 3/4, corresponding to the lower critical dimension in the d-dimensional short-range version of the model. For finite temperatures the large m saddle-point equations are solved self-consistently which gives access to the correlation function, the order parameter and the spin-glass susceptibility. Special attention is paid to the different forms of finite-size scaling effects below and above the lower critical value, \\sigma =5/8, which corresponds to the upper critical dimension 8 of the hypercubic short-range model.",
    "The members of the scarce category of Of^+ supergiants present properties that are intermediate between regular O-stars and Wolf-Rayet (WR) stars. Significant similarities between these transitional stars and WN-type objects are now clearly established, at least in the visible and near-infrared domains, pointing to common stellar wind properties. In this study, we report on the first dedicated X-ray observations of HD16691 (O4If^+) and HD14947 (O5f^+), revealing a soft thermal spectrum in agreement with the expected X-ray emission from a single O-type star. However, the X-ray luminosity of our targets is slightly lower than expected for single O-type stars, suggesting that the particular properties of their stellar wind has also a significant impact on the X-ray emission of these objects on the way to the WN category. We argue that the X-ray under-luminosity of HD16691 and HD14947 may be interpreted as the signature in X-rays of the intermediate stage between O and WR stars, as a consequence of enhanced wind density.",
    "The AARTFAAC project aims to implement an All-Sky Monitor (ASM), using the Low Frequency Array (LOFAR) telescope. It will enable real-time, 24x7 monitoring for low frequency radio transients over most of the sky locally visible to the LOFAR at timescales ranging from milliseconds to several days, and rapid triggering of follow-up observations with the full LOFAR on detection of potential transient candidates. These requirements pose several implementation challenges: imaging of an all-sky field of view, low latencies of processing, continuous availability and autonomous operation of the ASM. The first of these has already resulted in the correlator for the ASM being the largest in the world in terms of its number of input channels. It will generate $\\sim 1.5 \\cdot 10^5$ correlations per second per spectral channel when built. Test observations using existing LOFAR infrastructure were carried out to quantify and constrain crucial instrumental design criteria for the ASM. In this paper, we present an overview of the AARTFAAC data processing pipeline and illustrate some of the aforementioned challenges by showing all-sky images obtained from one of the test observations. These results provide quantitative estimates of the capabilities of the instrument.",
    "Wolf-Rayet (WR) stars are the evolved descendants of massive O-type stars and are considered to be progenitor candidates for Type Ib/c core-collapse supernovae (SNe). Recent results of our HST/WFC3 survey of Wolf-Rayet stars in M101 are summarised based on the detection efficiency of narrow-band optical imaging compared to broad-band methods. Weshow that on average of 42% WR stars, increasing to ~85% in central regions, are only detected in the narrow-band imaging. Hence, the non-detection of a WR star at the location of ~10 Type Ib/c SNe in broad-band imaging is no longer strong evidence for a non-WR progenitor channel."
  ],
  "sampled": [
    "Process calculi grounded in logic, such as DILL and CP, provide a solid foundation for deadlock-free concurrent programming. However, a discrepancy has existed between the proof construction rules and the term constructors of the -calculus, specifically with regard to the fundamental operator for parallel composition, which lacks a corresponding rule in linear logic. To address this mismatch, Kokke et al. (2019) introduced Hypersequent Classical Processes (HCP), which leverages hypersequents (collections of sequents) to capture parallelism in typing judgments. Nevertheless, the transition from CP to HCP is a significant one. Currently, HCP lacks reduction semantics, and the incorporation of delayed actions means that CP processes interpreted as HCP processes do not exhibit the same behavior as they would in CP. To bridge this gap, we propose HCP-, a variant of HCP that incorporates reduction semantics and eliminates delayed actions. We establish progress, preservation, and termination properties for HCP- and demonstrate that it supports the same communication protocols as CP.",
    "Let's talk about a simplified version of the BDDC preconditioner. Essentially, we're imposing constraints on a select group of subobjects - think subdomain subedges, subfaces, and vertices between pairs of subedges. The good news is that we can prove the condition number of this preconditioner is capped at C(1 + log(L/h))^2, where C is a constant and h and L represent the characteristic sizes of the mesh and subobjects, respectively. The beauty of this is that we have a lot of flexibility in choosing L, which means we can theoretically get the condition number down to O(1). We'll dive into the advantages and disadvantages of this preconditioner, as well as how it performs when applied to heterogeneous problems. And to top it off, we'll share some numerical results from our supercomputer simulations.",
    "We provide examples of the Heun function as solutions to wave equations in general relativity. Specifically, we show that the Dirac equation in the Nutku helicoid metric background yields Mathieu functions in 4D spacetime, but in 5D spacetime, it leads to the double confluent Heun function. We demonstrate how to reduce this solution to the Mathieu function using specific transformations. Additionally, we apply Atiyah-Patodi-Singer spectral boundary conditions to account for the metric's singularity at the origin.",
    "Scientists have long been puzzled by the slow decline in X-ray activity during long-duration solar flares. Research suggests that this phenomenon can only be explained by a continuous process of magnetic reconnection and energy release in the corona, the outer atmosphere of the sun. Our team used data from the RHESSI spacecraft to investigate two key questions: How efficient is this energy release process during the decay phase of long-duration flares, and can we accurately calculate the rate of energy release from this data? To answer these questions, we reconstructed images of selected flares during their decay phase and analyzed the physical properties of the coronal sources. This allowed us to study the effectiveness of the energy release process and evaluate the accuracy of each component involved in the energy equation.",
    "We uncover the characteristic geometric structure of clusters in random media under the FK measure through a multi-scale analysis, with implications holding in all dimensions greater than or equal to 2, contingent on slab percolation under the averaged measure - a condition expected to be met throughout the supercritical phase. This breakthrough builds upon Pisztora's work, providing a crucial framework for understanding the supercritical regime in disordered FK, Ising, and Potts models.",
    "Classical T Tauri stars (CTTS) exhibit weak photospheric absorption lines, a phenomenon known as veiling. This veiling is typically attributed to excess continuous emission generated by shock-heated gas at the stellar surface, beneath the accretion streams. To investigate the relationship between veiling, stellar brightness, and emission line strengths, we selected four CTTS with exceptionally strong veiling: RW Aur A, RU Lup, S CrA NW, and S CrA SE. We monitored these stars through photometric and spectroscopic observations at multiple epochs.\n\nAccording to standard accretion models, variations in the accretion rate should lead to changes in excess emission, resulting in corresponding fluctuations in stellar brightness. However, our findings indicate that the veiling of absorption lines in these stars is highly variable and often so extreme that it would require the release of multiple stellar luminosities of potential energy. Surprisingly, the veiling factors derived during periods of high line dilution show only a weak correlation with brightness. Furthermore, the emission line strengths do not follow the expected trend with veiling.\n\nWe observed that veiling can change dramatically within a single night and is not correlated with the rotation periods of the two stars for which this data is available. In at least three of the stars, high veiling is accompanied by the filling-in of photospheric lines with line emission, resulting in large veiling factors unrelated to changes in continuous emission from shocked regions. We also explored the impact of dust extinction and electron scattering in the accretion stream on veiling measurements in CTTS.\n\nOur conclusion is that veiling cannot be relied upon as a measure of accretion rates in CTTS with rich emission line spectra.",
    "Contrary to popular belief, giant low surface brightness (GLSB) galaxies may not be as massive and dark matter-dominated as previously thought. This misconception stems from unreliable rotation curves. Our new study focuses on two prototypical GLSB galaxies, Malin 1 and NGC 7589, and re-examines existing HI observations to derive new rotation curves. Surprisingly, these curves exhibit a steep rise in the central regions, similar to high surface brightness (HSB) systems. By decomposing the mass of these galaxies, we find that baryons may play a dominant role in the inner regions, with stellar mass-to-light ratios comparable to those of HSB galaxies. Our results, combined with recent studies, suggest that GLSB galaxies have a dual structure: a central HSB early-type spiral galaxy surrounded by an extended LSB disk. We also tested the MOND theory, which accurately predicts the rotation curve of NGC 7589 but struggles to explain Malin 1.",
    "The study examines the multiplicity distribution, moments, scaled variance, entropy, and reduced entropy of target evaporated fragments in various nucleus-emulsion interactions. The results show:\n\n* Gaussian distribution fits the multiplicity distribution in forward and backward hemispheres.\n* Multiplicity moments increase with order and are energy-independent for second-order moments.\n* Scaled variance is close to 1, indicating weak correlations among produced particles.\n* Entropy is similar in forward and backward hemispheres within experimental errors.",
    "We explore the theoretical behavior of a quantum dot over time when it's excited by off-resonant light pulses that quickly prepare the dot's states with the help of acoustic phonons. Our research reveals that three key processes occur when short laser pulses are used: the dot's states are initially 'dressed' as the pulse turns on, followed by relaxation caused by phonons, and finally an 'undressing' as the pulse ends. By examining different pulse shapes, we find that a gradual 'undressing' at the end of the pulse has a significant impact on the final state of the dot in short-pulse protocols. Additionally, we demonstrate that in systems with excitons and biexcitons, the characteristics of the laser pulse, such as its frequency offset and duration, as well as the biexciton binding energy, can be used to selectively target specific quantum dot states.",
    "In the standard quantum mechanics framework, the idea that measurements have probabilities was added later, mainly because of experimental results, rather than being a natural part of the mathematical model. However, an alternative approach called quantum logics provides a clear understanding of probability from the start. This approach includes the use of projection lattices in von Neumann algebras, where updating probabilities is equivalent to the measurement process described by Lueders and von Neumann. This leads to the definition of five levels of compatibility and measurability in quantum logics, which can be understood as: no quantum interference, having a joint probability distribution, being able to measure things at the same time, the final state not depending on the order of measurements, and two events belonging to the same logical group. While these levels are distinct in general, they coincide in the usual quantum mechanics framework, von Neumann algebras, and some other cases.",
    "Imagine a special kind of material that's made up of layers, each with its own unique properties. When light passes through this material, it behaves in a certain way depending on the direction it's traveling. We've taken a closer look at how this works in materials that have a special kind of symmetry, called elliptical birefringence.\n\nWhat we found is that the way light behaves in each layer affects how it behaves in the layers around it. This creates a kind of \"coupling\" between the layers, which changes the way light moves through the material. This coupling adds some extra complexity to the way light behaves, and it can even create gaps in the frequencies of light that can pass through the material.\n\nWe've studied the conditions under which these gaps form, and we've found that it depends on how the light is polarized in each layer. We've developed a simple way to measure this effect, and we've looked at how to make the gaps as big as possible. Our research helps us better understand how light interacts with these special materials, and it could have important implications for the development of new technologies.",
    "Imagine being able to supercharge your machine learning models by tapping into the power of random subspaces! That's exactly what we're exploring in this study, where we're pushing the boundaries of classical empirical risk minimization by introducing a game-changing twist: our hypothesis space is a random subspace of a given space. And the best part? This approach can lead to significant computational savings! But the million-dollar question is: does this shortcut come at the cost of accuracy? We're excited to report that our research shows it's possible to have your cake and eat it too - at least, in certain scenarios. Building on recent breakthroughs for least squares and self-concordant loss functions, we're breaking new ground by extending these results to convex Lipschitz loss functions, like the hinge loss used in support vector machines. Our innovative proofs and technical tools have led to some amazing discoveries, including the existence of 'sweet spots' where computational efficiency can be boosted without sacrificing performance. And to put it all into practice, we've included some simple yet illuminating numerical experiments to drive the point home. Get ready to take your machine learning to the next level!",
    "In the world of healthcare, patient consent is the golden key that unlocks access to medical data. Traditionally, this consent has been obtained through a tedious paper form that patients must fill out and sign. But with the rise of e-Health systems, we're revolutionizing the way consent is captured and managed. By integrating consent into the very fabric of medical data access, patients are empowered to take control of their personal information like never before. They can grant and revoke consent with ease, adapting to their unique situation and needs. However, this flexibility comes with a challenge: capturing the nuances of consent in a way that's both accurate and efficient. Our solution? ACTORS, a groundbreaking goal-driven approach that harnesses the power of Teleo-Reactive programming to manage consent in a dynamic, context-aware way. By focusing on the patient's goals and adapting to changing domains and contexts, ACTORS is poised to transform the way we approach consent in healthcare.",
    "This paper undertakes a mathematical examination of the inverse random source problem pertaining to the time-fractional diffusion equation, wherein the source is presumed to be driven by a fractional Brownian motion. In the context of the direct problem, the stochastic time-fractional diffusion equation is investigated, given a random source. Conversely, the inverse problem seeks to determine the statistical properties of the source based on the expectation and variance of the final time data. Our analysis demonstrates that the direct problem is well-posed and possesses a unique mild solution, subject to certain conditions. Furthermore, we establish the uniqueness of the inverse problem and characterize its instability. The primary components of our analysis are founded upon the properties of the Mittag-Leffler function and the stochastic integrals associated with the fractional Brownian motion.",
    "Manifold learning techniques play a vital role in uncovering the underlying structure of high-dimensional data sets that possess a low intrinsic dimensionality. Many of these methods rely on graph-based approaches, where each data point is represented as a vertex, and the relationships between them are captured by weighted edges. Existing theoretical frameworks have demonstrated that the Laplacian matrix of such graphs converges to the Laplace-Beltrami operator of the underlying data manifold, assuming that the pairwise affinities are calculated using the traditional Euclidean norm. This paper takes a significant step forward by deriving the limiting differential operator for graph Laplacians constructed using any norm, thereby expanding the scope of manifold learning. Our proof involves a fascinating interplay between the geometric properties of the manifold, as characterized by its second fundamental form, and the convex geometry of the unit ball associated with the chosen norm. To illustrate the potential advantages of non-Euclidean norms in manifold learning, we apply our approach to the challenging task of mapping the complex motion of large molecules with continuous variability. Through a numerical simulation, we demonstrate that a modified Laplacian eigenmaps algorithm, based on the Earthmover's distance, outperforms the classic Euclidean Laplacian eigenmaps in terms of both computational efficiency and the sample size required to accurately recover the intrinsic geometry of the data.",
    "We propose an efficient integral equation method for solving the heat equation in a 2D multiply connected domain with Dirichlet boundary conditions. Unlike traditional approaches that rely on the heat kernel, our method involves discretizing time first, which leads to a non-homogeneous modified Helmholtz equation at each time step. The solution is expressed as a combination of volume and double layer potentials. We employ a fast multipole-accelerated solver to efficiently evaluate the volume potential. The boundary conditions are then enforced by solving an integral equation for the homogeneous modified Helmholtz equation, also accelerated by the fast multipole method. The computational cost per time step is linear, O(N) or O(N log N), for a total of N discretization points in the boundary and domain.",
    "We propose a scheme for determining the initial quantum state of qudits through sequential state-discrimination measurements. Since the qudits belong to a set of nonorthogonal quantum states, they cannot be distinguished with absolute certainty. However, unambiguous state discrimination allows for error-free measurements, albeit with the possibility of occasionally failing to provide a conclusive answer about the qudit's state. Notably, qudits have the potential to transmit more information per transmission than qubits. We explore a scenario in which Alice sends one of N qudits, each with a dimension of N. We examine two cases: one where all states have the same overlap, and another where the qudits are divided into two sets with different overlaps between qudits in different sets. Furthermore, we investigate the robustness of our scheme against a simple eavesdropping attack and find that using qudits instead of qubits increases the likelihood of an eavesdropper introducing errors and being detected.",
    "Imagine having super secure access control in Hyperledger Fabric blockchain. That's what we're going for here! We're making it happen by combining multiple IDs, attributes, and policies with the access control regulators. \n\nFirst, we took a close look at the current access control system used by Hyperledger Fabric. Then, we came up with a new and improved way to make access control decisions based on multiple IDs, attributes, and policies. It's way easier for users and developers to make these decisions now!\n\nOur new implementation makes it simple to add attributes and register new certificates (for new users) by wrapping the Fabric CA client. The best part? Our research shows that it's totally possible to combine multiple IDs, attributes, and policies using Hyperledger Fabric's smart contract tech. And the cherry on top? The performance impact is basically zero, especially when compared to just giving everyone access without any control!",
    "This research pioneers the development of pyramidal convolution (PyConv), a novel approach that enables the simultaneous processing of input data across multiple filter scales. By incorporating a hierarchical structure of kernels, each with distinct filter types, sizes, and depths, PyConv is capable of capturing a wide range of details within a scene. Notably, this enhanced recognition capability is achieved without incurring additional computational costs or parameter increases compared to traditional convolutional methods. Furthermore, PyConv's flexibility and extensibility provide a vast design space for tailoring network architectures to diverse applications. With its far-reaching potential to transform the computer vision landscape, we demonstrate PyConv's efficacy across four fundamental tasks: image classification, video action recognition, object detection, and semantic image segmentation. Our results show substantial performance gains compared to baseline models, including a 50-layer PyConv network that surpasses a 152-layer ResNet counterpart on the ImageNet dataset while requiring fewer parameters, lower computational complexity, and fewer layers. Additionally, our framework establishes a new benchmark for scene parsing on the ADE20K dataset. The PyConv code is publicly available at: https://github.com/iduta/pyconv.",
    "The CERN Axion Solar Telescope (CAST) has made significant progress in the search for solar axions. In the first part of CAST phase II, we achieved a major milestone by scanning axion masses up to 0.4 eV using 4He gas at variable pressure. Our results set a new upper limit on the axion-photon coupling of g < 2.17 x 10^10 GeV$-1 at 95% CL for axion masses below 0.4 eV, depending on the pressure setting.\n\nCurrently, we are conducting a search for axions with masses up to 1.2 eV using 3He as a buffer gas in the second part of CAST phase II. We will present our expectations for the sensitivity of this search.\n\nLooking ahead, we will discuss near-future perspectives and long-term options for a new helioscope experiment, building on the success of CAST.",
    "Despite observations showing that Arctic sea ice is rapidly melting and Antarctic sea ice is expanding, climate models typically predict a moderate decrease in both. However, some models do show similar trends to the observations. Recent studies have suggested that these models are consistent with the observations when natural climate variability is considered. Our analysis of climate model simulations from 1979-2013 reveals that the models are not consistent with the observations. In the Arctic, the models that show rapid sea ice melting like we've observed also show too much global warming. In fact, the models would need to be run over 100 times to get the observed level of Arctic sea ice melting by chance. In the Antarctic, the models that show rapid sea ice expansion like we've observed also show too little global warming. This suggests that the models are getting the right answers for the wrong reasons in both the Arctic and Antarctic.",
    "Unlocking the Power of Biometrics in IoT Devices: Overcoming the Challenges\n\nAs the Internet of Things (IoT) continues to grow, biometric features are emerging as a crucial tool for authenticating devices. However, there are several obstacles hindering the large-scale development and deployment of biometrics models. This investigation delves into the factors limiting the use of human physiological features (such as face, eyes, fingerprints, and electrocardiogram) and behavioral characteristics (like signature, voice, gait, and keystroke) in IoT devices.\n\nWe explore the various machine learning and data mining methods employed in authentication and authorization schemes for mobile IoT devices, as well as the threat models and countermeasures used to safeguard biometrics-based authentication systems.\n\nOur research provides a comprehensive analysis of the current state of biometric-based authentication schemes for IoT devices, highlighting the challenges and opportunities for future research. By understanding the complexities and limitations of biometrics in IoT, we can unlock the full potential of this technology and create a more secure and connected world.",
    "Device identification through web-based fingerprinting has gained significant attention from both researchers and commercial entities. Currently, most fingerprinting methods rely on software-based features that can be easily altered by users, rendering them ineffective. This paper proposes a new approach that leverages the HTML5 standard to create fingerprinting features based on a device's hardware, which are much more difficult to modify or conceal. This approach offers a higher level of accuracy in identifying devices. We suggest several methods for HTML5 web applications to identify a device's hardware and present initial experimental results on fingerprinting a device's graphics processing unit (GPU).",
    "We derive the partition function of Chern-Simons theory on a three-sphere with exceptional gauge groups, which can be expressed in terms of the refined closed topological string partition function. This equivalence is established through the relation 2 = gs(1-b), where  is the single Khler parameter, gs is the string coupling constant, and b is the refinement parameter, taking values of 5/3, 5/2, 3, 4, and 6 for the G2, F4, E6, E7, and E8 groups, respectively. The non-vanishing BPS invariants, denoted by NdJL,JR, are found to be N2(0,1/2) = 1 and N11(0,1) = 1. Additionally, the Chern-Simons partition function contains a term corresponding to refined constant maps in string theory. Our derivation relies on the universal form of the Chern-Simons partition function on a three-sphere, restricted to the exceptional line Exc, characterized by Vogel's parameters satisfying  = 2(+). This line encompasses points corresponding to all exceptional groups. Notably, identical results are obtained for the F line, defined by  = +, which includes the SU(4), SO(10), and E6 groups, with non-zero BPS invariants N2(0,1/2) = 1 and N7(0,1) = 1. In both cases, the refinement parameter b is expressed in terms of universal parameters, restricted to the line, as b = -/, which is equivalent to -2/1 in Nekrasov's parameterization.",
    "The centerpoint theorem is a fundamental concept in discrete geometry, stating that for any set of n points in d-dimensional space, there exists a point c (not necessarily part of the original set) such that every half-space containing c includes at least n/(d+1) points from the original set. This point c is called a centerpoint, which can be seen as a higher-dimensional equivalent of a median. However, what if we want to have multiple representatives instead of just one? In one-dimensional data, it's common to use quantiles as representatives. We propose an extension of this concept to higher dimensions. Our idea is to find a small set Q of points such that every half-space containing at least one point from Q includes a significant proportion of the original points, and every half-space containing more points from Q includes an even larger proportion. This concept is related to weak -nets and weak -approximations, but is stronger than the former and weaker than the latter.",
    "We have developed a new software package called PsrPopPy for simulating pulsar populations. It's based on the Psrpop package, but we've rewritten the code in Python to make it more modular and flexible. We've also kept some external libraries in their original Fortran code. The software comes with pre-written scripts for standard simulations, but you can also write your own custom scripts. The modular design makes it easy to add new features, such as different models for period or luminosity distributions.\n\nWe're also exploring ways to expand the software's capabilities. To demonstrate its potential, we've used PsrPopPy to analyze survey results and found that pulsar spectral indices follow a normal distribution with a mean of -1.4 and a standard deviation of 1.0. We've also used the software to model pulsar spin evolution and calculate the relationship between luminosity and spin parameters. Our results show that the underlying population is best described by a power-law dependence of radio luminosity on period and period derivative. Specifically, we found that L  P^(-1.39  0.09) ^(0.48  0.04), which is similar to the relationship found for -ray pulsars. Using this relationship, we've generated a model population and examined the age-luminosity relation for all pulsars, which could be measurable with future large-scale surveys using the Square Kilometer Array.",
    "We investigate the dynamics of a spin ensemble strongly coupled to a resonator driven by external pulses. When the spin ensemble's mean frequency resonates with the cavity mode, we observe damped Rabi oscillations, accurately described by our model, including the dephasing effect of inhomogeneous spin broadening. We show that precise knowledge of this broadening is crucial for understanding spin-cavity dynamics. By driving the system with pulses that match specific resonance conditions, we can enhance coherent oscillations between the spin ensemble and cavity by several orders of magnitude. Our theoretical approach is validated by an experiment using negatively charged nitrogen-vacancy centers in diamond coupled to a superconducting waveguide resonator.",
    "We explore the properties of an inhomogeneous quantum Ising spin-1/2 chain in a transverse field, focusing on its ground-state Riemannian metric and cyclic quantum distance. To analyze this model, we employ a general canonical transformation to convert the spin system into a fermionic Hamiltonian, which can be diagonalized. By applying a gauge transformation to the spin Hamiltonian using a twist operator, we introduce a parameter manifold ring, denoted by S^1, and derive the ground-state Riemannian metric exactly. We then investigate the ground-state cyclic quantum distance and the second derivative of the ground-state energy across different regions of inhomogeneous exchange coupling parameters. Notably, our results show that the quantum ferromagnetic phase in the uniform Ising chain is characterized by a constant ground-state Riemannian metric and an invariant cyclic quantum distance, whereas in the paramagnetic phase, the metric rapidly decays to zero.",
    "Rotation measure synthesis, a technique that leverages Fourier transforms to estimate Faraday dispersion, has emerged as a primary tool for probing cosmic magnetic fields. Interestingly, we have found that this method can be mathematically equated to the one-dimensional interferometric intensity measurement equation, albeit in a distinct Fourier space. This equivalence enables the adaptation of familiar concepts from two-dimensional intensity interferometry, designed to accommodate various instrumental conditions, to the analysis of Faraday dispersion. Notably, we demonstrate how to model the impact of channel averaging during Faraday reconstruction, a limitation that has hindered progress in polarimetric science using wide-band measurements. Through simulations of one-dimensional sparse reconstruction with channel averaging, we show that it is possible to recover signals with large rotation measure values that were previously undetectable. This breakthrough is particularly significant for low-frequency and wide-band polarimetry. Furthermore, we extend these concepts to incorporate mosaicking in Faraday depth into the channel averaging process. This work provides the first comprehensive framework for undertaking wide-band rotation measure synthesis, including the capability to combine data from multiple telescopes, which is expected to substantially enhance the quality and quantity of polarimetric science. The significance of this development lies in its potential to accurately probe extreme environments characterized by high magnetic fields, such as those associated with pulsars and Fast Radio Bursts (FRBs), and to utilize these sources as probes of cosmological fields.",
    "This study investigates the characteristic properties of charged particle production in high-energy hadron-nucleus collisions using a range of statistical models. We compare the predictive power of four different approaches: the Negative Binomial distribution, the shifted Gompertz distribution, the Weibull distribution, and the Krasznovszky-Wagner distribution. These distributions, derived from various functional forms, are based on either phenomenological parameterizations or models of the underlying dynamics. Notably, some of these distributions have also been applied to Large Hadron Collider (LHC) data for both proton-proton and nucleus-nucleus collisions. Our analysis employs a variety of physical and derived observables to assess the relative success of each model.",
    "In 1975, John Tukey introduced the concept of a multivariate median, defined as the point with the highest \"depth\" within a given data cloud in R^d. Later, David Donoho and Miriam Gasko developed a method to measure the depth of an arbitrary point z with respect to the data by identifying the smallest proportion of data points separated by hyperplanes passing through z. This concept has since proven to be highly fruitful, leading to the development of a rich statistical methodology based on data depth and nonparametric depth statistics. Various notions of data depth have been introduced, differing in their computability, robustness, and sensitivity to asymmetric data shapes. Each notion is suited to specific applications due to its distinct properties. The upper level sets of a depth statistic form a family of set-valued statistics, known as depth-trimmed or central regions, which describe the distribution's location, scale, and shape. The most central region serves as a median. The concept of depth has been generalized from empirical distributions (data clouds) to general probability distributions on R^d, enabling the derivation of laws of large numbers and consistency results. Furthermore, it has been extended to functional spaces, accommodating data with more complex structures.",
    "Designing optoelectronic devices at the nanoscale requires strain-engineering in SiGe nanostructures. We've developed a new approach to achieve high tensile strain without external stressors, making it more scalable. By laterally confining SiGe structures with a Si substrate, we've created Ge-rich SiGe nano-stripes with a large tensile hydrostatic strain component. This strain is highest at the center of the top surface and decreases towards the edges.\n\nUsing advanced techniques like tip-enhanced Raman spectroscopy, finite element method simulations, and ab initio calculations, we've measured the strain state of these nano-stripes with unprecedented resolution (~30 nm). Our results show that the lattice deformation is greater than in thermally relaxed Ge/Si(001) layers, due to the combination of lateral confinement and plastic relaxation at the SiGe/Si interface.\n\nWe've also probed the effect of this tensile lattice deformation on the stripe surface using X-ray photoelectron emission microscopy, achieving a spatial resolution better than 100 nm. Our findings show a positive work function shift compared to bulk SiGe alloys, which is supported by electronic structure calculations. These results have significant implications for designing optoelectronic devices at the nanoscale.",
    "In the midst of a pandemic, the ability to share electronic medical records and models across regions is crucial. However, applying data or models from one region to another can be a recipe for disaster, as it often leads to distribution shift issues that defy traditional machine learning assumptions. That's where transfer learning comes in - a powerful solution to bridge the gap.\n\nTo unlock the full potential of deep transfer learning algorithms, we put two data-based approaches (domain adversarial neural networks and maximum classifier discrepancy) and model-based transfer learning algorithms to the test in infectious disease detection tasks. We also delved into well-defined synthetic scenarios where the data distribution differences between two regions were crystal clear.\n\nOur experiments yielded exciting results: transfer learning can be a game-changer in infectious disease classification when (1) the source and target regions share similarities, but the target region lacks sufficient training data, and (2) the target region's training data is unlabeled. In the first scenario, model-based transfer learning shines, with performance rivaling that of data-based transfer learning models. However, there's still more work to be done to tackle the domain shift in real-world research data and address the performance drop.",
    "Over the past decade, the phenomenon of bound states in the continuum (BIC) has garnered significant attention in the realms of optics and photonics, sparking a flurry of research endeavors. In particular, the investigation of quasi-BICs in simplistic structures has proven to be a fascinating area of study, as these structures often exhibit pronounced quasi-BIC characteristics. A paradigmatic example of such a structure is the dielectric cylinder, which has been extensively explored in various studies, both in isolation and in combination with other cylinders. This present work delves into the properties of quasi-BICs during a gradual transformation from a homogeneous dielectric cylinder situated in an air environment to a ring with narrow walls, achieved by incrementally increasing the diameter of the inner air cylinder. The findings of this study reveal a remarkable crossover of quasi-BICs from the strong-coupling to the weak-coupling regime, manifesting as a transition from the avoided crossing of branches to their intersection, with the quasi-BIC persisting solely on one linear branch. Notably, in the strong-coupling regime, three waves interact in the far-field zone: two waves corresponding to the resonant modes of the structure and the wave scattered by the structure as a whole. This observation prompts a critical examination of the Fano resonance concept, which is typically employed to describe the interference of only two waves under weak coupling conditions, thereby raising questions about its validity in this context.",
    "Turbulent thermal diffusion is a complex phenomenon that arises from the interplay between temperature-stratified turbulence and the inertia of small particles. This process gives rise to a non-diffusive turbulent flux of particles that aligns with the direction of the turbulent heat flux. The magnitude of this flux is directly proportional to the product of the mean particle number density and the effective velocity of inertial particles.\n\nPrevious research has only explored this effect under limited conditions, specifically for small temperature gradients and Stokes numbers (Phys. Rev. Lett. **76**, 224, 1996). In this study, we have developed a generalized theory of turbulent thermal diffusion that can accommodate arbitrary temperature gradients and Stokes numbers.\n\nTo validate our theory, we conducted laboratory experiments in oscillating grid turbulence and multi-fan produced turbulence, which simulated strongly stratified turbulent flows. Our results show that the ratio of the effective velocity of inertial particles to the characteristic vertical turbulent velocity decreases to less than 1 at large Reynolds numbers.\n\nFurthermore, we found that the effective velocity of inertial particles and the effective coefficient of turbulent thermal diffusion increase with Stokes numbers, peaking at small Stokes numbers and decreasing for larger values. Additionally, the effective coefficient of turbulent thermal diffusion decreases with increasing mean temperature gradients.\n\nOur developed theory has been successfully validated through comparison with the results of our laboratory experiments, demonstrating its accuracy and reliability.",
    "Unlocking the Secrets of Pulsar Radio Emission: A New Perspective\n\nFor decades, the rotating vector model (RVM) has been the standard approach to understanding pulsar radio emission. However, this model relies on a simplifying assumption that the emission is confined to a narrow cone around the tangent to a dipolar field line. But what if we could do better?\n\nA more exact treatment, known as the tangent model, reveals that the visible point of emission changes as the pulsar rotates, tracing a trajectory on a sphere of radius r. By solving for this trajectory and the angular velocity of the visible point, we uncover a more nuanced understanding of pulsar emission.\n\nRecent research has even suggested that this motion could be observable using interstellar holography. But how accurate is the RVM in capturing this phenomenon? Our analysis reveals that the RVM introduces significant errors, particularly for pulsars with emission spanning a wide range of rotational phases. In fact, the RVM tends to underestimate the range of phases over which emission is visible.\n\nOur findings have important implications for the geometry of pulsar emission. They suggest that the visible radio emission is likely to originate from heights exceeding 10% of the light-cylinder distance, where retardation effects become significant. This challenges our current understanding and opens up new avenues for research into the mysteries of pulsar radio emission.",
    "In image recognition, it is not uncommon for training samples to be incomplete, failing to represent all target classes. Zero-shot learning (ZSL) addresses this limitation by leveraging class semantic information to classify samples from unseen categories that are absent from the training set. This paper presents an end-to-end framework, dubbed the Global Semantic Consistency Network (GSC-Net), which harnesses the semantic information of both seen and unseen classes to facilitate effective zero-shot learning. Furthermore, we incorporate a soft label embedding loss to capitalize on the semantic relationships between classes. To extend GSC-Net to the more realistic setting of Generalized Zero-shot Learning (GZSL), we introduce a parametric novelty detection mechanism. Our approach achieves state-of-the-art performance on both ZSL and GZSL tasks across three visual attribute datasets, thereby validating the efficacy and advantages of the proposed framework.",
    "You might think that Category theory is all about supporting Mathematical Structuralism, but that's actually a misconception. The truth is, Category theory needs a different philosophical approach to math altogether. While structural math focuses on unchanging patterns, categorical math looks at how things change and transform - and often, there's no fixed pattern to be found. In this paper, I'll explore a new way of understanding categorical math that breaks free from structuralism, and show how it can change the way we think about the history of math and how we teach it.",
    "In a dynamic, out-of-balance system of exciton-polariton condensates, where polaritons are created through incoherent energy injection, we demonstrate the ability to generate stable, ring-shaped vortex memory elements with a topological charge of either +1 or -1. By utilizing simple, carefully designed potential guides, we can selectively replicate the same topological charge or invert it onto a separate, spatially distinct ring pump. This innovative manipulation of binary information unlocks the potential for a novel type of processing, where vortices serve as robust, topologically protected memory components, paving the way for groundbreaking advancements in data storage and manipulation.",
    "Over the course of three years, our team put the LOFT mission's satellite instrumentation to the test, simulating the harsh conditions of space to gauge its resilience. As part of the ESA's Cosmic Vision program, we subjected the silicon drift detectors to intense proton radiation, with energies of 0.8 and 11 MeV, to observe how it affected their performance. We also blasted them with high-speed dust particles to see how they'd hold up to debris impacts. In this paper, we'll dive into the details of our experiments and explore what our findings mean for the LOFT mission's success.",
    "This paper explores the effectiveness of low-level multimodal features in extracting movie similarities within a content-based movie recommendation framework. We present a multimodal representation model that combines textual information from subtitles, audio cues, and visual features to capture movie characteristics. In the textual domain, we employ topic modeling to extract discriminative topics from movie subtitles. For the visual domain, we focus on extracting semantically meaningful features related to camera movements, colors, and faces. In the audio domain, we utilize simple classification aggregates based on pre-trained models. We integrate these features with static metadata (e.g., directors, actors) to demonstrate the enhancement of content-based movie similarity estimation using low-level multimodal information. To validate our approach, we constructed a dataset of 160 well-known movies and evaluated movie similarities through recommendation rankings. Our extensive experiments show that incorporating low-level features from all three modalities (text, audio, and visual) improves the performance of a content-based recommendation system by over 50% relative increase compared to traditional metadata-based approaches. To the best of our knowledge, this is the first study to leverage a comprehensive range of features from all involved modalities to enhance content similarity estimation, surpassing metadata-based methods.",
    "Delving into the mysteries of black hole radiation, we explore the quantum gravity of a Reissner-Nordstrom black hole, electrified with a powerful charge. By applying a canonical quantization approach to a spherically symmetric geometry, we uncover the secrets of the Wheeler-De Witt equation, solving it in two crucial regions: between the outer apparent horizon and infinity, and between the spacetime singularity and the inner apparent horizon. Our findings reveal that the mass loss rate of an evaporating black hole, fueled by thermal radiation, aligns perfectly with semiclassical results - but only when we carefully select an integration constant guided by physical intuition. Moreover, we successfully solve the Wheeler-De Witt equation in the region between the inner Cauchy horizon and the outer apparent horizon, yielding the same mass loss rate expression. This groundbreaking study builds upon the foundation of Schwarzschild black holes, taking a significant leap forward by generalizing the concept to charged Reissner-Nordstrom black holes.",
    "We introduce Multi-Agent A* (MAA*), a complete and optimal algorithm for solving decentralized partially-observable Markov decision problems (DEC-POMDPs) with a finite time limit. MAA* is ideal for planning optimal actions for a team of cooperative agents operating in uncertain environments, such as multi-robot coordination, network traffic control, or distributed resource allocation. Our approach combines classical heuristic search and decentralized control theory to effectively solve these complex problems. Experimental results demonstrate the significant advantages of MAA*. We also present an anytime variant of MAA* and discuss potential extensions, including a method for solving problems with no time limit.",
    "We applied machine learning to classify objects in SDSS DR6, building on the Galaxy Zoo categorizations of early types, spirals, and point sources/artifacts. By training an artificial neural network on a subset of human-classified objects, we tested its ability to replicate human classifications for the remaining sample. Our results show that the neural network's success hinges on the input parameters chosen. While colors and profile-fitting parameters can separate objects into three classes, adding adaptive shape parameters, concentration, and texture significantly improves the results. However, adaptive moments, concentration, and texture alone cannot distinguish between early type galaxies and point sources/artifacts. Using a 12-parameter set, the neural network achieves over 90% accuracy in reproducing human classifications for all three morphological classes. Notably, our results remain robust even when using an incomplete magnitude training set. Our findings suggest that machine learning algorithms hold promise for morphological classification in future wide-field imaging surveys, with the Galaxy Zoo catalogue serving as a valuable training resource.",
    "The Lambek calculus, a prominent logical framework for capturing the intricacies of natural language syntax, has undergone significant expansions to tackle more nuanced linguistic complexities. Initially, the calculus was limited to context-free settings, but subsequent extensions have broadened its scope. Notably, Morrill and Valentin's 2015 extension incorporates exponential and bracket modalities, relying on a non-standard contraction rule that intricately interacts with bracket structures. This novel approach diverges from the traditional contraction rule. This paper presents a proof of the undecidability of the derivability problem within their extended calculus. Furthermore, we examine the restricted, decidable fragments identified by Morrill and Valentin, demonstrating their membership in the NP complexity class.",
    "For decades, the transition between the two phases of 4D Euclidean Dynamical Triangulation was thought to be a second-order phenomenon, but a groundbreaking discovery in 1996 revealed that it's actually a first-order transition - as long as the system is large enough [5,9]! However, some lingering questions remained: did the numerical methods used influence the outcome? We're excited to report that we've tackled these concerns head-on! By allowing the volume to fluctuate freely, taking measurements after a fixed number of attempted moves, and harnessing the power of an optimized parallel tempering algorithm [12], we've overcome the limitations of previous studies. And the result? A resounding confirmation that the phase transition is indeed first order, even in systems as large as 64k 4-simplices! But that's not all - we've also developed a local criterion to identify whether parts of a triangulation are in the elongated or crumpled state, and uncovered a fascinating connection between EDT and the balls in boxes model. This has led to a modified partition function with an additional, third coupling, opening up new avenues for exploration. And, we're thrilled to propose a class of modified path-integral measures that could potentially eliminate the metastability of the Markov chain, transforming the phase transition into a second-order phenomenon. The possibilities are endless, and we can't wait to see where this research takes us!",
    "We've identified the types of groups for which the Domino Problem can be solved. These groups are either virtually free (which means they are similar to finite groups) or have a subgroup that is similar to the integers (specifically, the group of integers with a finite number of elements).",
    "Detecting dark matter indirectly through gamma rays produced by its annihilation in the Galactic halo is a promising approach. We show that distinctive spectral patterns near the dark matter particles' mass, a common prediction in most models, can greatly enhance the ability of gamma-ray telescopes to detect dark matter signals. Our analysis provides projected limits on these features, including the traditionally sought-after line signals, and demonstrates that they can be more effective in understanding dark matter's nature than the broader spectral features expected at lower energies.",
    "Achieving carbon neutrality demands a comprehensive research agenda that tackles the technical and economic hurdles that come with transitioning to 100% renewable electricity. As the share of variable renewable energy sources like wind and solar power grows, maintaining a balance between supply and demand in the grid becomes increasingly complex. The performance and impact of inverters used in these systems also need to be addressed. This study explores the implications of shifting to carbon neutrality and identifies key research challenges in areas such as system planning, operation, and stability, as well as the integration of energy storage, demand-side participation, distributed control, and energy sector coupling. We highlight existing knowledge gaps and present our recent findings that can help fill these gaps, ultimately improving grid operation and estimation. Our comparative case studies provide numerical results on the operational stability and economics of power grids with high levels of renewable energy, helping stakeholders develop targeted roadmaps and make informed decisions.",
    "Convolutional neural networks (CNNs) are widely used in computer vision, but their large size requires significant storage and memory. We introduce Frequency-Sensitive Hashed Nets (FreshNets), a novel architecture that reduces memory and storage needs by exploiting redundancy in CNNs. By converting filter weights to the frequency domain and grouping similar frequencies using a hash function, we can share parameter values and reduce model size. Our approach, evaluated on eight datasets, outperforms several baselines in compressed performance.",
    "We designed a learning exercise that combined Project-Based Learning with Japanese manga techniques to improve Requirements Development processes. We applied manga's established methods, such as character development and storytelling, to RD. By using this innovative approach, students were able to clearly define project objectives early on and successfully developed high-quality, unique system ideas.",
    "Defying the conventional wisdom of the standard Hawking formula, which forecasts the complete evaporation of black holes, we delve into the uncharted territory of quantum gravity. Our investigation reveals a fascinating phenomenon: fermions tunneling out of a 5-dimensional rotating black string. But here's the twist - the temperature of this process is not solely dictated by the string itself, but is also influenced by the quantum number of the escaping fermion and the presence of an extra spatial dimension. This quantum correction throws a wrench into the works, slowing down the temperature's ascent and ultimately giving rise to a remnant that persists even after evaporation.",
    "This study presents a novel approach to word representation, leveraging second-order vector embeddings derived from topological features of nearest neighbors in pre-trained contextual word embeddings. We investigate the efficacy of these second-order embeddings as input features in three distinct natural language processing models: a deep model for named entity recognition, a deep model for recognizing textual entailment, and a linear model for paraphrase recognition. Our findings indicate that nearest neighbor information alone is sufficient to capture a significant proportion of the performance benefits afforded by pre-trained word embeddings. Furthermore, second-order embeddings demonstrate improved robustness in handling heterogeneous data, albeit at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information yields further performance enhancements in certain cases. We also observe that incorporating nearest neighbor features from multiple first-order embedding samples, which are subject to variance due to random initialization, can contribute to downstream performance gains. Finally, our analysis reveals intriguing characteristics of second-order embedding spaces that warrant further investigation, including increased density and distinct semantic interpretations of cosine similarity.",
    "Unlocking the Full Potential of RIS-Aided Millimeter Wave Wireless Systems: Enhanced Localization Capabilities Revealed\n\nImagine a wireless system that not only provides robustness to blockage and enhanced coverage but also offers pinpoint localization accuracy. In this groundbreaking paper, we explore the untapped potential of Reconfigurable Intelligent Surfaces (RIS) to revolutionize wireless localization.\n\nBy harnessing the power of sparse reconstruction algorithms, we can extract high-resolution channel estimates and translate them into precise position information. However, the complexity of sparse recovery in RIS-aided mmWave systems poses a significant challenge, given the massive number of RIS elements and communication arrays.\n\nTo overcome this hurdle, we introduce a novel multidimensional orthogonal matching pursuit strategy for compressive channel estimation. This innovative approach involves computing projections on multiple independent dictionaries, rather than a single large dictionary, to achieve high-accuracy channel estimation at reduced complexity.\n\nBut that's not all. We also combine this strategy with a cutting-edge localization method that doesn't rely on the absolute time of arrival of the Line of Sight (LoS) path. The results are astounding: in a realistic 3D indoor scenario, our RIS-aided wireless system demonstrates a significant improvement in localization accuracy.\n\nDiscover how RIS-aided millimeter wave wireless systems can unlock new possibilities in wireless localization and transform the future of communication.",
    "Imagine being able to safeguard confidentiality like never before! Detecting and quantifying information leaks through timing side channels is crucial, and we've got a game-changing solution. While static analysis has been the go-to approach, it's often too computationally demanding for real-world applications and only provides a simple 'yes' or 'no' answer. But what if you need to leak information about a secret in a controlled way? That's where our innovative dynamic analysis method comes in!\n\nWe've broken down the problem into two manageable tasks. First, we create a timing model of the program using a neural network - and it's incredibly effective! Then, we analyze the neural network to quantify information leaks with precision. Our experiments have shown that both tasks are not only possible but also highly practical, outperforming existing side channel detectors and quantifiers.\n\nOur groundbreaking contributions include a custom neural network architecture that uncovers side channels with ease and an MILP-based algorithm that estimates side-channel strength with accuracy. We've tested our approach on a range of micro-benchmarks and real-world applications, and the results are astounding. Our neural network models can learn the timing behaviors of programs with thousands of methods, and we can efficiently analyze neural networks with thousands of neurons to detect and quantify information leaks through timing side channels. Get ready to revolutionize confidentiality protection!",
    "The inner asteroid belt between 2.1 and 2.5 au is a crucial region, supplying most chondritic meteorites and near-Earth asteroids. Bounded by a secular resonance and the 1:3 mean motion resonance with Jupiter, asteroids can only escape through large eccentricity changes or scattering by Mars. However, Yarkovsky forces are ineffective for asteroids over 30 km in diameter. This study examines chaotic diffusion near the 1:2 mean motion resonance with Mars, revealing that while it increases inclination and eccentricity dispersion, it doesn't alter their mean values. Surprisingly, the resonance mitigates asteroid scattering by Mars at high eccentricities, prolonging their lifetime in the belt. Our findings suggest that gravitational forces alone cannot explain the observed eccentricity changes, and resonant trapping plays a key role in shielding asteroids from Mars encounters.",
    "The discovery of nonstandard neutrino interactions (NSI) could significantly impact the precision of next-generation neutrino oscillation experiments. To better understand and constrain the NSI parameter space, additional types of experiments are necessary. In this study, we investigate the constraints on NSI with electrons using current and future $e^+e^-$ collider experiments, including Belle II, STCF, and CEPC. Our findings indicate that Belle II and STCF will provide competitive and complementary bounds on electron-type NSI parameters, rivaling the current global analysis, and will significantly improve constraints on tau-type NSI. Furthermore, CEPC alone will impose stringent constraints on the NSI parameter space with electrons. Notably, by combining data from three different running modes, we can lift the degeneracy between left-handed (vector) and right-handed (axial-vector) NSI parameters, allowing us to constrain the allowed ranges for $|\\epsilon_{ee}^{eL}|$ ($|\\epsilon_{ee}^{eV}|$) and $|\\epsilon_{ee}^{eR}|$ ($|\\epsilon_{ee}^{eA}|$) to be smaller than 0.002 at CEPC, even if both are present.",
    "The Deep Underground Neutrino Experiment (DUNE) is a cutting-edge project that aims to study neutrinos and search for proton decay. The experiment will use four large tanks filled with liquid argon, which will detect particles and measure their properties. To make sure the design is optimal, two prototype detectors are being tested at CERN. Before that, a smaller version of the detector was built and tested in 2017, and it worked well.\n\nAn important part of the detector is the light detection system, which helps trigger the collection of data and provides additional information about the particles being detected. In the smaller detector, five special light sensors were installed to test different configurations. During the test, data was collected on the light produced when particles interacted with the liquid argon.\n\nThe results of the test showed how well the light detection system worked and provided new insights into the properties of liquid argon. Our studies helped improve our understanding of this material.",
    "In a breakthrough move, top chip manufacturers have unleashed the power of Multithreaded processors, capable of handling a diverse range of workloads. However, to unlock their full potential, efficient resource utilization is crucial. One key to unlocking this potential lies in harnessing memory-level parallelism (MLP). This paper proposes a revolutionary MLP-aware operating system (OS) scheduling algorithm, specifically designed for Multithreaded Multi-core processors. By dynamically monitoring MLP availability in each thread and balancing it with system resources, the OS can generate a optimized thread schedule for the next quantum, leading to a significant boost in overall performance. We present a comprehensive qualitative comparison of our solution with existing hardware and software techniques, paving the way for further innovation. The possibilities are endless - future research directions include quantitative evaluation and refinement of the scheduling optimization, promising to take performance to new heights.",
    "Unlock the Power of Compressed Sensing: A Breakthrough in Blind Calibration\n\nImagine a world where compressed sensing measurement systems can be calibrated with ease, even when faced with unknown gains on each measurement. Our innovative approach makes this a reality, using a few unknown but sparse signals to achieve blind calibration.\n\nYou might expect this problem to be fraught with complexity, similar to blind source separation and dictionary learning, which are notorious for their non-convexity and local minima. But, surprisingly, we've discovered that this challenge can be transformed into a convex optimization problem, solvable with standard algorithms.\n\nOur numerical simulations demonstrate the remarkable effectiveness of this approach, even when dealing with highly uncalibrated measurements. The key? Providing a sufficient number of unknown, sparse calibrating signals. What's more, we've observed that the success or failure of this method is governed by sharp phase transitions, offering valuable insights into the underlying dynamics.\n\nGet ready to unlock the full potential of compressed sensing with our groundbreaking blind calibration technique.",
    "We tackle multi-source morphological reinflection, a generalization of the standard single-source task. The input includes a target tag and multiple source form-tag pairs for a lemma. This approach leverages complementary information from different source forms, such as stems. We propose a novel encoder-decoder architecture with multiple encoders, outperforming single-source models. Our dataset is publicly available to facilitate future research.",
    "**Timely Knowledge Extraction from Semantically Annotated Data Streams**\n\nThe increasing demand for extracting non-trivial knowledge from data streams, particularly on the Web and in the Internet of Things (IoT), requires efficient and expressive reasoning. However, computing expressive reasoning on large streams is a significant challenge.\n\n**Introducing Laser: A Novel Reasoner for Stream Reasoning**\n\nWe propose Laser, a new reasoner that supports a pragmatic fragment of the logic LARS, which extends Answer Set Programming (ASP) for streams. Laser's core innovation is a novel evaluation procedure that annotates formulae to avoid duplicate re-computation at multiple time points.\n\n**Significant Performance Improvement**\n\nLaser's procedure, combined with a judicious implementation of LARS operators, achieves significantly better runtimes than state-of-the-art systems like C-SPARQL, CQELS, and Clingo-based LARS implementations. This breakthrough enables the application of expressive logic-based reasoning to large streams, opening up new possibilities for stream reasoning use cases.",
    "The second law of thermodynamics sets boundaries on how much energy and info can be swapped between physical systems. We're building on a framework that already exists for two systems, and expanding it to work with multiple systems. We've found a key measure that describes how info is shared between them. We've also come up with an improved version of this measure. To make it more concrete, we've tested our ideas using a model featuring two rival 'Maxwell demons'.",
    "Unlocking the Secrets of Graphene's Hidden Spin: How Defects in Transition Metal Dichalcogenides Can Revolutionize Proximity-Induced Spin-Orbit Coupling!\n\nImagine being able to harness the power of graphene's intrinsic spin-orbit coupling, a phenomenon once thought to be negligible. By stacking graphene with transition metal dichalcogenides (TMDCs), we can unlock this hidden potential. But what if we could take it a step further? What if we could deliberately introduce defects into the TMDC layer to amplify the spin-orbit coupling in graphene?\n\nIn this groundbreaking study, we explore the untapped potential of alloyed ${\\rm G/W_{\\chi}Mo_{1-\\chi}Se_2}$ heterostructures, where the composition of the TMDC layer holds the key to unlocking the strength and nature of the induced spin-orbit coupling. Using cutting-edge density functional theory simulations, we delve into the world of defect distributions and diverse compositions ($\\chi$).\n\nOur findings are nothing short of remarkable. Despite the dramatic impact of individual defects on local parameters, the low-energy spin and electronic behavior can be accurately predicted using a simple effective medium model. This model relies solely on the composition ratio of the metallic species in the TMDC layer.\n\nBut that's not all. We also demonstrate that the topological state of these alloyed systems can be precisely tuned by controlling this ratio. The implications are profound: we can now harness the power of proximity-induced spin-orbit coupling to create novel, tunable topological materials.\n\nJoin us on this journey into the uncharted territory of graphene-TMDC heterostructures, where the boundaries of spin-orbit coupling are being rewritten.",
    "Atomic masses are very important in nuclear astrophysics calculations. Because we don't have exact values for some unusual atoms, scientists have been working hard to develop new tools to measure them. There are two main ways to measure atomic masses: the Time-of-Flight (TOF) method and the Penning trap method. The TOF method is useful when the Penning trap method can't be used. The NSCL facility is a great place to use the TOF method to measure the masses of very unusual atoms. Recently, we used a special technique called TOF-Brho at the NSCL facility to measure the masses of some atoms that are important for understanding how stars work and how neutron stars are formed.",
    "A long-standing mystery has shrouded the realm of black holes: why do super-massive active galactic nuclei (AGN) and stellar mass X-ray binaries (XRBs) share so many properties, yet broad emission lines (BELs) are exclusive to AGN? The detection of these lines in SDSS databases has led to a puzzling conclusion: not a single AGN with a mass below 10^5 M_sun has been found. But is this because they genuinely don't exist, or are they simply flying under the radar due to inefficient BEL production?\n\nIn this groundbreaking study, we set out to uncover the truth. By simulating the ionizing spectral energy distribution for a vast range of black hole masses - from 10 to 10^9 M_sun, spanning the entire spectrum from XRBs to AGN - we calculated the equivalent widths (EWs) of key ultraviolet and optical lines, including Ly, H, CIV, and MgII. Our calculations leveraged the powerful LOC (locally optimally emitting cloud) model to describe the BELR.\n\nOur findings are nothing short of astonishing. Contrary to expectations, the hardening of the SED shape with decreasing mass does not diminish BEL EWs. Instead, it's the finite size of the BELR - controlled by the black hole's mass - that regulates the production of these emission lines. We discovered a striking peak in EWs for typical AGN black holes around 10^8 M_sun, below which the lines become increasingly faint, with a precipitous drop-off below 10^6 M_sun. This could be the underlying reason for the absence of low-mass AGN in SDSS databases.",
    "Get ready for a breakthrough in synchronization technology! For the first time ever, we're putting pulse-coupled oscillator-based algorithms to the test on FPGA-based radios - and the results are astounding! Our measurements reveal that these algorithms can achieve precision in the low microsecond range when implemented in the physical layer. But that's not all - we're taking it to the next level with an innovative algorithm extension that compensates for phase rate deviations in the hardware. The outcome? A precision of under one microsecond is now possible! This game-changing technology has the potential to revolutionize ad hoc wireless systems, enabling fully distributed synchronization of transmission slots and sleep cycles - even in scenarios where centralized synchronization is impossible. The future of wireless communication just got a whole lot brighter!",
    "Human Trajectory Prediction (HTP) has been getting a lot of attention lately, and tons of solutions have been thrown into the mix. But here's the thing: we need a way to compare these methods fairly. That's where this paper comes in - we're tackling the question of how to measure just how tricky a dataset is when it comes to predicting human movement. To do that, we've come up with a set of indicators that focus on three key ideas: how predictable are the trajectories, how regular are they, and how complex is the context. We've put the most popular HTP datasets to the test using these indicators, and we're discussing what this means for benchmarking HTP algorithms. Oh, and we've made our code available on Github, so feel free to check it out!",
    "This paper demonstrates how classical linear stochastic systems can be implemented using quantum optical components, which offer faster response and processing times due to their higher bandwidth. A procedure is provided for constructing the quantum optical realization, and its application in a measurement feedback loop is described, with examples illustrating the main results.",
    "Systems biology relies on complex networks of biochemical reactions to understand how biological cells function at the molecular and cellular levels. In these networks, reactions occur at different rates, with some happening much faster than others. By focusing on the fastest reactions first, we can break down the system's behavior into a series of equilibrium states, where different groups of variables reach a balance. In particular, when two opposing reactions have similar strengths and overshadow the others, the system reaches equilibrium. This process can be mathematically described using tropical analysis, allowing us to simplify complex models and better understand the underlying dynamics.",
    "We analyzed the Suzaku data of the galactic disk and outflow regions of the starburst galaxy M82 using spectral analysis. Our thermal modeling of the central disk regions revealed that at least three temperature components are required to accurately describe the data. Notably, the Ly line fluxes of O VIII and Ne X exceeded the expected values for a plasma in collisional ionization equilibrium. The ratios of Ly to Ly lines for O VIII and Ne X were also higher than expected, suggesting that charge exchange may be occurring.\n\nIn contrast, the spectra of the outflow wind region were well-reproduced using two-temperature thermal models. From these models, we derived the metal abundances of O, Ne, Mg, and Fe in the outflow. The abundance ratios of O/Fe, Ne/Fe, and Mg/Fe were approximately 2, 3, and 2, respectively, relative to the solar values reported by Lodders (2003). Since there was no evidence of charge exchange in the outflow region, these metal abundances are likely more reliable than those in the central region. This abundance pattern suggests that starburst activity enriches the outflow by ejecting metals into intergalactic space through supernovae type II explosions.",
    "Unveiling the Cosmic Dust Makers: A Supernova Surprise\n\nFor centuries, scientists believed that dust grains were born in the gentle winds of aging stars. But a new wave of evidence suggests that supernovae, those explosive stellar fireworks, may be the unsung heroes of dust creation. To settle the score, we need to know how much of this freshly minted dust can survive the intense shockwaves that follow a supernova blast and make it into the interstellar medium.\n\nOur team has developed a cutting-edge code, GRASH_Rev, to track the evolution of dust from its formation in the supernova explosion to its eventual merger with the surrounding interstellar gas. We put our code to the test with four iconic supernovae in the Milky Way and Large Magellanic Cloud: SN1987A, CasA, the Crab Nebula, and N49. The results? Our simulations align perfectly with observations, revealing that a remarkable 1-8% of the dust created in these supernovae can withstand the intense shockwaves and make it into the interstellar medium.\n\nThis means that supernovae produce dust at a rate of (3.9  3.7)  10^(-4) M yr^(-1) in the Milky Way, a staggering ten times higher than the dust production rate of aging stars. While this is an impressive feat, it's still not enough to counterbalance the dust destruction caused by supernovae themselves. This leaves us with a tantalizing mystery: where does the rest of the dust come from? The answer, it seems, lies in the accretion of dust in the gas phase. The cosmic dust saga has just gotten a whole lot more interesting!",
    "This paper explores hatching process strategies for additive manufacturing using high-power electron beams through numerical simulations. Building on validated simulation software, we investigate the limitations of a basic process strategy at higher beam powers and scan velocities, up to 10 kW. We then introduce modified strategies to overcome these limitations, enabling fast and cost-effective production of fully dense parts with smooth top surfaces. These optimized strategies maximize beam power usage, unlocking the full potential of high-power electron beam guns.",
    "Bayesian optimization is a type of algorithm designed to find the best solution to a complex problem while minimizing the number of attempts. However, traditional Bayesian optimization methods assume that each attempt has the same cost, which is not always the case. In reality, the cost of each attempt can vary greatly depending on the specific conditions. For instance, training a neural network can become much more expensive as the number of layers increases. To address this limitation, cost-aware Bayesian optimization methods have been developed, which measure progress based on cost metrics such as time, energy, or money. One such method is Cost Apportioned Bayesian Optimization (CArBO), which aims to find the best solution while keeping costs as low as possible. CArBO starts with an efficient initial design and then uses a cost-cooled optimization phase that adjusts its cost model as it progresses. In tests on 20 complex problems, CArBO was able to find better solutions than other methods while staying within the same cost budget.",
    "This work presents a marsupial robotic system combining a legged and aerial robot for collaborative mapping and exploration. The system leverages the strengths of both robots, with the legged robot providing dexterous locomotion and long endurance, and the aerial robot offering 3D navigation capabilities. When the legged robot is hindered by terrain or geometry, it can deploy the aerial robot to explore areas it cannot reach. The two robots share LiDAR-based maps and plan exploration paths autonomously, with the legged robot determining when and where to deploy the aerial robot. Experimental studies demonstrate the system's expanded exploration capabilities and ability to reach previously inaccessible areas.",
    "We suggest a way to make antiprotons spin in a specific direction in a storage ring. We would use a beam of positrons (the opposite of electrons) that is also spinning in a specific direction and moving alongside the antiprotons. If we get the speed of the positrons just right, they can make the antiprotons spin in the same direction. We've done some calculations that show this can work really well.\n\nTo make this happen, we need a strong beam of positrons. We think we can get this from a special source that uses a radioactive material called $^{11}$C. Another way to do it is to use special light to create the positrons, but this is a bit more complicated. Either way, we can use these positrons to make the antiprotons spin in the same direction, whether they're moving slowly or quickly. It would take about an hour to get a large number of antiprotons spinning in the same direction. This method is about 10 times better than other ways that have been suggested.",
    "In the context of nucleic acid molecules, loops play a vital role as secondary structure elements, particularly near the melting point. By employing a theoretical framework that incorporates the logarithmic entropy term c ln m for loops of length m, we investigate the behavior of homopolymeric single-stranded nucleic acid chains under the influence of external force and varying temperature. As the chain length approaches infinity, a phase transition emerges between a compact, folded structure at low temperatures and low forces, and a molten, unfolded structure at high temperatures and high forces. We derive the impact of c on phase diagrams, critical exponents, melting points, and force extension curves using analytical methods. Notably, in the absence of external force, a melting transition is only possible within a narrow range of loop exponents (2 < c < 2.479), whereas outside this range, the chain remains either folded (c  2) or unfolded (c  2.479). However, applying an external force can induce a melting transition with singular behavior for all loop exponents c < 2.479, which can be experimentally observed using single-molecule force spectroscopy. These findings have significant implications for the hybridization and denaturation of double-stranded nucleic acids. Furthermore, we demonstrate that the Poland-Scheraga model, which neglects intra-strand base pairing in denatured regions, can be improved by incorporating realistic loop exponents (c ~ 2.1), leading to the formation of pronounced secondary structures within single strands. This, in turn, affects the melting temperature of the duplex and renormalizes the effective loop exponent c^, influencing universal aspects of the duplex melting transition.",
    "We investigate the Zeeman spin-splitting in hole quantum wires aligned along the [011] and [011] crystallographic axes of a high-mobility, undoped (100)-oriented AlGaAs/GaAs heterostructure. Our findings indicate that the spin-splitting can be toggled on (finite g*) or off (zero g*) by rotating the field from a parallel to a perpendicular orientation relative to the wire. Notably, the wire's properties remain identical for both orientations with respect to the crystallographic axes. Furthermore, we observe that the g-factor in the parallel orientation decreases as the wire is narrowed. This contrasts with electron quantum wires, where the g-factor is enhanced by exchange effects as the wire is narrowed. This phenomenon provides evidence for a k-dependent Zeeman splitting, which arises from the spin-3/2 nature of holes.",
    "Get ready to unlock the secrets of the universe! An ingenious analogy with real Clifford algebras on even-dimensional vector spaces reveals a groundbreaking approach to assigning space and time dimensions modulo 8 to any algebra (represented over a complex Hilbert space) that contains two self-adjoint involutions and an anti-unitary operator with specific commutation relations. And the best part? This assignment is perfectly compatible with the tensor product, meaning the space and time dimensions of the tensor product are simply the sums of the space and time dimensions of its factors! This could be the key to understanding the mysterious presence of such algebras in PT-symmetric Hamiltonians and the behavior of topological matter. But that's not all - this construction also enables us to build an indefinite (i.e. pseudo-Riemannian) version of the spectral triples of noncommutative geometry, defined over Krein spaces instead of Hilbert spaces. Within this revolutionary framework, we can express the Lagrangian (both bosonic and fermionic) of a Lorentzian almost-commutative spectral triple. And as the cherry on top, we've discovered a space of physical states that solves the long-standing fermion-doubling problem! To top it all off, we've even applied this framework to the example of quantum electrodynamics, opening up new avenues for exploration and discovery!",
    "Unveiling the Hidden Symmetries of the Universe: A Journey Beyond Galilean Physics\n\nDelve into the fascinating realm of space-time symmetries, where we explore the actions of massive free relativistic particles and uncover the secrets of post-Galilean physics. By venturing into canonical space, we reveal the complete set of point space-time symmetries that govern these actions.\n\nBut that's not all - our journey also leads to the discovery of an infinite family of generalized Schrdinger algebras, each characterized by an integer M. The M=0 case corresponds to the familiar Schrdinger algebra, but what about the others? We dive deeper, examining the Schrdinger equations associated with these algebras, their solutions, and the intriguing projective phases that emerge.",
    "The development of accretion disc theory lags behind stellar evolution theory, despite the ultimate goal of achieving a similarly mature phenomenological picture. A major challenge in this field is incorporating insights from numerical simulations into practical models that can be compared to observations. One crucial aspect that requires more precise incorporation is non-local transport.\n\nTo highlight the need for this, we review the limitations of the practical approach of Shakura-Sunyaev (1973), which is a mean field theory that does not account for large scale transport. Observations of coronae and jets, as well as results from shearing box simulations of the magnetorotational instability (MRI), suggest that a significant portion of disc transport is indeed non-local.\n\nWe demonstrate that the Maxwell stresses in saturation are dominated by large scale contributions, and that the physics of MRI transport cannot be fully captured by a viscosity. We also clarify the standard physical interpretation of the MRI as it applies to shearing boxes.\n\nWhile computational limitations have focused attention on local simulations, the next generation of global simulations is expected to inform improved mean field theories. In fact, mean field accretion theory and mean field dynamo theory should be unified into a single theory that predicts the time evolution of spectra and luminosity from separate disc, corona, and outflow contributions.\n\nUltimately, any mean field theory has a finite predictive precision that must be quantified when comparing predictions to observations. By addressing these challenges, we can work towards a more comprehensive understanding of accretion disc theory.",
    "This paper investigates the global well-posedness of two Diffuse Interface systems modeling the motion of an incompressible two-phase fluid mixture in a bounded smooth domain $\\Omega\\subset \\mathbb{R}^d$, $d=2,3$, where capillarity effects are present. We examine dissipative mixing effects arising from the mass-conserving Allen-Cahn dynamics with the physically relevant Flory-Huggins potential. Specifically, we consider the Navier-Stokes-Allen-Cahn system for nonhomogeneous fluids and the Euler-Allen-Cahn system for homogeneous fluids. Our analysis, which combines energy and entropy estimates, a novel end-point estimate, a new Stokes problem estimate, and logarithmic type Gronwall arguments, establishes the existence and uniqueness of global weak and strong solutions, as well as their separation from pure states.",
    "Unlock the secrets of scattering experiments with our groundbreaking Fock-space projection operators! These innovative tools provide explicit expressions for realistic final states, effortlessly summing over unobserved quanta and tackling non-emission into specific momentum space sub-regions. Get ready to take your research to the next level!",
    "Join us as we explore the fascinating mathematical structures behind Feynman graphs, a crucial tool in perturbative quantum field theory calculations. Not only are these structures intriguing in their own right, but they also enable us to develop algorithms for computing these graphs. In this talk, we'll delve into the connections between Feynman integrals, periods, shuffle algebras, and multiple polylogarithms, uncovering the hidden patterns and relationships that make these calculations possible.",
    "In a groundbreaking calculation, we uncover the generalized parton distributions of the photon under the influence of non-zero momentum transfer in both transverse and longitudinal directions. By applying Fourier transforms to these GPDs, we successfully map the parton distributions of the photon in position space, shedding new light on the photon's internal structure.",
    "Transformers have revolutionized sequence modeling, but their success comes at a cost: they require massive memory storage to retain all historical token-level representations. To address this efficiency bottleneck, we introduce Memformer, a novel neural network architecture that leverages an external dynamic memory to encode and retrieve past information. This innovative approach enables Memformer to process long sequences with unprecedented efficiency, achieving linear time complexity and constant memory space complexity. Furthermore, we propose a new optimization technique, memory replay back-propagation (MRBP), which facilitates long-range back-propagation through time while significantly reducing memory requirements. Our experimental results demonstrate that Memformer outperforms baseline models while using a remarkable 8.1 times less memory space and achieving 3.2 times faster inference speeds. A closer examination of the attention patterns reveals that our external memory slots effectively encode and retain crucial information across timesteps, underscoring the efficacy of our approach.",
    "We have calculated the leading logarithmic behavior of the cross-section for producing a pseudoscalar Higgs boson through gluon-gluon fusion, considering all orders of perturbation theory and focusing on the high-energy limit. Additionally, we have determined the Higgs rapidity distribution with the same level of accuracy. Our calculations take into account the contributions of top and bottom quarks, as well as their interference. We express our results in terms of single and double integrals, which we have evaluated explicitly up to the next-to-next-to-leading order (NNLO). By applying our results, we have improved the known NNLO inclusive cross-section, which was previously computed using an effective theory where fermions in the loop were integrated out. We find that finite fermion mass effects on the inclusive cross-section are relatively small, reaching only a few percent even for large pseudoscalar masses.",
    "Identifying outlying elements in probability distributions can be a challenging problem. To illustrate this, consider the Voting Rights Act enforcement issue of maximizing the number of simultaneous majority-minority districts in a political districting plan. Unbiased random walks on districting plans are unlikely to find optimal solutions. A common approach is to use biased random walks, which favor plans with more majority-minority districts. We propose an alternative method, called short bursts, which involves performing an unbiased random walk for a limited number of steps (burst length), then restarting from the most extreme plan encountered in the previous burst. Our empirical results show that short-burst outperforms biased random walks in maximizing majority-minority districts, with improvements observed across various burst lengths. We also explore the effectiveness of short bursts in more complex state spaces with different probability distributions, abstracting from our use case.",
    "This study employs molecular dynamics simulations to investigate the wetting properties of graphitic surfaces in contact with various solutions containing 1-8 wt% of commercially available non-ionic surfactants with long hydrophilic chains (up to 160  in length). To efficiently simulate these systems without compromising computational efficiency, a coarse-grained model is necessary. The MARTINI force field with polarizable water is particularly well-suited for this purpose, offering advantages such as faster exploration of long time scales and broader applicability. While the model's accuracy has been questioned, it accurately predicts the wetting properties of pure water on graphitic surfaces, consistent with atomistic simulations and theoretical predictions. However, the model overestimates the micellar formation process in aqueous surfactant solutions, which can be mitigated by initializing surfactants near the contact line. Although simulated equilibrium contact angles exceed experimental values, this study provides valuable guidelines for preliminary surfactant assessment and screening.",
    "We summarize recent experiments on superfluid $^3$He in nanofluidic sample chambers, highlighting the challenges overcome and the methods developed. These advances enable a systematic study of $^3$He film superfluidity and the surface and edge excitations of topological superfluids.",
    "Breaking Language Barriers: Unlocking the Power of Code-Mixed Machine Translation in Multilingual Communities\n\nIn today's diverse linguistic landscape, code-mixed machine translation has emerged as a crucial task, enabling seamless communication across languages. As part of the WMT 2022 shared tasks, we took on the challenge of developing a machine translation system that can effortlessly translate between English, Hindi, and Hinglish - a unique blend of both languages.\n\nOur innovative approach tackled two critical tasks: translating monolingual English and Hindi to code-mixed Hinglish, and vice versa. Notably, our system handled both Roman and Devanagari scripts, leveraging monolingual data in both languages. The second task focused solely on Roman script data.\n\nWe're thrilled to report that our system achieved top-tier ROUGE-L and WER scores for the first task, setting a new benchmark for monolingual to code-mixed machine translation. This paper delves into the details of our winning approach, which combined the power of mBART with specialized pre-processing and post-processing techniques, including transliteration from Devanagari to Roman. We also share our experiments for the second task, successfully translating code-mixed Hinglish to monolingual English.",
    "Recent advances in contrastive learning have demonstrated significant potential in self-supervised spatio-temporal representation learning. However, existing approaches often rely on naive sampling of different clips to construct positive and negative pairs, which inadvertently introduces a bias towards the background scene. This phenomenon can be attributed to two primary factors. Firstly, scene differences tend to be more pronounced and easier to distinguish than motion differences. Secondly, clips extracted from the same video often share similar backgrounds but exhibit distinct motions, leading to the model prioritizing the static background over the motion pattern when these clips are treated as positive pairs. To address this challenge, we propose a novel dual contrastive formulation. Specifically, we decompose the input RGB video sequence into two complementary modalities, namely static scene and dynamic motion. Subsequently, the original RGB features are attracted to the static features and the aligned dynamic features, respectively, thereby enabling the simultaneous encoding of static scene and dynamic motion into a compact RGB representation. Furthermore, we employ feature space decoupling via activation maps to distill static- and dynamic-related features. We refer to our approach as Dual Contrastive Learning for Spatio-Temporal Representation (DCLR). Comprehensive experiments validate that DCLR learns effective spatio-temporal representations, achieving state-of-the-art or comparable performance on the UCF-101, HMDB-51, and Diving-48 datasets.",
    "The electronic band structure and Fermi surfaces of ferromagnetic CeRh3B2 are calculated using the FLAPW and LSDA+U methods. By assuming various ground states to describe the 4f electronic state, we propose a fully orbital- and spin-polarized state, |lz=0, sx=1/2>, as the ground state, deviating from the conventional LS-coupled CEF ground state typically expected in 4f compounds. This proposal is supported by the fact that both the observed magnetic moment and dHvA frequencies are well-explained by the calculated electronic structure and Fermi surfaces. The unconventional ground state is stabilized by the strong 4f-4f direct mixing between neighboring Ce atoms along the extremely short distance along the c-axis in the hexagonal crystal cell.",
    "Simulating matrix acidization in porous media is challenging due to changing porosity. The improved DBF framework can simulate this process by discretizing mass and momentum conservation equations into a pressure-velocity linear system. However, this system can only be solved using direct solvers, which are inefficient for large-scale simulations. To overcome this, we propose a decoupled scheme that breaks down the linear system into two independent systems, one for pressure and one for velocity. These systems can be solved using parallel and iterative solvers, ensuring large-scale simulations can be completed within a reasonable time frame. A numerical experiment demonstrates the correctness and improved efficiency of the decoupled scheme.",
    "When it comes to understanding the world, two closely linked concepts come into play: sensemaking and narrative. Sensemaking is the way we organize and connect new information with what we already know and have inferred from past experiences. Narratives are a key outcome of this process, providing a more comprehensive view of the world than any single piece of information could on its own. Both are essential to how humans make sense of their surroundings, and would be equally valuable for a computational system trying to do the same. This paper explores the theories behind sensemaking and narrative, and how they help people build a understanding of the world based on the information they encounter. We also examine the connections between sensemaking and narrative research, and highlight the benefits of incorporating these concepts into a specific computational task: visual storytelling. Finally, we describe our system for visual storytelling, which leverages sensemaking and narrative, and share examples from its current implementation.",
    "The current evaluation metrics used in natural language generation (NLG) are flawed because they are not robust to dialect variations, making it difficult to determine how well systems perform for diverse user groups. In fact, these metrics may even penalize systems for producing text in less common dialects. Unfortunately, there is no existing method to measure how metrics respond to changes in dialect. To address this issue, we define dialect robustness and dialect awareness as key objectives for NLG evaluation metrics. We develop a set of methods and statistical tests to evaluate metrics against these goals. Our analysis reveals that current state-of-the-art metrics are not dialect-robust and are more sensitive to semantic changes than dialect features. To overcome this limitation, we propose a novel training approach called NANO, which incorporates regional and language information into the pretraining process of a metric. We demonstrate that NANO provides an efficient way to improve dialect robustness while enhancing performance on standard metric benchmarks.",
    "Geographic routing uses node position information to improve routing in sensor networks. However, its applicability has been a major challenge. Current approaches either make unrealistic assumptions about wireless networks or use expensive methods to simplify the communication graph. \n\nWe aim to answer three key questions: \n\n1. When should we use geographic routing?\n2. How do we determine if a network is suitable for geographic routing?\n3. In what situations does geographic routing make sense?\n\nTo address these questions, we identify four core principles of geographic routing and explore their impact on network topology. We then introduce a measure called geographic eccentricity to evaluate a network's fitness for geographic routing. Finally, we propose a distributed algorithm that either enables geographic routing or determines if the network's eccentricity is too high.",
    "Unlocking the secrets of superconductivity, our research reveals that two-band models exhibit a rich tapestry of spatial variation and correlation, governed by two distinct characteristic lengths. This complexity is a significant departure from the simpler dynamics of one-band systems. Notably, our findings show that short-range correlations persist even in the vicinity of the phase transition point, offering new insights into the intricate behavior of two-band superconductors.",
    "**Key Takeaways:**\n\n* We introduce online prediction methods for time series that can effectively handle nonstationary artifacts like trend and seasonality.\n* Applying appropriate transformations to time series before prediction can significantly improve theoretical and empirical prediction performance.\n* Our proposed method, NonSTOP (NonSTationary Online Prediction), is a fully online approach that can handle seasonality, trends, and cointegration in univariate and multivariate time series.\n* Our algorithms and regret analysis build upon recent related work and expand the applicability of such methods.\n* We provide sub-linear regret bounds with relaxed assumptions and support our results with experiments on simulated and real data.\n\nLet me know if you'd like me to make any further adjustments!",
    "Breakthrough in Logic Program Termination: A Game-Changing Heuristic Framework\n\nSay goodbye to the limitations of traditional termination and non-termination proof approaches! Our innovative framework offers a fresh alternative, introducing the concept of termination prediction. This cutting-edge method accurately predicts whether a logic program will terminate, even when conventional proof methods fall short.\n\nWe've made a significant leap forward by establishing a comprehensive characterization of infinite SLDNF-derivations, applicable to a wide range of queries. Our algorithm successfully predicts termination for general logic programs with arbitrary non-floundering queries.\n\nBut don't just take our word for it! Our termination prediction tool has yielded outstanding experimental results. In a benchmark test of 296 programs from the Termination Competition 2007, our tool achieved a remarkable 100% accuracy rate, outperforming state-of-the-art analyzers like AProVE07, NTI, Polytool, and TALP. In fact, our tool successfully predicted termination for 18 programs that were previously unsolvable by these leading analyzers. The only exceptions were five programs that exceeded the experiment time limit.",
    "Random forests have captured the imagination of researchers and practitioners alike, but despite their popularity, the underlying theory remains shrouded in mystery. In this groundbreaking paper, we shed new light on this complex topic, making two significant contributions to the field. First, we introduce a novel, theoretically sound variant of random regression forests, and rigorously prove its consistency. Then, we put our algorithm to the test, pitting it against other theoretically driven models and the industry-standard random forest algorithm. Our experiments yield fascinating insights into the trade-offs that theorists have made to create tractable models, revealing the secrets behind their success.",
    "We introduce a novel inference and learning algorithm for Factorial Hidden Markov Models (FHMMs) that overcomes the scalability limitations of traditional methods for sequential data analysis. By combining insights from stochastic variational inference, neural networks, and copula theory, our approach eliminates the need for message passing between latent variables, enabling efficient distributed computing and accelerated learning. Our experimental results demonstrate that our algorithm maintains the accuracy of established structured mean-field methods while outperforming them on long sequences and large FHMMs.",
    "To better understand how liquids respond to strong external electric fields, molecular dynamics simulations were conducted on pure water, sodium chloride solutions, and polymer solutions. The goal was to uncover the molecular-level mechanisms behind the formation of liquid bridges and jets, which are crucial in nanofiber production. The simulations revealed that, in the resulting nanoscale structures, molecules align their dipole moments parallel to the applied field throughout the sample. However, the presence of ions can disrupt this alignment, causing the structure to break down into droplets. The threshold field strength required to maintain a stable liquid column was found to depend on ion concentration. Additionally, the simulations observed conformational changes in the polymer during the jetting process.",
    "In the fast-paced world of content recommendation, one major hurdle remains: matching new users with content that resonates with their unique tastes. As podcasting continues to explode in popularity, traditional approaches to tackling the cold-start problem are put to the test. We dive into the challenge of applying these methods to Spotify's vast library of over 200,000 podcasts, using music consumption behavior as a key to unlocking user preferences. The results are striking: our techniques yield a significant 50% boost in consumption across both offline and online experiments. But that's not all - we also delve into the performance of our models and explore the crucial question: to what extent does music data influence the recommendations, and how can we mitigate bias?",
    "We investigate the Casimir energy and entropy of two ideal metal spheres at both large and small distances. Our results show that the Helmholtz free energy exhibits non-monotonic behavior with respect to separation and temperature, resulting in regions with negative entropy. Furthermore, we observe non-monotonic entropy behavior with temperature and sphere separation. We explore the origins of this anomalous entropy behavior and its implications for thermodynamics.",
    "Real-world problems often have high-dimensional or continuous action spaces, making it impossible to evaluate all possible actions. This paper proposes a framework for policy evaluation and improvement using sampled action subsets, applicable to any policy iteration-based reinforcement learning algorithm. We introduce Sampled MuZero, an extension of MuZero that learns in complex action spaces by planning over sampled actions, and demonstrate its effectiveness in Go and two continuous control benchmark domains.",
    "We introduce two new beamforming methods that use a deep neural network (DNN) trained with multichannel loss functions. Unlike traditional methods that use loss functions designed for monaural speech enhancement, our approach directly optimizes the performance of mask-based beamforming. We achieve this by using multichannel loss functions that evaluate the estimated spatial covariance matrices based on the multichannel Itakura-Saito divergence. Our experiments show that DNNs trained with these loss functions can effectively construct various beamformers and are robust to different microphone configurations.",
    "Nano-FTIR imaging is a cutting-edge technique that combines FTIR spectroscopy and s-SNOM microscopy to achieve nanometer-scale spatial resolution. However, its sequential data acquisition process limits its ability to capture large spatial areas, resulting in lengthy measurement times. To address this issue, various mathematical approaches have been proposed, all of which rely on acquiring only a small fraction of randomly selected measurements. While these approaches show promise, randomly selecting measurements can be impractical for scanning procedures and may not yield the desired time savings. This study explores alternative sub-sampling schemes that can accelerate data acquisition. The results show that several sub-sampling methods, including original Lissajous, triangle Lissajous, and random reflection sub-sampling, can produce comparable results to random sub-sampling at a 10% rate. This suggests that random sub-sampling is not necessary for efficient data acquisition, opening up new possibilities for faster and more practical nano-FTIR imaging.",
    "**Key Findings:**\n\n* We calculated screening masses in the deconfined phase of (3+1)d SU(3) pure gauge theory at finite temperature near the transition point, for two different channels of angular momentum and parity.\n* We compared the ratio of these screening masses to those of massive excitations with the same quantum numbers in the 3d 3-state Potts model in the broken phase near the transition point at zero magnetic field.\n* We also studied the inverse decay length of correlations between the real and imaginary parts of the Polyakov loop and compared the results to expectations from perturbation theory and mean-field Polyakov loop models.",
    "Imagine having a superpower that helps you detect anomalies and threats in complex systems with uncanny accuracy. That's exactly what the Mahalanobis distance-based confidence score has achieved in the realm of pre-trained neural classifiers. This innovative method has been hailed as a game-changer, outperforming its peers in detecting both out-of-distribution and adversarial examples.\n\nBut here's the fascinating part: despite its impressive track record, the Mahalanobis method operates under an assumption that seems too good to be true - that the class conditional distributions of pre-trained features have tied covariance. So, what's behind its remarkable success in real-world scenarios?\n\nOur investigation reveals a surprising twist: the method's strength doesn't stem from its intended purpose of classification prediction confidence, but rather from unrelated information. This means that the conventional wisdom about why the Mahalanobis confidence score works so well is actually misguided.\n\nMoreover, we discovered that this method taps into different insights than another popular anomaly detection approach, ODIN, which relies on prediction confidence. This realization inspired us to combine the two methods, resulting in a powerful detector that boasts improved performance and robustness.\n\nOur findings offer a profound understanding of how neural classifiers respond to anomalous inputs, shedding light on the intricate dynamics at play. By uncovering the secrets behind the Mahalanobis method's success, we can unlock even more effective ways to safeguard our systems against unknown threats.",
    "Certain algorithms, like those used to differentiate mathematical expressions, work with the underlying structure of the expressions in a way that makes mathematical sense. To formally describe such an algorithm, we need to specify three things: how it works computationally, what it means mathematically, and how to apply it to actual expressions. To achieve this, we need to combine reasoning about the syntax (or structure) of the expressions with reasoning about their meaning.\n\nA syntax framework is a mathematical tool that helps us reason about syntax. It consists of four parts: a way to map expressions to their syntactic structures, a language to reason about these structures, a way to refer to the structure of an expression, and a way to evaluate the expression represented by a structure.\n\nWe compare two approaches to formalizing a mathematical algorithm that relies on syntax. In the first approach, we use a formal theory T to define the syntactic structures of the expressions the algorithm works with. We then use a higher-level theory to define how to quote (or refer to) these structures and how to evaluate them. In the second approach, every expression in T is represented by a syntactic structure, and we define quotation and evaluation as operators within T itself.",
    "This study explores the dynamics of two consumer-resource systems that interact with each other, modeled using chemostat-like equations. The key assumption is that the resource dynamics unfold at a much slower pace than those of the consumer. This disparity in time scales enables a comprehensive analysis of the system. By separating the fast-scale consumer variables from the slow-scale resource variables, we can examine their phase planes independently. In isolation, each pair exhibits a unique, asymptotically stable steady state, without self-sustaining oscillations (although damped oscillations around equilibrium are possible). However, when the consumer-resource pairs are weakly linked through direct reciprocal inhibition, the entire system displays self-sustaining relaxation oscillations with periods significantly longer than the intrinsic relaxation time of either pair. Notably, the model equations effectively capture the behavior of diverse, locally linked consumer-resource systems, including populations experiencing interspecific interference competition and coupled lasers with cavity losses.",
    "Despite advances in technology, wireless local area networks (WLANs) still struggle with a significant performance gap between users in the uplink, primarily due to the varying channel conditions inherent in the wireless medium. To address this issue, cooperative medium access control (MAC) protocols, such as CoopMAC, have been proposed. This work reveals that cooperation among nodes involves a critical tradeoff between throughput and bit-cost, or the energy required to transmit a single bit, which is influenced by the level of cooperation. For carrier sense multiple access (CSMA) based networks, we derive the throughput/bit-cost tradeoff curve theoretically. We also introduce fairMAC, a novel distributed CSMA protocol, and demonstrate that it can theoretically achieve any operating point on the tradeoff curve as packet lengths approach infinity. Our theoretical findings are validated through rigorous Monte Carlo simulations.",
    "Unlock the Power of Social Tagging: Revolutionizing Web Search and Navigation\n\nImagine a world where searching and navigating the web becomes a breeze. Social tagging is making this a reality by harnessing the collective power of users to categorize and describe online resources. This collaborative approach generates a weighted list of tags that accurately capture the essence of each resource.\n\nWhen combined with traditional classification systems like Wikipedia, social tags open up new avenues for exploration. They offer alternative navigation methods, such as pivot-browsing, popularity-driven navigation, and filtering, making it easier to discover hidden gems. Moreover, social tags provide valuable metadata that can significantly enhance document search, often uncovering insights that lie beyond the content itself.\n\nIn this innovative project, we propose integrating a user-friendly interface for adding custom tags to Wikipedia articles. This game-changing feature has the potential to transform article navigation and retrieval. To test its effectiveness, we've developed a prototype that applies social tagging to Wikipedia. Get ready to experience the future of web search and navigation!",
    "Imagine you're trying to find a specific face in a crowded stadium. Classical computers would look at each face one by one, but a quantum computer can process multiple faces simultaneously, making it much faster. This powerful technology, known as Quantum Machine Learning, has gained massive attention worldwide in a short span. Researchers have proposed numerous models that apply quantum principles to classify patterns, but there's a significant gap - most of these models are only tested on artificial data, not real-world datasets. This project aims to bridge that gap by using a quantum classifier to identify patterns with binary attributes. We've successfully applied this classifier to image datasets, achieving promising results, especially in cases where one class is more important than the others, like in medical diagnosis where the rare condition is often the most critical.",
    "Our investigation focuses on the shock-cloud interaction zone in the southern region of the supernova remnant HB 21, leveraging near- and mid-infrared data collected using the InfraRed Camera (IRC) on the AKARI satellite and the Wide InfraRed Camera (WIRC) at the Palomar 5 m telescope. The IRC images, spanning 4 um (N4), 7 um (S7), and 11 um (S11) bands, as well as the WIRC H2 v=1->0 S(1) 2.12 um image, reveal similar diffuse patterns surrounding a shocked CO cloud. By comparing the emission to H2 line emission from various shock models, we analyzed the data. The IRC colors are consistent with the thermal admixture model, which describes the H2 gas with a power-law relation between infinitesimal H2 column density and temperature, dN  T^(-b)dT. Our findings indicate n(H2)  3.9  10^4 cm^(-2), b  4.2, and N(H2; T > 100K)  2.8  10^21 cm^(-2). We interpreted these parameters in the context of multiple shock-cloud interaction scenarios, including planar C-shocks, bow shocks, and shocked clumps, discussing their strengths and limitations. Notably, the observed H2 v=1->0 S(1) intensity exceeds the prediction from the power-law admixture model by a factor of four, mirroring the trend observed in the northern part of HB 21 (Paper I). Furthermore, we explored the limitations of the thermal admixture model in relation to the derived model parameters.",
    "Imagine a world where AI models can efficiently process visual data on your smartphone or other resource-constrained devices. Recently, vision transformers have made tremendous progress, outperforming large convolution-based models in many areas. However, when it comes to small models for mobile devices, ConvNets still hold their own, offering a perfect balance of performance and simplicity.\n\nThat's why we're excited to introduce ParC-Net, a game-changing ConvNet-based backbone model that combines the best of both worlds. By fusing the strengths of vision transformers into ConvNets, we've created a powerful and efficient model that's perfect for mobile and resource-constrained devices.\n\nAt the heart of ParC-Net lies our innovative position-aware circular convolution (ParC) operation. This lightweight convolution technique offers a global receptive field while producing location-sensitive features, just like local convolutions. By combining ParC with squeeze-excitation operations, we've created a meta-former-like model block that incorporates attention mechanisms similar to those found in transformers. The best part? This block can be easily plugged into existing ConvNets or transformers, making it a seamless upgrade.\n\nBut don't just take our word for it  our experiments show that ParC-Net outperforms popular lightweight ConvNets and vision transformer-based models in common vision tasks and datasets, all while requiring fewer parameters and offering faster inference speeds. For example, on ImageNet-1k, ParC-Net achieves an impressive 78.6% top-1 accuracy with just 5.0 million parameters, saving 11% parameters and 13% computational cost compared to MobileViT, while also offering 0.2% higher accuracy and 23% faster inference speed. And when compared to DeIT, ParC-Net uses only half the parameters while gaining 2.7% accuracy.\n\nBut that's not all  ParC-Net also excels in MS-COCO object detection and PASCAL VOC segmentation tasks. Want to try it out for yourself? Our source code is available on GitHub at https://github.com/hkzhang91/ParC-Net.",
    "Imagine you're trying to solve a puzzle with a complex equation: x^n - x + t = 0. We've cracked the code for when n is 2, 3, or 4, and surprisingly, the answers involve special functions called hypergeometric functions. By building on these solutions and using some clever math tricks, we've discovered new ways to simplify these special functions and even calculate some tricky infinite integrals using basic math operations. It's like finding a hidden key to unlock new secrets in the world of mathematics!",
    "Air-gap covert channels are a type of secret communication method that allows attackers to steal data from computers that are not connected to a network. Over the years, researchers have demonstrated various types of air-gap covert channels, including those that use electromagnetic, magnetic, acoustic, optical, and thermal signals. In this paper, we're introducing a new type of vibrational covert channel that takes advantage of the vibrations generated by a computer's internal fans. \n\nWe've discovered that a computer's vibrations are correlated with the speed of its fans, and these vibrations can affect the entire surface on which the computer is placed. Our method involves malware that can control these vibrations by regulating the fan speed. We've found that nearby smartphones can detect these vibrations using their built-in accelerometers, which can be accessed by any app without needing the user's permission. This makes the attack highly difficult to detect.\n\nWe've developed a malware called AiR-ViBeR that encodes data into a low-frequency vibrational signal, which can then be decoded by a malicious app on a nearby smartphone. We discuss the attack model, provide technical background, and share the implementation details and evaluation results. Our results show that AiR-ViBeR can successfully exfiltrate data from an air-gapped computer to a nearby smartphone on the same table, or even an adjacent table, using vibrations. Finally, we propose some countermeasures to prevent this type of attack.",
    "A comprehensive cost analysis of a 25 W average load magnetic refrigerator utilizing commercial-grade Gadolinium (Gd) is conducted via a numerical modeling approach. The calculation takes into account the expenses associated with magnetocaloric material, magnet material, and operational costs, all of which contribute to the overall expenditure. The results indicate that the minimum total cost, assuming a 15-year device lifespan, falls within the range of $150-$400, largely dependent on the prices of magnetocaloric and magnet materials. Notably, the magnet cost dominates the overall expense, closely followed by operational costs, while the magnetocaloric material cost is relatively insignificant. To achieve the lowest cost, the optimal design parameters are identified as a magnetic field of approximately 1.4 Tesla, a particle size of 0.23 mm, a regenerator length of 40-50 mm, and a utilization rate of around 0.2, which remain consistent across various device lifetimes and material/magnet prices. The operating frequency, however, varies as a function of device lifetime. The performance characteristics of the magnetic refrigeration device are benchmarked against those of a conventional A+++ refrigeration unit, revealing comparable lifetime costs, with the magnetic refrigeration device being slightly more cost-effective, assuming the magnet cost can be recovered at the end of its life.",
    "We establish the existence of initial data sets that combine an asymptotically flat end with an asymptotically cylindrical end, a geometry commonly referred to as \"trumpets\" in the numerical relativity community.",
    "**Key Discovery: Enzymes Harness Quantum Effects to Boost Chemical Reactions**\n\nEnzymes have evolved unique protein structures to accelerate complex chemical transformations. Our research reveals a crucial mechanism by which the enzyme ketosteroid isomerase (KSI) exploits quantum effects to enhance its catalytic power.\n\n**Triad of Tyrosine Residues Enables Quantum Proton Delocalization**\n\nWe used experiments and advanced simulations to show that a triad of strongly hydrogen-bonded tyrosine residues in KSI's active site facilitates quantum proton delocalization. This delocalization dramatically stabilizes the deprotonation of an active site tyrosine residue, resulting in a significant isotope effect on its acidity.\n\n**Extended Quantum Proton Delocalization in the Active Site**\n\nWhen an intermediate analog is docked, it integrates into the hydrogen bond network, leading to extended quantum proton delocalization in the active site. Our findings illuminate the critical role of nuclear quantum effects in the hydrogen bond network that stabilizes KSI's reactive intermediate and shed light on proton behavior in biological systems with strong hydrogen bonds.",
    "We introduce ENSEI, a secure inference framework that enables efficient privacy-preserving visual recognition. By combining homomorphic encryption and secret sharing, we can perform homomorphic convolution in the frequency domain, greatly simplifying the computations involved. \n\nOur approach builds on the frequency-domain secure convolution (FDSC) protocol, which we have designed and optimized using number-theoretic transforms (NTTs). We evaluate the trade-offs between time- and frequency-domain homomorphic convolution in our experiments, demonstrating the advantages of our approach.\n\nThe results are significant: ENSEI achieves 5-11x online time reduction, up to 33x setup time reduction, and up to 10x reduction in overall inference time compared to the best known works. Furthermore, we show that binary neural networks can benefit from an additional 33% bandwidth reduction with only 1% accuracy degradation on the CIFAR-10 dataset.",
    "Recommender systems are a lifesaver when it comes to dealing with information overload. They help us figure out what we might like from a sea of niche options. Over the years, lots of personalized recommendation algorithms have been developed, and most of them rely on similarities, like collaborative filtering and mass diffusion. We're proposing a new way to measure vertex similarity, called CosRA, which combines the best of both the cosine index and the resource-allocation (RA) index. When we tested CosRA on real-world recommender systems like MovieLens, Netflix, and RYM, we found that it performed better than some benchmark methods in terms of accuracy, diversity, and novelty. Plus, CosRA doesn't require any tweaking of parameters, which makes it super useful in real-life applications. We also experimented with adding two adjustable parameters, but it didn't make a huge difference in CosRA's performance.",
    "As the need to handle rapidly changing data grows, many real-world applications rely on multi-label data streams. However, these data streams often experience changes in distribution, known as concept drift, which can quickly render existing classification models ineffective. To address this issue, we introduce a new algorithm called Label Dependency Drift Detector (LD3), which is an unsupervised concept drift detector that leverages label dependencies within the data to identify changes in multi-label data streams. Our approach utilizes a label influence ranking method that captures dynamic temporal dependencies between labels, and uses a data fusion algorithm to detect concept drift. Notably, LD3 is the first unsupervised concept drift detection algorithm designed specifically for multi-label classification problems. We evaluate LD3's performance by comparing it to 14 supervised concept drift detection algorithms, adapted for the multi-label problem area, using 12 datasets and a baseline classifier. The results show that LD3 outperforms comparable detectors by 19.8% to 68.6% in terms of predictive performance, on both real-world and synthetic data streams.",
    "The long-standing debate over the universality of Cepheid Period-Luminosity relations has centered on the potential impact of metallicity effects on the intercept and slope of these relations. This study aims to resolve this issue by calibrating the Galactic Period-Luminosity relations across multiple photometric bands (B to K) and comparing the results to the well-established relations in the Large Magellanic Cloud (LMC). Leveraging a robust dataset of 59 calibrating stars with distances measured using five independent methods, we provide a detailed analysis of absorption corrections and projection factors. Our findings reveal no significant difference in the slopes of the Period-Luminosity relations between the LMC and our Galaxy, conclusively demonstrating the universality of these relations across all photometric bands and galaxies (at least for the LMC and Milky Way). While the potential impact of metal content on zero-point variation is beyond the scope of this study, our data suggest an upper limit of 18.50 for the LMC distance modulus.",
    "It is a well-established fact that ensembling methods significantly enhance prediction accuracy. However, a crucial limitation of these methods is their inability to effectively distinguish between component models. This paper presents a novel approach to stacking with auxiliary features, which enables the fusion of relevant information from multiple systems to achieve superior performance. The incorporation of auxiliary features empowers the stacker to not only consider systems that agree on an output, but also to take into account the provenance of that output. We validate our approach through rigorous testing on three disparate and challenging problems: Cold Start Slot Filling, Tri-lingual Entity Discovery and Linking, and ImageNet object detection tasks. Our results yield new state-of-the-art performance on the first two tasks and substantial improvements on the detection task, unequivocally demonstrating the efficacy and broad applicability of our methodology.",
    "We've developed a quick and easy way to simulate how magnetization changes in tiny spin-valve structures when driven by spin-torque. Our method combines two tools: one that models spin transport and another that models magnetization dynamics. This approach considers how both spin transport and magnetization dynamics vary in space. We compared our results to real-life experiments and found that they match well. Our method accurately predicts the behavior of spin waves, including the current needed to sustain steady magnetization and the frequency changes of the waves. It also correctly models the giant magnetoresistance effect and magnetization switching. We also discuss the similarities with recently developed spin-caloritronics devices.",
    "We introduce a straightforward framework for computing hyperbolic Voronoi diagrams of finite point sets, leveraging affine diagrams. Our approach is grounded in the theoretical foundation that bisectors in Klein's non-conformal disk model can be represented as hyperplanes, equivalent to power bisectors of Euclidean balls. Consequently, our method involves a two-stage process: first, computing an equivalent clipped power diagram, followed by a mapping transformation dependent on the chosen representation of hyperbolic space (such as the Poincar conformal disk or upper-plane representations). We also explore extensions of this approach to accommodate weighted and k-order diagrams, and describe their corresponding dual triangulations. Furthermore, we examine the application of two essential primitives on hyperbolic Voronoi diagrams in the context of designing customized user interfaces for an image catalog browsing application in the hyperbolic disk: (1) identifying nearest neighbors, and (2) calculating smallest enclosing balls.",
    "This study examines the problem of diffusive bond dissociation in a double well potential under the influence of an external force. We calculate the probability distribution of rupture forces and provide a detailed analysis of how finite rebinding probabilities affect the dynamic force spectrum. Our focus is on barrier crossing during extension (under linearly increasing load) and relaxation from completely separated bonds. At high loading rates, the rupture force and rejoining force exhibit the expected loading rate dependence, determined by the potential shape. At low loading rates, the mean forces obtained from pull and relax modes converge as the system reaches equilibrium. We investigate how external parameters, such as cantilever stiffness and the presence of a soft linker, influence the rupture force distributions and mean rupture forces. Our results show that the equilibrium rupture force is either unaffected by the linker or changes predictably with linker compliance, depending on the linker implementation. Furthermore, we demonstrate that the equilibrium constant of on- and off-rates can be extracted from the determination of equilibrium rupture forces.",
    "We introduce a novel approach to identify viscous and turbulent flow regions, including boundary layers and wakes, by utilizing an invariant feature space. Our methodology leverages the principal invariants of strain and rotational rate tensors as inputs to an unsupervised Gaussian mixture model, which is a type of machine learning algorithm. The key advantage of this approach is that it is coordinate-frame independent, as it relies on Galilean-invariant principal invariants of strain and rotational rate. This enables the distinction between two distinct flow regions: a viscous, rotation-dominated region (encompassing boundary layers and wakes) and an inviscid, irrotationally-dominated region (characterizing the outer flow). We validate our methodology by applying it to both laminar and turbulent flow cases (simulated using Large Eddy Simulation) past a circular cylinder at Reynolds numbers of 40 and 3900. The simulations were performed using a high-order nodal Discontinuous Galerkin Spectral Element Method. Our results demonstrate the effectiveness of Gaussian mixture clustering in identifying viscous and rotational flow regions. Furthermore, we compare our approach to traditional sensors, highlighting that our method eliminates the need for arbitrary threshold selection, which is a limitation of traditional sensors.",
    "Quantum systems that we design and build ourselves let us explore some pretty cool phenomena that don't happen naturally. Superconducting circuits are like LEGO blocks, making it easy to create and connect artificial atoms. We've created an artificial molecule by linking two strongly connected fluxonium atoms, which has a magnetic moment that we can adjust. By applying an external magnetic field, we can switch the molecule between two modes: one where it has a magnetic dipole moment and one where it has a magnetic quadrupole moment. When we tweak the external field, we find that the molecule's coherence is limited by local noise. Being able to engineer and control these artificial molecules opens the door to building more complex circuits for super-secure qubits and simulating quantum systems.",
    "Imagine racing against the clock to make high-stakes decisions that involve a series of complex tasks and unpredictable outcomes. Our method tackles this challenge by harnessing the power of iterative refinement routines, each tackling a different facet of the decision-making puzzle. But here's the catch: we need to orchestrate these routines like a conductor leading an orchestra, allocating precious computational resources to ensure the right notes are played at the right time. This paper shines a spotlight on the master control problem of deliberation scheduling, where every second counts. We present a range of models that capture the diverse scenarios and computational strategies for making decisions under intense time pressure. We explore two approaches: precursor models, where all decisions are made before the clock starts ticking, and recurrent models, where decisions are made on the fly, adapting to new information and anticipating what's to come. We outline algorithms for both approaches and share the results of our real-world experiments, providing a glimpse into the cutting-edge of time-critical decision making.",
    "A novel approach to designing estimators is presented, known as the role model strategy. This method involves mimicking the output of a superior estimator that has access to better input observations. Under certain Markov conditions, this strategy is proven to produce the optimal Bayesian estimator. The strategy is demonstrated through two simple channel examples. Additionally, it is combined with time averaging to create a statistical model by solving a convex program numerically. Originally developed for low-complexity decoder design in iterative decoding, the role model strategy has potential applications beyond the field of communications.",
    "Current computer vision systems struggle to accurately recognize objects in artistic representations, such as paintings, cartoons, or sketches, especially when there is limited data available. To address this challenge, we propose a novel method that can recognize objects in artistic modalities without requiring any labeled data from those specific domains. Our approach takes into account the stylistic differences between and within domains, ensuring that the system learns features that are consistent across different artistic styles.\n\nTo achieve this, we create a complementary training modality that mimics the artistic style of the target domain. We then train the network to learn features that are invariant between the two training modalities. Notably, we can generate these artificial labeled source domains automatically using style transfer techniques, which involve applying the style of diverse target images to represent the target domain's style.\n\nUnlike existing methods that require a large amount of unlabeled target data, our approach can work effectively with as few as ten unlabeled images. We evaluate our method on various cross-domain object and scene classification tasks, as well as on a new dataset we are releasing. Our experiments demonstrate that our approach, although conceptually simple, significantly outperforms existing domain adaptation techniques in recognizing objects in artistic representations.",
    "Delve into the fascinating realm of semi-linear Cauchy problems with quadratic nonlinearity in gradients, where the large time behavior of solutions takes center stage. This paper tackles the complex Cauchy problem with a general state space that may degenerate on its boundary, yielding two striking types of large time behavior: pointwise convergence of the solution and its gradient, and convergence of solutions to associated backward stochastic differential equations. When the state space is R^d or the space of positive definite matrices, we uncover both types of convergence under growth conditions on model coefficients. The far-reaching implications of these large time convergence results have significant applications in risk sensitive control and long term portfolio choice problems, opening up new avenues for exploration and discovery.",
    "Revolutionizing our understanding of dark energy, the decaying vacuum model (DV) has garnered significant attention in recent studies. By treating dark energy as a dynamic vacuum, the DV model reveals a striking linear decay of vacuum energy with the Hubble parameter in the late universe, resulting in an additional matter component. Leveraging a comprehensive suite of recent datasets - including supernovae, gamma-ray bursts, baryon acoustic oscillations, CMB, Hubble rate, and x-rays in galaxy clusters - we constrain the DV model's parameters with unprecedented precision. Our findings are nothing short of remarkable: the best-fit matter density contrast m in the DV model far surpasses that of the CDM model. We present the confidence contours in the m-h plane up to 3 confidence level, as well as the normalized likelihoods of m and h, respectively. The implications are profound, challenging our current understanding of the cosmos and opening up new avenues for exploration.",
    "MgO-based Magnetic Tunnel Junctions (MTJs) with perpendicular magnetization are ideal components for building Spin Transfer Torque (STT) magnetoresistive memories. However, until now, STT alone has been insufficient to achieve the desired low switching current density of less than 10^6 A/cm^2. Recently, a study published in Nature Materials (Wang et al., 2012) demonstrated the possibility of magnetization switching at ultra-low current densities with the assistance of an electric field. While this phenomenon has been theoretically studied using a macrospin approach, we present a comprehensive micromagnetic analysis. Our findings reveal that the switching process involves a complex nucleation mechanism, including the formation of magnetic vortices.",
    "Imagine if we could break free from the limitations of traditional perceptron learning algorithms. What if we could harness the power of proximal activation functions to unlock new possibilities in machine learning? Our innovative approach does just that, by generalizing Rosenblatt's classic algorithm to accommodate a broader range of activation functions. But that's not all - we've also discovered that this generalization can be seen as a clever incremental gradient method, cleverly disguised as an energy minimization algorithm. The beauty of this approach lies in its simplicity: by leveraging a generalized Bregman distance, we can sidestep the need to differentiate the activation function, making the entire process more efficient and intuitive. And the best part? This new perspective opens the door to a wide range of novel algorithms, including a fresh take on the iterative soft-thresholding algorithm for sparse perceptrons - a game-changer for anyone looking to push the boundaries of machine learning.",
    "The phenomenon of radiation force exerted by acoustic waves on objects has been extensively researched since the pioneering work of Rayleigh, Langevin, and Brillouin, leading to significant advancements in acoustic micromanipulation over the past decade. However, previous studies have only derived expressions for the acoustic radiation force on a stationary particle, overlooking the impact of particle displacement on the radiated wave. This study investigates the acoustic radiation force on a translating monopolar source moving at a constant velocity much slower than the speed of sound. Our findings show that the Doppler effect-induced asymmetry in the emitted field generates a radiation force opposing the source's motion.",
    "Accurately modeling the base of the solar convective envelope poses a significant challenge. Since the discovery of rotation inversions, solar modelers have grappled with the reality that a narrow region has a profound impact on the Sun's overall dynamics. This critical zone, known as the tachocline, marks the transition from differential to solid body rotation and is also thought to be the source of the solar magnetic dynamo. Furthermore, it is influenced by turbulent forces. Notably, current solar models exhibit significant discrepancies with the observed sound speed profile in this region. This paper demonstrates how helioseismology can provide additional constraints on this region by inverting the Ledoux discriminant. We present a comparative analysis of inversions for Standard Solar Models constructed using different opacity tables and chemical abundances, shedding light on the origins of the discrepancies between solar models and the actual Sun.",
    "Unlocking the Secrets of Human Behavior: A New Era of Collaboration\n\nImagine a world where humans and artificial intelligence work together in perfect harmony. To make this vision a reality, we need to crack the code of human behavior. Research trends reveal a fascinating assumption: human reasoning is the gold standard for artificial intelligence. As a result, game theory, theory of mind, machine learning, and other disciplines are converging to replicate and understand human behavior.\n\nThe next generation of autonomous systems will rely on AI agents and humans working together as a cohesive unit. To achieve this, autonomous agents must be able to incorporate practical models of human behavior, enabling them to not only mimic human actions but also anticipate user behavior and work in tandem with them.\n\nThis paper sets out to provide a comprehensive review of the most influential approaches to modeling human behavior. We'll delve into two key areas: (i) techniques that learn from exploration and feedback, such as Reinforcement Learning, and (ii) direct modeling of human reasoning mechanisms, including beliefs and biases, without relying on trial and error. By exploring these approaches, we can unlock the secrets of human behavior and pave the way for a new era of human-AI collaboration.",
    "Tackling botnets has long been a significant hurdle. The resilience of command and control (C&C) channels has increased, making it more difficult to identify botmasters in peer-to-peer (P2P) botnets. This paper proposes a probabilistic approach to reconstructing the topologies of C&C channels in P2P botnets. Due to the geographical distribution of P2P botnet members, it is impossible to monitor all members, and there is a lack of necessary data for applying other graph reconstruction methods. Currently, there is no universal method for reconstructing C&C channel topologies for all types of P2P botnets. Our method estimates the probability of connections between bots by utilizing inaccurate receiving times from multiple cascades, network model parameters of the C&C channel, and end-to-end delay distribution of the Internet. These receiving times can be gathered by observing the external responses of bots to commands. Our simulation results show that over 90% of the edges in a 1000-member network with a mean node degree of 50 can be accurately estimated by collecting receiving times from 22 cascades. Even with receiving times from only half of the bots, this level of accuracy can be achieved with 95 cascades.",
    "In Grand Unified Theories (GUTs), the masses of gaugino particles may not be uniform at the unification scale, which can impact the detection of neutral Higgs bosons (h/H/A) at the Large Hadron Collider (LHC). This study examines the effects of non-uniform gaugino masses on Higgs boson production in a specific decay chain involving gluinos, squarks, and neutralinos. In the universal gaugino mass scenario, only the light Higgs boson can be produced in this decay chain, whereas non-uniform gaugino masses can lead to dominant production of heavy neutral Higgs bosons. The study also explores the allowed parameter space in light of WMAP constraints on dark matter relic density and demonstrates that combining representations can provide the required amount of dark matter in any parameter space point. Furthermore, it is shown that heavy Higgs bosons can be detected in the studied decay chain in regions with preferred neutralino relic density.",
    "Get ready to revolutionize integrated photonic-electronic circuits with the game-changing power of subwavelength modulators! Despite the challenges posed by weak light-matter interactions, we've cracked the code to designing a modulator that's not only incredibly compact (think nanometer scale footprint!) but also boasts low switching energy, minimal insertion loss, and a massive modulation depth. Introducing our groundbreaking vanadium dioxide dual-mode plasmonic waveguide electroabsorption modulator, built on a cutting-edge metal-insulator-VO$_2$-insulator-metal (MIVIM) waveguide platform! By harnessing the dynamic index of vanadium dioxide, our modulator can seamlessly route plasmonic waves through the low-loss dielectric insulator layer in the \"on\" state and the high-loss VO$_2$ layer in the \"off\" state, resulting in a staggering reduction in insertion loss while maintaining an enormous modulation depth. And the best part? This ultracompact waveguide modulator can achieve a whopping modulation depth of ~10dB with an active size of just 200x50x220nm$^3$ (or ~{\\lambda}$^3$/1700), all while requiring a mere drive-voltage of ~4.6V. This high-performance plasmonic modulator is poised to unlock the full potential of fully-integrated plasmonic nanocircuits in next-generation chip technology - the future is bright, and it's arriving faster than you think!",
    "As vehicles become increasingly connected, combating automobile theft has become a pressing concern. To address this issue, various countermeasures have been proposed, including data mining, biometrics, and additional authentication methods. Among these, data mining has emerged as an effective approach to capture the unique characteristics of owner-drivers. Previous studies have applied diverse algorithms to driving data to distinguish owner-drivers from thieves. However, these supervised learning-based methods require labeled datasets, which is impractical when it comes to gathering and applying thief driving patterns. To overcome this limitation, we propose a driver identification method utilizing Generative Adversarial Networks (GANs). The key advantage of GANs lies in their ability to build identification models using only owner-driver data. We trained a GAN solely with owner-driver data and employed the trained discriminator to identify the owner-driver. Our evaluation using actual driving data demonstrates that our identification model accurately recognizes owner-drivers. By integrating our proposed model with other driver authentication methods, we anticipate that the industry can develop effective automobile theft countermeasures for real-world implementation.",
    "Measuring electronic structure parameters in quasi-two-dimensional metals can be achieved through the convenient method of slow oscillations (SlO) of magnetoresistance. We explore the potential of applying this approach to multi-band conductors, such as iron-based high-temperature superconducting materials. Our findings indicate that SlO can effectively measure the interlayer transfer integral in multi-band conductors, similar to its application in single-band metals. Furthermore, SlO enables the measurement and comparison of effective masses and electron scattering rates across different bands.",
    "This review covers recent progress in precision LHC calculations for Standard Model processes, including weak gauge-boson and Higgs-boson production, as presented at the 2015 Rencontres de Blois.",
    "This paper presents a novel approach to speech emotion recognition that leverages both speech features and speech transcriptions (text). The speech features, including Spectrogram and Mel-frequency Cepstral Coefficients (MFCC), are effective in capturing low-level emotional characteristics in speech, while the text provides valuable insights into the semantic meaning of the spoken words. By combining these two sources of information, our approach can detect emotions more accurately. We experimented with various Deep Neural Network (DNN) architectures, each designed to process different combinations of speech features and text inputs. Our results show that the proposed network architectures outperform existing state-of-the-art methods on a benchmark dataset. Notably, the combined MFCC-Text Convolutional Neural Network (CNN) model achieved the highest accuracy in recognizing emotions in the IEMOCAP dataset.",
    "We propose integrating variational semantic memory into meta-learning to enable few-shot learning with long-term knowledge. This memory module accumulates and stores semantic information in a hierarchical Bayesian framework, growing from scratch and consolidating through task experience. It adapts to individual tasks through principled memory recall, allowing efficient accrual and adaptation of semantic information. Our approach outperforms deterministic methods, achieving state-of-the-art results on four benchmarks and demonstrating the benefits of variational semantic memory for few-shot recognition.",
    "Unlocking the Secrets of Tetraquarks: A Groundbreaking Approach to Unraveling their Mysteries\n\nIn a major breakthrough, we have successfully derived the relativistic four-quark equations that incorporate open-charm and open-strange components within the coupled-channel formalism. By dynamically mixing meson-meson states with four-quark states, we have constructed amplitudes that encompass all four quark flavors (u, d, s, c). The poles of these amplitudes hold the key to determining the masses of tetraquarks, and our calculations have yielded mass values for these exotic particles with spin-parity JP=1-,2-. This pioneering work opens up new avenues for understanding the properties and behavior of tetraquarks, shedding light on the mysteries of these enigmatic particles.",
    "Get ready to revolutionize your cosmological forecasting with the Fisher Matrix, the game-changing tool that's taking the scientific community by storm! We're thrilled to introduce Fisher4Cast, the ultimate Fisher Matrix framework that's not only ridiculously easy to use but also open source, rigorously tested, and packed with awesome features like a user-friendly GUI, automated LATEX file creation, and point-and-click Fisher ellipse generation. And the best part? It's designed to be super flexible, so you can easily extend it to fit your needs, and it's compatible with open-source alternatives like Octave and Scilab.\n\nBut that's not all! With Fisher4Cast, we're pushing the boundaries of cosmological forecasting with stunning 3-D and 4-D visualizations that will blow your mind. We're also diving deep into the impact of growth and curvature on future surveys, and we can't wait to share our findings with you.\n\nSince its early release in 2008, Fisher4Cast has been downloaded over 750 times, and we're excited to unveil Version 2.2, complete with a Quick Start guide and the code behind our amazing figures. We're confident that Fisher4Cast will become an indispensable tool for cosmologists and scientists everywhere, and we can't wait to see the incredible things you'll achieve with it!",
    "The formalism of knowledge representation grounded in first-order logic effectively captures the nuances of natural language and accommodates multiple probabilistic inference models. While symbolic representation facilitates quantitative reasoning with statistical probability, its integration with machine learning models, which operate on numerical computations, poses significant challenges. In contrast, knowledge embedding, which involves the use of high-dimensional and continuous vectors, offers a viable approach to complex reasoning, enabling the preservation of semantic information and the establishment of quantifiable relationships among knowledge entities. This paper proposes the Recursive Neural Knowledge Network (RNKN), a novel framework that combines medical knowledge grounded in first-order logic with recursive neural networks for multi-disease diagnosis. Following efficient training on manually annotated Chinese Electronic Medical Records (CEMRs), the RNKN model learns diagnosis-oriented knowledge embeddings and weight matrices. Experimental results demonstrate that the diagnostic accuracy of RNKN surpasses that of classical machine learning models and Markov Logic Networks (MLNs). Furthermore, the results indicate that the performance of RNKN improves as the explicitness of evidence extracted from CEMRs increases. Notably, the RNKN model exhibits increasingly interpretable knowledge embeddings as the number of training epochs increases.",
    "Deep within the electron cooler device, a series of solenoids work in tandem to precision-guide the electron beam. However, these solenoids have a hidden impact on the ion beam circulating in the adjacent storage ring. If not perfectly calibrated, the solenoids can disrupt the ion beam's transverse motion, causing it to become inextricably linked. This paper delves into the consequences of uncompensated solenoids in the CSRm storage ring at the Institute of Modern Physics in Lanzhou, China, and presents a novel method for calculating the resulting coupled beam envelopes.",
    "A near-infrared excess has been detected at the white dwarf PHL5038 in UKIDSS photometry, indicating the presence of a cool, substellar companion. \n\nUsing NIRI on Gemini North, we obtained H- and K-grism spectra and images of PHL5038. The target was resolved into two components: an 8000K DA white dwarf and a likely L8 brown dwarf companion, separated by 0.94\". \n\nThe spectral type of the secondary was determined using standard spectral indices for late L and T dwarfs. The projected orbital separation of the binary is 55AU, making it only the second known wide WD+dL binary, after GD165AB. \n\nThis object has the potential to be used as a benchmark for testing substellar evolutionary models at intermediate to older ages.",
    "\"Unveiling the Hidden Secrets of the Milky Way: How Dynamical Streams and Substructure Shape Our Understanding of Galaxy Mass\"\n\nIn a groundbreaking study, we delve into the mysteries of the Milky Way's mass and escape speed, revealing the profound impact of dynamical streams and substructure on our estimates. By harnessing the power of high-resolution, magneto-hydrodynamical cosmological simulations, we uncover the intricate phase space structure of local volumes around solar-like positions, exposing significant variations between positions within individual galaxies and across the suite.\n\nOur findings show that substructure unevenly populates the high-velocity tail, leading to discrepancies in mass estimates. A complex interplay of streams, sample noise, and truncation of the high-velocity tail below the escape speed gives rise to a distribution of mass estimates with a median that falls short of the true value by a staggering 20%, and a spread of a factor of 2 across the suite.\n\nBy correcting for these biases, we arrive at a revised estimate of the Milky Way's mass, presented in Deason et al., of 1.29  10^12 M, with a range of +0.37 to -0.47  10^12 M. This breakthrough sheds new light on the intricate dynamics of our galaxy, offering a more accurate understanding of the Milky Way's true nature.",
    "Unlocking the Power of Light: Revolutionary Object Recognition with a Twist\n\nImagine a world where a single photon can reveal more than just a single bit of information. Our innovative approach harnesses the potential of high-dimensional correlated orbital angular momentum (OAM) states to achieve unprecedented information extraction rates in object recognition.\n\nThe magic lies in the correlations, which remain unaffected by the target object's axial rotation. Like a fingerprint, the object's joint OAM coincidence spectrum remains unchanged, even when rotated randomly between measurements.\n\nBut that's not all. Our method can fully reconstruct complex, off-axis objects using OAM correlations alone, revealing novel symmetries in the phases of OAM-object interactions. We've also explored how mutual information rates vary with off-axis translation in the beam field, discovering that object symmetry signatures and information rates remain independent of environmental factors at a safe distance from the beam center.\n\nThe implications are profound, paving the way for dynamic scanning applications in scenarios where symmetry and minimal, non-invasive measurements are crucial. Get ready to unlock the secrets of light and revolutionize object recognition!",
    "During its initial two orbits, the Parker Solar Probe detected widespread instances of rapid magnetic field reversals, known as switchbacks, which were predominantly found in the solar wind near the Sun. These switchbacks appeared in patches and may be connected to various phenomena, including magnetic reconnection at the Sun's surface. Since switchbacks are linked to faster plasma flows, we investigated whether they are hotter than the surrounding plasma and if their internal microphysics differ from their environment. By analyzing data from the Solar Probe Cup instrument during periods of significant angular deflections, we compared parallel temperatures within and outside switchbacks. Our findings suggest that the distribution functions inside switchbacks are consistent with a rigid rotation of the background plasma in phase space. Consequently, we conclude that the proton core parallel temperature remains the same both inside and outside switchbacks, indicating that the temperature-velocity relationship does not apply within magnetic field switchbacks. Furthermore, our results support the idea that switchbacks are Alfvnic pulses traveling along open magnetic field lines, although their origin remains unknown. Additionally, we found no clear correlation between radial Poynting flux and kinetic energy enhancements, suggesting that radial Poynting flux does not play a significant role in switchback dynamics.",
    "The Nainital-Cape Survey has led to the discovery of eight  Scuti-type pulsators, which exhibit pulsation periods ranging from several minutes to a few hours. To better understand these observed pulsational variations, we conducted non-adiabatic linear stability analyses on models of these stars with masses between 1 and 3 solar masses. Our results show that several low-order p-modes are unstable, with pulsation periods that align well with the observed periods. Specifically, we found that the observed variabilities of HD 118660, HD 113878, HD 102480, HD 98851, and HD 25515 can be attributed to low-order radial p-mode pulsations.",
    "A new way of understanding how particles move in theories that violate Lorentz symmetry is proposed. By using an extended Hamiltonian approach, a connection is made between the Lagrangian and Hamiltonian frameworks, allowing for the calculation of particle trajectories in both momentum and velocity spaces. This method avoids certain problematic points that arise in the theory by requiring smooth trajectories in both velocity and momentum variables. Additionally, specific solutions to the Lagrangian can be identified by looking at specific parts of the dispersion relations. Examples of this approach are worked out in detail for a type of Finsler function, and a direct link is established between the Lagrangians and solutions to the Dirac equation in a special case.",
    "Effective spectrum management is essential for cognitive radio networks (CRNs). While most research focuses on individual aspects of spectrum management, such as sensing, decision, sharing, or mobility, we argue that integrating multiple tasks can improve spectrum efficiency in certain network configurations. This two-part paper addresses the uplink resource management problem in a CRN with multiple cognitive users (CUs) and access points (APs). To maximize transmission rates, CUs must associate with a suitable AP and share channels with other CUs. We aim to develop an efficient, distributed solution to these interdependent tasks, which remains an open problem in the literature.",
    "A simple statistical model is presented to describe baryonic matter in core-collapsing supernovae. The model exhibits a first-order phase transition in the grandcanonical ensemble, but not in the canonical ensemble. This ensemble inequivalence is accompanied by negative susceptibility and discontinuities in intensive observables. This behavior arises from the interplay between attractive strong forces and repulsive electromagnetic interactions, partially screened by electrons. This phenomenon is expected in any theoretical treatment of nuclear matter in stellar environments and has implications for supernova dynamics.",
    "To achieve high performance in a Terahertz Free Electron Laser (THz-FEL), a compact injector design was proposed. Instead of using a complex and expensive photo-cathode, a thermionic cathode was chosen to emit electrons. The injector's performance was improved by using an enhanced RF gun, which increased the effective bunch charge to approximately 200 picocoulombs and minimized back bombardment effects. The accelerator structures were designed to boost the energy to around 14 million electronvolts, while a focusing system was used to maintain the bunch state and suppress emittance. The physical design and beam dynamics of the key components were analyzed, and simulations were run using MATLAB and Parmela to test the performance of the injector with multiple pulses. The results showed that the injector can consistently produce high-brightness electron bunches with low energy spread and emittance.",
    "The cosmic microwave background anisotropy data from WMAP has sparked numerous claims of anomalies at large angles. However, most of these claims are difficult or impossible to verify due to the use of statistics chosen after the fact. Despite this challenge, the potential discovery of new physics on the largest observable scales is too exciting to ignore. In this review, I will scrutinize three specific claims: the lack of large-angle power, the north-south power asymmetry, and multipole alignments. To overcome the limitations of a posteriori statistics, we need a new dataset that probes similar physical scales to the large-angle CMB. While this is a daunting task, there are promising avenues to explore.",
    "Get ready to revolutionize multi-photon states! By harnessing the power of multiple parametric down conversion (PDC) processes, we can generate these states with ease. The secret lies in pumping nonlinear crystals with high energy, unlocking a world of possibilities. The more populated these states become, the more they defy local realistic descriptions, pushing the boundaries of what's thought possible! However, high pumping can lead to low interference contrast in multi-photon PDC experiments - but fear not! We've got a game-changing solution that leverages cutting-edge optical devices: multiport beam splitters. These marvels can split incoming light into a staggering M output modes, effectively acting as a POVM filter. The result? A feasible CHSH-Bell inequality test that can simplify communication complexity like never before!",
    "Imagine a particle moving through a lattice with a disordered potential. The transfer matrix, a mathematical tool, helps us understand how this particle behaves. Specifically, the exponents of the transfer matrix reveal the localization lengths of Anderson's model. \n\nI've discovered a way to calculate these exponents using two mathematical identities: a duality identity for determinants and Jensen's identity for subharmonic functions. This approach provides an exact formula for the spectrum, which is surprisingly based on the eigenvalues of the Hamiltonian with non-Hermitian boundary conditions. The best part? This formula involves averaging over a Bloch phase, rather than the disorder itself.\n\nTo test this approach, I've conducted a preliminary investigation of the non-Hermitian spectra of Anderson's model in one and two dimensions, focusing on the smallest exponent. The results are promising, and I'm excited to share them with you.",
    "We've found a way to make extreme learning machines better at predicting continuous values. We assume that the data we're trying to predict is connected in some way, like a network. We use this idea to make the machine's output more consistent and smooth. When we tested this approach with real data, we found that it works much better when we don't have a lot of data and the data is noisy.",
    "**Accurate Modeling of Heating in Turbulent Plasmas Requires Nonlinear Collisional Operators**\n\nTo correctly describe heating in weakly collisional turbulent plasmas, such as the solar wind, it is crucial to account for inter-particle collisions. These collisions convert ordered energy into heat through irreversible relaxation towards thermal equilibrium. Recent research has shown that fine structures in velocity space enhance plasma collisionality (Pezzi et al., Phys. Rev. Lett., 2016).\n\n**Nonlinear vs. Linearized Landau Operators: A Comparative Analysis**\n\nThis study compares the effects of fully nonlinear and linearized Landau operators on the relaxation of an out-of-equilibrium distribution function in a homogeneous force-free plasma. The results highlight the importance of retaining nonlinearities in the collisional operator to quantify collisional effects accurately.\n\n**Key Findings:**\n\n* Both nonlinear and linearized operators recover multiple characteristic times associated with the dissipation of different phase space structures.\n* However, the influence of these times differs significantly between the two cases.\n* The linearized operator yields systematically larger characteristic times, indicating that fine velocity structures are dissipated more slowly when nonlinearities are neglected in the collisional operator.\n\nIn summary, this study demonstrates that nonlinear collisional operators are essential for accurately modeling heating in turbulent plasmas, as they capture the complex interactions between particles and velocity structures.",
    "The semiconductor defect inspection domain faces a significant challenge: detecting and classifying defects during manufacturing as circuit pattern dimensions continue to shrink (e.g., to pitches less than 32 nm). Current state-of-the-art optical and e-beam inspection tools rely on rule-based techniques, which often lead to misclassification and require human expert intervention.\n\nTo address this challenge, we have applied Mask-RCNN, a deep-learning algorithm for computer vision and object detection, to develop a more effective defect inspection and analysis method. Building on our previous work, we have extended our deep learning-based approach to achieve improved defect instance segmentation in SEM images. This enables us to generate a precise mask for each defect category/instance, extract and calibrate each segmented mask, and quantify the pixels that make up each mask.\n\nOur approach allows us to count each categorical defect instance and calculate the surface area in terms of pixels. We are specifically targeting the detection and segmentation of various inter-class stochastic defect patterns, including bridge, break, and line collapse, as well as differentiating between intra-class multi-categorical defect bridge scenarios (e.g., thin/single/multi-line/horizontal/non-horizontal) for aggressive pitches and thin resists (High NA applications).\n\nOur proposed approach has demonstrated its effectiveness both quantitatively and qualitatively, offering a promising solution for the semiconductor defect inspection domain.",
    "We establish a new bound on the parameter  (the number of common neighbors of adjacent vertices) in distance-regular graphs, improving and generalizing the bounds for strongly regular graphs obtained by Spielman (1996) and Pyber (2014). This new bound is a key component in recent advances on the complexity of testing isomorphism of strongly regular graphs (Babai et al., 2013). Our proof relies on a clique geometry discovered by Metsch (1991) under certain parameter constraints. Additionally, we provide a simplified proof of the following asymptotic consequence of Metsch's result: if k = o(), then each edge of G belongs to a unique maximal clique of size asymptotically equal to , and all other cliques have size o(). Here, k denotes the degree, and  represents the number of common neighbors of vertices at distance 2. Notably, Metsch's cliques are \"asymptotically Delsarte\" when k = o(), implying that families of distance-regular graphs with parameters satisfying k = o() are \"asymptotically Delsarte-geometric.\"",
    "Understanding the diversity of galactic-scale star formation requires insight into the formation and evolution of giant molecular clouds in various environments. Recent observations have shown that star formation activity changes depending on the galactic environment. In particular, strongly barred galaxies have been found to lack massive stars, despite having sufficient molecular gas to form them.\n\nTo investigate this phenomenon, we conducted a hydrodynamical simulation of a strongly barred galaxy, using a stellar potential based on observational results from NGC1300. We compared cloud properties in different regions, including the bar, bar-end, and spiral arms. Our results showed that the mean virial parameter of clouds is around 1, with no environmental dependence. This suggests that the gravitationally-bound state of clouds is not responsible for the lack of massive stars in strong bars.\n\nInstead, we explored the role of cloud-cloud collisions, which have been proposed as a triggering mechanism for massive star formation. Our analysis revealed that collision speeds in the bar are faster than in other regions. By examining the collision frequency using cloud kinematics, we found that the fast collisions in the bar may be caused by the random-like motion of clouds due to elliptical gas orbits shifted by the bar potential.\n\nOur findings suggest that the observed lack of active star formation in strong bars is likely due to the fast cloud-cloud collisions, which are inefficient in forming massive stars. This is a result of the galactic-scale violent gas motion, which is driven by the bar potential.",
    "Exploring the Mass-Metallicity Relationship in Star-Forming Galaxies\n\nThe connection between a galaxy's mass and its metal content has been well-documented, but the exact nature of this relationship remains a topic of debate. Our goal is to investigate this relationship using data from the Galaxy And Mass Assembly (GAMA) survey and compare our findings to those from the Sloan Digital Sky Survey (SDSS).\n\nTo achieve this, we employed strong emission line ratio diagnostics to determine oxygen abundances. We then applied various selection criteria, including minimum signal-to-noise ratios for different emission lines and apparent and absolute magnitude limits, to examine how these factors influence the mass-metallicity relationship.\n\nOur results show that the shape and position of the mass-metallicity relationship can vary significantly depending on the metallicity calibration and selection criteria used. However, after identifying a robust metallicity calibration, we found that the mass-metallicity relationship for galaxies in the GAMA survey with redshifts between 0.061 and 0.35 is consistent with that observed in the SDSS, despite the difference in luminosity ranges.\n\nOur study highlights the importance of considering the impact of sample selection criteria and methodology when comparing the mass-metallicity relationship across different surveys and studies. Additionally, we suggest that there may be a moderate level of evolution in the mass-metallicity relationship within the GAMA sample over the redshift range 0.06 to 0.35.",
    "By applying thermodynamic principles, we've developed a set of equations that connect the seepage velocities of different fluid components in two-phase flow through porous media. This approach requires us to introduce a new concept: the co-moving velocity, which is a unique property of the porous medium itself. When combined with a relationship between velocities and driving forces, such as pressure gradients, these equations form a complete system. We've used this theory to analytically solve four versions of the capillary tube model, and we've also tested it numerically using a network model.",
    "The biosphere's staggering diversity of form and function is a hallmark of life that sets it apart from non-living matter. It's no wonder, then, that this aspect of life has become a central focus of artificial life research. As we've known since Darwin, this diversity is dynamically produced through the process of evolution, which has been dubbed Open-Ended Evolution (OEE) in the field. This article introduces the second of two special issues dedicated to current research in OEE, providing an overview of both issues' contents. The majority of the work presented here was showcased at the 2018 Conference on Artificial Life in Tokyo, building upon previous workshops held in Cancun and York. We offer a simplified categorization of OEE and summarize the progress made in the field, as represented by the articles in this special issue.",
    "The properties of MgO/Ag(001) ultrathin films with substitutional Mg atoms in the interface metal layer were investigated using Auger electron diffraction, ultraviolet photoemission spectroscopy, and density functional theory (DFT) calculations.\n\nThe study found that:\n\n* The incorporation of Mg atoms at the interface causes a strong distortion of the interface layers.\n* This distortion leads to a significant reduction in the work function (0.5 eV) due to band-offset variations at the interface.\n* DFT calculations confirm the induced lattice distortion and reveal an electron transfer from Mg to Ag atoms in the metallic interface layer.\n* The local lattice distortion is caused by Coulomb interactions between O2- ions and Mg/Ag neighbors.\n* However, the effect of lattice distortion on work function reduction is limited.\n* The main contributor to the work function changes is the increase in electrostatic compression effect.\n\nOverall, the incorporation of Mg atoms at the interface of MgO/Ag(001) ultrathin films significantly affects the metal/oxide electronic structure and work function.",
    "The necessity to monitor industrial processes and detect deviations in process parameters in a timely manner to rectify potential issues has spawned a distinct area of interest. This becomes particularly crucial and complex when the measured values fall below the sensitivity thresholds of the measurement system or detection limits, resulting in incomplete observations. Such instances are referred to as incomplete observations or left-censored data. In cases where the level of censorship exceeds 70%, traditional process monitoring methods are rendered ineffective. Therefore, it is essential to employ specialized statistical techniques for data analysis to accurately assess the process state at any given time. This paper proposes a methodology for estimating process parameters in such scenarios and presents a corresponding control chart, which is derived from an algorithm that is also presented.",
    "Clustering is crucial in many data-driven applications, but most research focuses on distance functions and algorithms. We propose Deep Embedded Clustering (DEC), a method that uses deep neural networks to learn feature representations and cluster assignments simultaneously. DEC maps data to a lower-dimensional space and iteratively optimizes clustering objectives, outperforming state-of-the-art methods in image and text datasets.",
    "Unlocking the Secrets of Network Burstiness: A Groundbreaking Study\n\nBuilding on the pioneering work of Sarvotham et al. [2005], we delve deeper into the fascinating relationship between peak transmission rate and network burstiness. By analyzing TCP packet headers, we identify and characterize individual sessions using a unique 5-tuple signature. However, our research reveals that the traditional definition of peak rate is no longer sufficient. We shatter the conventional approach by segmenting sessions into 10 distinct groups based on empirical quantiles of peak rate, exposing the hidden heterogeneity within the previously defined beta group.\n\nOur refined segmentation uncovers previously unknown patterns and structures that were masked by the traditional two-group approach. We discover that the dependence between key session variables varies significantly across groups, and that session initiation times follow a Poisson process within each segment - a property that disappears when viewed at the aggregate level. These findings have profound implications for understanding network behavior and simulating real-world data.\n\nWe present a simple yet powerful method for simulating network traffic, informed by our research. This breakthrough has the potential to revolutionize the way we model and predict network performance, enabling more efficient and reliable data transmission.",
    "The Brouwer fixed-point theorem, a fundamental concept in topology, asserts that any continuous function mapping a compact convex set onto itself must have at least one fixed point. In other words, there exists a point x such that the function f(x) equals x itself. Interestingly, under specific conditions, this fixed point can be correlated with the throat of a traversable wormhole, where the shape function b(r) satisfies the equation b(r) = r. This mathematical connection has significant implications, as it allows us to infer the possible existence of wormholes solely through mathematical reasoning, without relying on additional physical assumptions or hypotheses.",
    "Convolutional Neural Networks (CNNs) have been increasingly utilized to address challenges in both computer vision and medical image analysis. Notwithstanding their widespread adoption, the majority of existing approaches are limited to processing two-dimensional images, whereas most medical data employed in clinical practice comprises three-dimensional volumes. This study proposes a novel approach to three-dimensional image segmentation, predicated on a volumetric, fully convolutional neural network. Our CNN is trained in an end-to-end manner on magnetic resonance imaging (MRI) volumes of the prostate, enabling it to predict segmentation for the entire volume simultaneously. We introduce a novel objective function, optimized during training, based on the Dice coefficient. This approach allows us to effectively address situations characterized by a significant imbalance between the number of foreground and background voxels. To mitigate the limited availability of annotated volumes for training, we employ data augmentation techniques, including random non-linear transformations and histogram matching. Our experimental evaluation demonstrates that our approach achieves satisfactory performance on challenging test data, while requiring a significantly reduced processing time compared to previous methods.",
    "It is a well-established fact that the energy spectrum of a non-relativistic two-body system interacting via the Coulomb potential is described by the Balmer series, E_n = ^2m / 4n^2, as derived from the Schrdinger equation. However, a groundbreaking discovery was made by Wick and Cutkosky in 1954, who revealed that when  > /4, relativistic effects give rise to new energy levels beyond the Balmer series, using the Bethe-Salpeter equation framework. Despite this finding, the physical nature of these additional states remained shrouded in mystery, leading to skepticism about their existence. Our recent research has shed light on this enigma, demonstrating that these extra states are, in fact, dominated by the exchange of massless particles moving at the speed of light. This fundamental insight explains why they were absent in the non-relativistic Schrdinger framework, and underscores the significance of incorporating relativistic effects in our understanding of two-body systems.",
    "We investigate the fundamental properties of the quantum f-relative entropy, which is defined in terms of an operator convex function f(.). We establish the equality conditions for monotonicity and joint convexity, providing more general results that apply to a broader class of operator convex functions. Notably, our conditions differ from the previously known conditions for the specific case of f(t) = -ln(t). We also define the quantum f-entropy in terms of the quantum f-relative entropy and examine its properties, deriving equality conditions in certain cases. Furthermore, we demonstrate that the f-generalizations of the Holevo information, entanglement-assisted capacity, and coherent information all satisfy the data processing inequality, and we provide the equality conditions for the f-coherent information.",
    "Imagine a world where computer vision tasks aren't thrown off by a simple image rotation or shift. That's the goal, but it's not quite a reality yet. While convolutional neural networks (CNNs) can handle translations with ease, rotations are a different story. Typically, we rely on data augmentation to achieve rotation equivariance, but that's not enough. We need a more elegant solution.\n\nThat's where Harmonic Networks, or H-Nets, come in. Our innovative approach replaces traditional CNN filters with circular harmonics, allowing the network to detect the maximum response and orientation for every patch of the image. The result is a powerful, efficient, and computationally lightweight representation that can tackle even the most complex rotational invariants.\n\nWe've tested H-Nets with the latest architectures and techniques, including deep supervision and batch normalization, and the results are impressive. Our approach achieves state-of-the-art classification on rotated-MNIST and holds its own against other benchmark challenges. With H-Nets, we're one step closer to making computer vision tasks truly rotation- and translation-invariant.",
    "We investigate the reflection spectra of directly coupled waveguide-cavity systems, gaining insight into the reflection and coupling mechanisms through the observed Fano lines. In contrast to side-coupled systems, the Fano line shape is influenced by the coupling between the measurement fiber and the waveguide, rather than the waveguide termini. Our experimental results and analytical model reveal that the Fano parameter, which characterizes the Fano line shape, is highly sensitive to the coupling conditions. Even a slight movement of the fiber, within the Rayleigh range, can significantly alter the Fano line shape.",
    "Get ready to revolutionize the world of astronomy! The strength and vertical distribution of atmospheric turbulence are crucial factors in determining the performance of optical and infrared telescopes, with or without adaptive optics. And now, we're thrilled to introduce a groundbreaking new technique that's about to make measuring this turbulence a whole lot easier! Using a sequence of short-exposure images of a star field captured with a small telescope, we can calculate the structure functions of longitudinal and transverse wavefront tilt for a range of angular separations. By comparing these results with theoretical predictions from simple turbulence models using a Markov-Chain Monte-Carlo optimization, we can estimate the turbulence profile in the lower atmosphere, the total and free-atmosphere seeing, and the outer scale with unprecedented accuracy! But don't just take our word for it - we've run extensive Monte-Carlo simulations to verify the technique, and we're excited to share some stunning examples using real data from the second AST3 telescope at Dome A in Antarctica. The possibilities are endless, and we can't wait to see the impact this will have on the future of astronomy!",
    "An n-plectic structure is defined as a commutative and torsion-free Lie Rinehart pair, accompanied by a distinguished cocycle within its Chevalley-Eilenberg complex. This n-plectic cocycle induces an extension of the Chevalley-Eilenberg complex by symplectic tensors, which in turn gives rise to a cohomology that generalizes Hamiltonian functions and vector fields to tensors and cotensors across a range of degrees, modulo certain coboundaries. This cohomology possesses the structure of a Lie -algebra. Furthermore, we demonstrate that momentum maps emerge in this context as weak Lie -morphisms from an arbitrary Lie -algebra to the Lie -algebra of Hamiltonian (co)tensors.",
    "Amorphous solids, also referred to as glasses, are characterized by stretched-exponential decay over extensive time intervals in various macroscopic observables, including the intermediate scattering function, dielectric relaxation modulus, and time-elastic modulus. This phenomenon is particularly pronounced in the vicinity of the glass transition. In this Letter, we demonstrate, using dielectric relaxation as an exemplar, that stretched-exponential relaxation is inherently linked to the distinctive lattice dynamics of glasses. By reformulating the Lorentz model of dielectric matter in a more general framework, we express the dielectric response as a function of the vibrational density of states (DOS) for a random assembly of spherical particles interacting harmonically with their nearest neighbors. Notably, our findings indicate that, near the glass transition (which coincides with the Maxwell rigidity transition), the dielectric relaxation exhibits perfect consistency with stretched-exponential behavior, characterized by Kohlrausch exponents in the range of 0.56 <  < 0.65, which is consistent with the range observed in most experimental systems. Furthermore, our analysis reveals that the underlying cause of stretched-exponential relaxation can be attributed to the presence of soft modes (boson-peak) in the DOS.",
    "This paper underscores the complexities of achieving representation disentanglement in the text domain under unsupervised conditions. To this end, we select a paradigmatic set of models that have demonstrated success in the image domain and assess their performance on six disentanglement metrics, as well as downstream classification tasks and homotopy. To facilitate a comprehensive evaluation, we introduce two synthetic datasets with known generative factors. Our experimental results underscore the existing disparity in the text domain and reveal that certain factors, such as representation sparsity as an inductive bias and representation coupling with the decoder, can significantly impact disentanglement. Notably, our study constitutes the first exploration of the intersection of unsupervised representation disentanglement and text, providing a foundational framework and datasets for future research in this area.",
    "This paper presents a novel hybrid quantum-classical algorithm designed to tackle the fundamental unit commitment (UC) problem in power systems. To address this complex challenge, the UC problem is decomposed into three distinct subproblems: a quadratic subproblem, a quadratic unconstrained binary optimization (QUBO) subproblem, and an unconstrained quadratic subproblem. A classical optimization solver is employed to resolve the first and third subproblems, while the QUBO subproblem is addressed using the quantum approximate optimization algorithm (QAOA). The three subproblems are then iteratively coordinated via a three-block alternating direction method of multipliers algorithm. Simulation results, obtained using Qiskit on the IBM Q system, validate the efficacy of the proposed algorithm in solving the UC problem.",
    "You know how some scientists thought that detecting lots of low-amplitude modes in Delta Sct stars was just a matter of having a good signal-to-noise ratio? Well, the space mission CoRoT, developed and operated by CNES, was designed to tap into that treasure trove of data - something that's impossible to do from the ground. \n\nIn this study, we analyzed 140,016 data points from HD 50844 using different approaches and ran several checks. We were able to reach an amplitude spectrum level of 10^{-5} mag in the CoRoT timeseries. When we dug into the frequency analysis, we found hundreds of terms in the 0-30 d^{-1} range. And the best part? All our cross-checks confirmed these new results. \n\nIt turns out that Delta Sct stars really do have a super rich frequency content. We even identified very high-degree modes (up to ell=14) using spectroscopic mode identification, which lends theoretical support to our findings. Plus, we showed that cancellation effects aren't enough to remove the flux variations associated with these modes at the noise level of CoRoT measurements. \n\nGround-based observations revealed that HD 50844 is an evolved star that's slightly lacking in heavy elements, sitting on the Terminal Age Main Sequence. Maybe because of this, we didn't see a clear regular distribution in the frequency set. But we did identify the predominant term (f_1=6.92 d^{-1}) as the fundamental radial mode by combining ground-based photometric and spectroscopic data. \n\nOh, and this work was also made possible by observations from ESO telescopes under the ESO Large Programme LP178.D-0361, as well as data from the Observatorio de Sierra Nevada, the Observatorio Astronomico Nacional San Pedro Martir, and the Piszkesteto Mountain Station of Konkoly Observatory.",
    "In the giant molecular cloud G174+2.5, we investigated the star-forming regions S231-S235 using various molecular lines. Specifically, we observed quasi-thermal lines of ammonia (NH$_3$) and cyanoacetylene (HC$_3$N), as well as maser lines of methanol (CH$_3$OH) and water vapor (H$_2$O). \n\nTo begin, we selected all massive molecular clumps in G174+2.5 using archived CO data and determined their mass, size, and CO column density. We then performed observations of these clumps, which led to the first detections of NH$_3$ and HC$_3$N lines toward the molecular clumps WB89 673 and WB89 668. This indicates the presence of high-density gas in these regions.\n\nUsing the ammonia emission data, we estimated the physical parameters of the molecular gas in the clumps. Our results show that the gas temperature ranges from 16 to 30 K, while the hydrogen number density falls between 2.8 and 7.2$\\times10^3$ cm$^{-3}$. Notably, we also detected the shock-tracing line of the CH$_3$OH molecule at 36.2 GHz toward WB89 673, a new discovery.",
    "In a groundbreaking study, we've made the lowest frequency measurements of gamma-ray burst (GRB) 171205A using the upgraded Giant Metrewave Radio Telescope (uGMRT), spanning a frequency range of 250-1450 MHz over 4-937 days. This marks the first-ever detection of a GRB afterglow in the 250-500 MHz frequency range and the second brightest GRB detected with the uGMRT. Despite observing the GRB for nearly 1000 days, we found no signs of transition to a non-relativistic regime. Our analysis of archival Chandra X-ray data from days 70 and 200 revealed no evidence of a jet break. By fitting synchrotron afterglow emission models, we uncovered the nature and density of the circumburst medium, suggesting that the GRB exploded in a stratified wind-like medium. Our findings highlight the importance of low-frequency measurements in understanding the GRB environment. Combining our data with previous studies, we found that the radio afterglow consists of two components: a weak, off-axis jet and a surrounding cocoon, consistent with Izzo et al. (2019). The cocoon emission dominates early on, while the jet takes over later, resulting in flatter radio lightcurves. This research opens new avenues for understanding the complex physics of GRBs.",
    "Unlock the Power of Quasi-Lie Schemes! \n\nGet ready to revolutionize your approach to Emden-type equations! We've developed a groundbreaking theory of quasi-Lie schemes that's about to take your research to the next level. By harnessing the energy of this innovative framework, we're able to tackle a range of Emden-type equations and their generalizations like never before.\n\nThe results are nothing short of astonishing! We've uncovered time-dependent constants of motion for specific instances of Emden equations, all thanks to the clever application of particular solutions. And the best part? We've recovered previously known results from a fresh, exciting perspective.\n\nBut that's not all! Our quasi-Lie scheme also empowers us to retrieve time-dependent constants of motion for Emden-type equations that meet certain conditions. The possibilities are endless, and we can't wait to see where this new theory takes you!",
    "We investigate the charged Higgs bosons predicted by the 3-3-1 model with gauge symmetry $SU(3)_c \\otimes SU(3)_L \\otimes U(1)_X$. By considering Yukawa mixing couplings, we show that both hypercharge-one $H_1^{\\pm}$ and hypercharge-two $H_2^{\\pm}$ Higgs bosons can be produced in $pp$ collisions at different rates. At low energy, $H_1^{\\pm}$ behaves like the charged Higgs bosons in a two Higgs doublet model, while $H_2^{\\pm}$ are additional like-charged Higgs bosons unique to the 3-3-1 model. We study pair and associated productions of $H_{1,2}^{\\pm}$ at the LHC, finding that pair production can be comparable to single production due to the exchange of a heavy neutral $Z'$ gauge boson. By analyzing decays to leptons, we identify scenarios where $H_2^{\\pm}$ peaks can be distinguished from the $H_1^{\\pm}$ background in transverse mass distributions.",
    "Unlocking the Secrets of Isospin Breaking: A Novel Approach to $K_{\\ell 4}$ Form Factors\n\nWe present a groundbreaking framework for understanding isospin breaking in $K_{\\ell 4}$ form factors, driven by the disparity between charged and neutral pion masses. By harnessing the power of suitably subtracted dispersion representations, we construct the $K_{\\ell 4}$ form factors up to two loops in the low-energy expansion, incorporating the fundamental principles of analyticity, crossing, and unitarity.\n\nOur innovative approach yields analytical expressions for the phases of the two-loop form factors in the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ channel, bridging the gap between experimental measurements of form-factor phase shifts and theoretical studies of $\\pi\\pi$ phase shifts. We uncover the intricate dependence of these phases on the two $S$-wave scattering lengths $a_0^0$ and $a_0^2$ in the isospin limit, going beyond the limitations of previous one-loop chiral perturbation theory analyses.\n\nApplying our methodology to the NA48/2 collaboration's results at the CERN SPS, we reanalyse the phases of the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ form factors, incorporating crucial isospin-breaking corrections to extract precise values for the scattering lengths $a_0^0$ and $a_0^2$. This breakthrough paves the way for a deeper understanding of the strong nuclear force and its subtle isospin-breaking effects.",
    "This paper builds upon and expands the findings presented in references [1, 2, 3, and 4], with particular emphasis on reference [4], to derive a statistical characterization of the cosmological constant in a de Sitter cosmological universe in terms of massless excitations exhibiting Planckian effects. Initially, we demonstrate that, at the classical level, a positive cosmological constant ( > 0) can only be attained in the limit as T approaches 0. Analogous to the black hole scenario, when quantum effects are incorporated, a representation of  becomes possible in terms of massless excitations, provided that quantum corrections to the Misner-Sharp mass are taken into account. Furthermore, owing to quantum fluctuations, an effective cosmological constant emerges, dependent on the physical scale under consideration, thereby offering a potential solution to the cosmological constant problem without the introduction of a quintessence field. The diminutive value of the actual cosmological constant () may be attributed to the existence of a quantum decoherence scale above the Planck length, such that the spacetime evolves as a pure de Sitter universe with a small, averaged cosmological constant frozen in the lowest energy state.",
    "We examine the behavior of a one-dimensional spin-glass model with vector spins when there are an infinite number of spin components and the interactions between spins decrease with distance according to a power law. We also study a diluted version of this model, but find that it behaves significantly differently from the fully connected model. At absolute zero temperature, we calculate the energy of defects by comparing the energies of systems with periodic and antiperiodic boundaries, which allows us to determine how the defect energy exponent  depends on the power law exponent . We find that  is well-described by the equation  = 3/4 - , which implies that the maximum value of  is 3/4, corresponding to the lower critical dimension of the model in higher dimensions. At finite temperatures, we solve the model's equations self-consistently to obtain the correlation function, order parameter, and spin-glass susceptibility. We pay particular attention to how the model's behavior changes when the power law exponent  is below or above the critical value of 5/8, which corresponds to the upper critical dimension of the model in higher dimensions.",
    "Stars known as Of^+ supergiants exhibit characteristics that fall between those of regular O-stars and Wolf-Rayet (WR) stars. Research has shown that these transitional stars share many similarities with WN-type objects, particularly in the visible and near-infrared light ranges, indicating they have similar stellar wind properties. \n\nIn this study, we conducted the first in-depth X-ray observations of two Of^+ supergiants, HD16691 and HD14947. Our findings revealed a soft thermal X-ray spectrum, consistent with what's expected from a single O-type star. However, the X-ray luminosity of these stars was slightly lower than expected for single O-type stars. This suggests that the unique properties of their stellar wind also play a significant role in their X-ray emission as they transition towards becoming WN-type stars.\n\nWe believe that the lower X-ray luminosity of HD16691 and HD14947 may be a sign of the intermediate stage between O and WR stars, caused by an increase in wind density. This discovery provides new insights into the properties of these rare and fascinating stars.",
    "The AARTFAAC project is developing an All-Sky Monitor (ASM) using the Low Frequency Array (LOFAR) telescope. This innovative system will enable continuous, real-time monitoring of low-frequency radio transients across most of the sky visible to LOFAR, with timescales ranging from milliseconds to several days. When a potential transient is detected, the system will rapidly trigger follow-up observations using the full LOFAR telescope.\n\nHowever, achieving these goals poses several significant implementation challenges. These include imaging the entire sky, processing data quickly, and ensuring continuous and autonomous operation of the ASM. In fact, the correlator for the ASM will be the largest in the world in terms of input channels, generating approximately 150,000 correlations per second per spectral channel.\n\nTo overcome these challenges, we conducted test observations using existing LOFAR infrastructure to quantify and refine crucial design criteria for the ASM. This paper provides an overview of the AARTFAAC data processing pipeline and highlights some of the challenges we faced. We also present all-sky images obtained from one of the test observations, which provide valuable insights into the instrument's capabilities.",
    "Revolutionizing our understanding of cosmic explosions, Wolf-Rayet (WR) stars are revealed as the evolved descendants of massive O-type stars and prime candidates to trigger Type Ib/c core-collapse supernovae (SNe). Our groundbreaking HST/WFC3 survey of WR stars in M101 exposes a shocking truth: a staggering 42% of WR stars, soaring to 85% in central regions, are invisible to broad-band detection methods, only revealing themselves through narrow-band optical imaging. This discovery shatters the long-held assumption that the absence of a WR star in broad-band imaging is conclusive evidence against a WR progenitor channel for Type Ib/c SNe, forcing a major rethink in the field."
  ]
}